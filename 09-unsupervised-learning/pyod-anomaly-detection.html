
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>PyOD: Anomaly Detection</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10. Deep Learning" href="../10-deep-learning/_intro.html" />
    <link rel="prev" title="Sklearn: Dimensional Reduction" href="sklearn-dimensional-reduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../util/intro.html">
                    Data Science
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01-python-programing/_intro.html">
   <b>
    1. Python Programing
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-python-programing/python-basic-concepts.html">
     Python: Basic Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-python-programing/python-data-types.html">
     Python: Data Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-python-programing/python-data-containers.html">
     Python: Data Containers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-python-programing/python-functions-objects.html">
     Python: Functions and Objects
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-python-programing/python-algorithms.html">
     (w) Python: Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-python-programing/python-external-sources.html">
     Python: External Sources
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-python-programing/selenium-web-scraping.html">
     Selenium: Web Scraping
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-mathematics/_intro.html">
   <b>
    2. Mathematics
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-mathematics/numpy-linear-algebra.html">
     NumPy: Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-mathematics/sympy-calculus.html">
     SymPy: Calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-mathematics/numpy-probability.html">
     NumPy: Probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-mathematics/numpy-statistics.html">
     NumPy: Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-mathematics/scipy-hypothesis-testing.html">
     SciPy: Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-mathematics/numpy-applied-mathematics.html">
     (w) NumPy: Applied Mathematics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-mathematics/networkx-network-analysis.html">
     (w) NetworkX: Network Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-data-manipulation/_intro.html">
   <b>
    3. Data Manipulation
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-data-manipulation/numpy-arrays.html">
     NumPy: Arrays
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-data-manipulation/pandas-data-exploratory.html">
     Pandas: Data Exploratory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-data-manipulation/pandas-data-cleaning.html">
     Pandas: Data Cleaning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-data-manipulation/pandas-data-transformation.html">
     Pandas: Data Transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-data-manipulation/janitor-pandas-extensions.html">
     Janitor: Pandas Extensions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-big-data/_intro.html">
   <b>
    4. Big Data
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-big-data/hiveql-data-manipulation.html">
     HiveQL: Data Manipulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-big-data/pyspark-data-exploratory.html">
     PySpark: Data Exploratory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-big-data/pyspark-data-cleaning.html">
     PySpark: Data Cleaning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-big-data/pyspark-data-transformation.html">
     PySpark: Data Transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-big-data/dask-parallelized-pandas.html">
     Dask: Parallelized Pandas
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-data-visualization/_intro.html">
   <b>
    5. Data Visualization
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-data-visualization/matplotlib-graph-construction.html">
     Matplotlib: Graph Construction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-data-visualization/seaborn-statistical-visualization.html">
     Seaborn: Statistical Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-data-visualization/plotly-interactive-visualization.html">
     Plotly: Interactive Visualization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-machine-learning/_intro.html">
   <b>
    6. Machine Learning
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-machine-learning/sklearn-machine-learning.html">
     Sklearn: Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-machine-learning/sklearn-classification.html">
     Sklearn: Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-machine-learning/sklearn-regression.html">
     Sklearn: Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-machine-learning/sklearn-feature-engineering.html">
     Sklearn: Feature Engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-machine-learning/lenskit-recommendation.html">
     Lenskit: Recommendation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-machine-learning/pyspark-machine-learning.html">
     PySpark: Machine Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-tabular-learning/_intro.html">
   <b>
    7. Tabular Learning
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-tabular-learning/sklearn-ensemble-learning.html">
     Sklearn: Ensemble Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-tabular-learning/xgboost-tree-boosting.html">
     XGBoost: Tree Boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-tabular-learning/ray-hyperparameter-optimization.html">
     Ray: Hyperparam Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-tabular-learning/shap-model-interpretation.html">
     Shap: Model Interpretation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-tabular-learning/imblearn-targeted-modeling.html">
     (w) Imblearn: Targeted Modeling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-time-series/_intro.html">
   <b>
    8. Time Series
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-time-series/statsmodels-temporal-analysis.html">
     Statsmodels: Temporal Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-time-series/prophet-forecasting-algorithms.html">
     Prophet: Forecasting Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-time-series/sktime-forecasting-pipeline.html">
     Sktime: Forecasting Pipeline
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-time-series/darts-deep-forecasting.html">
     (w) Darts: Deep Forecasting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="_intro.html">
   <b>
    9. Unsupervised Learning
   </b>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="mlxtend-association-rules.html">
     (w) Mlxtend: Association Rules
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sklearn-clustering.html">
     Sklearn: Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sklearn-dimensional-reduction.html">
     Sklearn: Dimensional Reduction
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     PyOD: Anomaly Detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-deep-learning/_intro.html">
   <b>
    10. Deep Learning
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-learning/gensim-text-mining.html">
     Gensim: Text Mining
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-learning/opencv-image-processing.html">
     (w) OpenCV: Image Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-learning/numpy-gradient-descent.html">
     Python: Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-learning/keras-multilayer-perceptron.html">
     Keras: Multilayer Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-learning/keras-recurrent-networks.html">
     Keras: Recurrent Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-learning/keras-convolutional-networks.html">
     (w) Keras: Convolutional Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-learning/pytorch-deep-learning.html">
     PyTorch: Deep Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-r-programing/_intro.html">
   <b>
    11. R Programing
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-r-programing/r-basic-concepts.html">
     R: Basic Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-r-programing/r-data-structures.html">
     R: Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-r-programing/tidyverse-data-wrangling.html">
     Tidyverse: Data Wrangling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-r-programing/dplyr-data-cleaning.html">
     Dplyr: Data Cleaning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-r-programing/ggplot-data-visualization.html">
     Ggplot: Data Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-r-programing/r-statistics.html">
     R: Statistics
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/hungpq7/tabular-book/master?urlpath=tree/09-unsupervised-learning/pyod-anomaly-detection.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/hungpq7/tabular-book/blob/master/09-unsupervised-learning/pyod-anomaly-detection.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/hungpq7/tabular-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/09-unsupervised-learning/pyod-anomaly-detection.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   1. Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-anomalies">
     1.1. Types of anomalies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approaches">
     1.2. Approaches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     1.3. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#feature-engine">
       Feature-engine
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scikit-learn">
       Scikit-learn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pyod">
       PyOD
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-models">
   2. Linear models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca">
     2.1. PCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-class-svm">
     2.2. One-class SVM
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proximity-based">
   3. Proximity-based
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#knn">
     3.1. KNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clustering-based">
     3.2. Clustering-based
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-outlier-factor">
     3.3. Local Outlier Factor
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#connectivity-based">
       Connectivity-based
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistic">
   4. Probabilistic
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-4-1-ecod">
     (w) 4.1. ECOD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#copula-based">
     4.2. COPula-based
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#histogram-based">
     4.3. Histogram-based
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#black-box">
   5. Black-box
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#isolation-forest">
     5.1. Isolation Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-5-2-autoencoder">
     (w) 5.2. Autoencoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>PyOD: Anomaly Detection</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   1. Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-anomalies">
     1.1. Types of anomalies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approaches">
     1.2. Approaches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     1.3. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#feature-engine">
       Feature-engine
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scikit-learn">
       Scikit-learn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pyod">
       PyOD
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-models">
   2. Linear models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca">
     2.1. PCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-class-svm">
     2.2. One-class SVM
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proximity-based">
   3. Proximity-based
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#knn">
     3.1. KNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clustering-based">
     3.2. Clustering-based
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-outlier-factor">
     3.3. Local Outlier Factor
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#connectivity-based">
       Connectivity-based
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistic">
   4. Probabilistic
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-4-1-ecod">
     (w) 4.1. ECOD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#copula-based">
     4.2. COPula-based
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#histogram-based">
     4.3. Histogram-based
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#black-box">
   5. Black-box
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#isolation-forest">
     5.1. Isolation Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-5-2-autoencoder">
     (w) 5.2. Autoencoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="pyod-anomaly-detection">
<h1>PyOD: Anomaly Detection<a class="headerlink" href="#pyod-anomaly-detection" title="Permalink to this headline">#</a></h1>
<section id="overview">
<h2>1. Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h2>
<p>An outlier is a data point that differs significantly from other observations. Outliers can cause serious problems in statistical analysis. Detecting outliers is more likely be an art rather than a science, therefore you need both quantitative and qualitative methods to identify outliers. However, there’s no best rule for handling outliers. You need to ask yourself <em>Why are they outliers?</em> and <em>How can they affect your analyses?</em>.</p>
<section id="types-of-anomalies">
<h3>1.1. Types of anomalies<a class="headerlink" href="#types-of-anomalies" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><em>Local anomalies</em>: Refers to the anomalies that deviant from their local neighbors. Imagine that there are 3 clusters in the dataset, in each cluster have some points that are different from other points in the same cluster. They are local anomalies.</p></li>
<li><p><em>Global anomlies</em>: The anomalies that far from normal data. Consider again 3 clusters in the dataset, if there are some points which far from all 3 clusters,that are called global anomalies.</p></li>
<li><p><em>Dependency anomalies</em>: refer to the data points that do not follow the dependency structure which normal data follow. Example salary covariates with education but anomalies do not.</p></li>
<li><p><em>Clustered anomalies</em>: is group of abnomalies, the data points are not scattered in space but appear in a group that are different from normal data</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/types_of_anomaly.png"><img alt="../_images/types_of_anomaly.png" class="align-center" src="../_images/types_of_anomaly.png" style="height: 220px;" /></a>
</section>
<section id="approaches">
<h3>1.2. Approaches<a class="headerlink" href="#approaches" title="Permalink to this headline">#</a></h3>
<p>Anomalies can be detected throught three types of Machine Learning approaches:</p>
<ul class="simple">
<li><p><em>Unsupervised</em>: no label provided, the algorithm automatically identify outliers. This is the most popular scenario you may face in real-world problems.</p></li>
<li><p><em>Semi-supervised</em>: you provide to the algorithm normal data only, it will capture the general patterns and decide if any new data do not follow that pattern. Also known as novelty detection.</p></li>
<li><p><em>Supervised</em>: requires observations marked as outliers or not, can be solved as a binary classification problem. Rarely seen in reality.</p></li>
</ul>
</section>
<section id="implementation">
<h3>1.3. Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h3>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfWine</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/wine_quality.csv&#39;</span><span class="p">)</span>
<span class="n">dfWine</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed_acidity</th>
      <th>volatile_acidity</th>
      <th>citric_acid</th>
      <th>residual_sugar</th>
      <th>chlorides</th>
      <th>free_sulfur_dioxide</th>
      <th>total_sulfur_dioxide</th>
      <th>density</th>
      <th>ph</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.0</td>
      <td>0.27</td>
      <td>0.36</td>
      <td>20.7</td>
      <td>0.045</td>
      <td>45.0</td>
      <td>170.0</td>
      <td>1.0010</td>
      <td>3.00</td>
      <td>0.45</td>
      <td>8.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.3</td>
      <td>0.30</td>
      <td>0.34</td>
      <td>1.6</td>
      <td>0.049</td>
      <td>14.0</td>
      <td>132.0</td>
      <td>0.9940</td>
      <td>3.30</td>
      <td>0.49</td>
      <td>9.5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.1</td>
      <td>0.28</td>
      <td>0.40</td>
      <td>6.9</td>
      <td>0.050</td>
      <td>30.0</td>
      <td>97.0</td>
      <td>0.9951</td>
      <td>3.26</td>
      <td>0.44</td>
      <td>10.1</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="feature-engine">
<h4>Feature-engine<a class="headerlink" href="#feature-engine" title="Permalink to this headline">#</a></h4>
<p>The simplest way to detect outliers is calculating statistics for a single variable and use them as thresholds to determine extreme values. There are four popular statistical ranges indicating normal data:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="95934c2d-881e-4757-9005-c41fe2c3fb77" name="9e5c8025-0306-4f2f-ad21-c1f959619445" type="radio">
</input><label class="sd-tab-label" for="95934c2d-881e-4757-9005-c41fe2c3fb77">
Gaussian</label><div class="sd-tab-content docutils">
<div class="math notranslate nohighlight">
\[\mu-k\sigma&lt;x&lt;\mu+k\sigma\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the mean</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> (hyperparameter) is usually from <span class="math notranslate nohighlight">\(2\)</span> to <span class="math notranslate nohighlight">\(3\)</span></p></li>
</ul>
</div>
<input id="052dd6fe-a677-4f18-b644-2541b88a8e50" name="9e5c8025-0306-4f2f-ad21-c1f959619445" type="radio">
</input><label class="sd-tab-label" for="052dd6fe-a677-4f18-b644-2541b88a8e50">
IQR</label><div class="sd-tab-content docutils">
<div class="math notranslate nohighlight">
\[Q_{0.25}-k\times\text{IQR}&lt;x&lt;Q_{0.75}+k\times\text{IQR}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q_{0.25}\)</span> and <span class="math notranslate nohighlight">\(Q_{0.75}\)</span> are quantiles</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{IQR}=Q_{0.75}-Q_{0.25}\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Interquartile_range">interquartile range</a></p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> (hyperparameter) is usually from <span class="math notranslate nohighlight">\(1.5\)</span> to <span class="math notranslate nohighlight">\(3\)</span></p></li>
</ul>
</div>
<input id="e2753ad6-55c1-4945-ad5d-dc6783ead348" name="9e5c8025-0306-4f2f-ad21-c1f959619445" type="radio">
</input><label class="sd-tab-label" for="e2753ad6-55c1-4945-ad5d-dc6783ead348">
MAD</label><div class="sd-tab-content docutils">
<div class="math notranslate nohighlight">
\[Q_{0.5}-k\times\text{MAD}&lt;x&lt;Q_{0.5}+k\times\text{MAD}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q_{0.5}\)</span> is the median</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{MAD}\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Median_absolute_deviation">median absolute deviation</a></p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> (hyperparameter) is usually from <span class="math notranslate nohighlight">\(3\)</span> to <span class="math notranslate nohighlight">\(3.5\)</span></p></li>
</ul>
</div>
<input id="656e058d-e989-4d8b-9fa5-16bb7c35f278" name="9e5c8025-0306-4f2f-ad21-c1f959619445" type="radio">
</input><label class="sd-tab-label" for="656e058d-e989-4d8b-9fa5-16bb7c35f278">
Quantiles</label><div class="sd-tab-content docutils">
<div class="math notranslate nohighlight">
\[Q_{k}&lt;x&lt;Q_{1-k}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q_{k}\)</span> and <span class="math notranslate nohighlight">\(Q_{1-k}\)</span> are quantiles</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> (hyperparameter) is usually <span class="math notranslate nohighlight">\(0.05\)</span> or <span class="math notranslate nohighlight">\(0.1\)</span></p></li>
</ul>
</div>
</div>
<p>All four approaches are implemented in Feature-engine via <a class="reference external" href="https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/Winsorizer.html"><code class="docutils literal notranslate"><span class="pre">Winsorizer</span></code></a> (clipping outliers) or <a class="reference external" href="https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/OutlierTrimmer.html"><code class="docutils literal notranslate"><span class="pre">OutlierTrimmer</span></code></a> (removing outliers). They share the same hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">capping_method</span></code>: the strategy for deciding extreme values, defaults to <em>gaussian</em>. Other options are <em>iqr</em>, <em>mad</em> and <em>quantiles</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fold</span></code>: the coefficient <span class="math notranslate nohighlight">\(k\)</span> in theshold formulas, defaults to <em>0.05</em> for quantiles strategy and <em>3</em> for other strategies.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tail</span></code>: in which side extreme values are considered outliers, defaults to <em>right</em>. Other options are <em>left</em> and <em>both</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trimmer</span> <span class="o">=</span> <span class="n">OutlierTrimmer</span><span class="p">(</span><span class="n">capping_method</span><span class="o">=</span><span class="s1">&#39;iqr&#39;</span><span class="p">,</span> <span class="n">fold</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">tail</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;citric_acid&#39;</span><span class="p">])</span>
<span class="n">trimmer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dfWine</span><span class="p">)</span>
<span class="n">trimmer</span><span class="o">.</span><span class="n">right_tail_caps_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;citric_acid&#39;: 0.5700000000000001}
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Beside <em>clipping</em> and <em>trimming</em>, we can perform a log transformation on our feature. This dampends outlier effects.</p>
</div>
</section>
<section id="scikit-learn">
<h4>Scikit-learn<a class="headerlink" href="#scikit-learn" title="Permalink to this headline">#</a></h4>
<p>Scikit-learn distinguishes <a class="reference external" href="https://scikit-learn.org/stable/modules/outlier_detection.html">outlier and novelty</a>. It provides a few algorithms for the task: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"><code class="docutils literal notranslate"><span class="pre">IsolationForest</span></code></a>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html"><code class="docutils literal notranslate"><span class="pre">LocalOutlierFactor</span></code></a>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html"><code class="docutils literal notranslate"><span class="pre">OneClassSVM</span></code></a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"><code class="docutils literal notranslate"><span class="pre">DBSCAN</span></code></a>.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">dfWine</span>\
    <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">cluster</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">dfWine</span><span class="p">))</span>\
    <span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;cluster == -1&#39;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed_acidity</th>
      <th>volatile_acidity</th>
      <th>citric_acid</th>
      <th>residual_sugar</th>
      <th>chlorides</th>
      <th>free_sulfur_dioxide</th>
      <th>total_sulfur_dioxide</th>
      <th>density</th>
      <th>ph</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>111</th>
      <td>7.2</td>
      <td>0.27</td>
      <td>0.46</td>
      <td>18.75</td>
      <td>0.052</td>
      <td>45.0</td>
      <td>255.0</td>
      <td>1.0000</td>
      <td>3.04</td>
      <td>0.52</td>
      <td>8.9</td>
      <td>5</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>227</th>
      <td>7.1</td>
      <td>0.25</td>
      <td>0.32</td>
      <td>10.30</td>
      <td>0.041</td>
      <td>66.0</td>
      <td>272.0</td>
      <td>0.9969</td>
      <td>3.17</td>
      <td>0.52</td>
      <td>9.1</td>
      <td>6</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>245</th>
      <td>6.1</td>
      <td>0.18</td>
      <td>0.36</td>
      <td>2.00</td>
      <td>0.038</td>
      <td>20.0</td>
      <td>249.5</td>
      <td>0.9923</td>
      <td>3.37</td>
      <td>0.79</td>
      <td>11.3</td>
      <td>6</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="pyod">
<h4>PyOD<a class="headerlink" href="#pyod" title="Permalink to this headline">#</a></h4>
<p>Due to the limitations of Scikit-learn is anomaly detection task, we are going to use PyOD, a library with more than 40 algorithms (mainly unsupervised). It uses the same interface withy Scikit-learn so that Data Scientist can feel familiar with it, but the authors <a class="reference external" href="https://pyod.readthedocs.io/en/latest/issues.html">don’t recommend</a> using this library with Scikit-learn. With a pre-selected <code class="docutils literal notranslate"><span class="pre">contamination</span></code>, all algorithms aim to calculate a <em>raw anomaly scores</em> via the method <a class="reference external" href="https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.decision_function"><code class="docutils literal notranslate"><span class="pre">decision_function()</span></code></a> and then convert them into probabilities via the method <a class="reference external" href="https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_proba"><code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code></a>.</p>
<p>To demonstrate the usage, we run an Isolation Forest algorithm and visualize the results. The list of algorithms can be found <a class="reference external" href="https://pyod.readthedocs.io/en/latest/index.html#implemented-algorithms">here</a>, devided into many categories such as linear, probabilistic and neighborhood-based. In the next sections, we learn these categories with the signature algorithms of each, how they calculate anomaly scores.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">detector</span> <span class="o">=</span> <span class="n">IForest</span><span class="p">(</span><span class="n">contamination</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">detector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dfWine</span><span class="p">)</span>

<span class="n">scoreOutlier</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">dfWine</span><span class="p">)</span>
<span class="n">probOutlier</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">dfWine</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">flagOutlier</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dfWine</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">scoreOutlier</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">detector</span><span class="o">.</span><span class="n">threshold_</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Anomaly Score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of data points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/pyod-anomaly-detection_15_0.png"><img alt="../_images/pyod-anomaly-detection_15_0.png" class="align-center" src="../_images/pyod-anomaly-detection_15_0.png" style="width: 498.5px; height: 346.5px;" /></a>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reducer</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">reducer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dfWine</span><span class="p">)</span>
<span class="n">projection</span> <span class="o">=</span> <span class="n">reducer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dfWine</span><span class="p">)</span>

<span class="n">dfProjection</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">projection</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC2&#39;</span><span class="p">])</span>
<span class="n">dfProjection</span> <span class="o">=</span> <span class="n">dfProjection</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">flagOutlier</span><span class="o">=</span><span class="n">flagOutlier</span><span class="p">,</span> <span class="n">probOutlier</span><span class="o">=</span><span class="n">probOutlier</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">dfProjection</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;PC2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;probOutlier&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s1">&#39;probOutlier&#39;</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/pyod-anomaly-detection_16_0.png"><img alt="../_images/pyod-anomaly-detection_16_0.png" class="align-center" src="../_images/pyod-anomaly-detection_16_0.png" style="width: 498.5px; height: 343.5px;" /></a>
</div>
</div>
</section>
</section>
</section>
<section id="linear-models">
<h2>2. Linear models<a class="headerlink" href="#linear-models" title="Permalink to this headline">#</a></h2>
<p>These algorithms rely on linear models to compute outlier scores.</p>
<section id="pca">
<h3>2.1. PCA<a class="headerlink" href="#pca" title="Permalink to this headline">#</a></h3>
<p>As far as we know, <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> is a technique which breaks down a matrix into vectors called principal components, then reconstruct the original data using only the first few most important components. This low dimensional space capture most variance in the data, meaning reconstructed points are very close to the principal components. However, as outliers differ from normal data, this statement may not be true. This insight is the key idea behind applying PCA in outlier detection: the outlier score for an observation will be obtained as the total of projected distances from that sample to all eigenvectors.</p>
<p>The implementation of this algorithm <a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA"><code class="docutils literal notranslate"><span class="pre">PCA</span></code></a> has the following hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_selected_components</span></code>: the number of principal components involving in calculating outlier scores, defaults to <em>None</em> (use all components).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weighted</span></code>: whether to use weighted sum in outlier score, where eigenvalues are the weights for their corresponding eigenvectors. Defaults to <em>True</em>.</p></li>
</ul>
</section>
<section id="one-class-svm">
<h3>2.2. One-class SVM<a class="headerlink" href="#one-class-svm" title="Permalink to this headline">#</a></h3>
<p>In classification, <a class="reference external" href="https://en.wikipedia.org/wiki/Support_vector_machine">SVM</a> is originally designed for binary problems; the main idea is to learn a <em>hyperplane</em> that is able to separates two classes <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(-1\)</span> with maximum margin. If we treat the dataset as a single class, then SVM can be used to learn a function assigning <span class="math notranslate nohighlight">\(1\)</span> to a small region that captures most of the data points and <span class="math notranslate nohighlight">\(-1\)</span> elsewhere (outlier). This function only works if it was trained on normal data only - no outlier included, so this is considered a novelty detection method rather than outlier detection. There are two algorithms using this approach:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.675.575&amp;rep=rep1&amp;type=pdf">OCSVM</a> (One-Class SVM), aiming to learn a <em>hyperplane</em> separating <em>most</em> data point from the origin with maximum margin.</p></li>
<li><p><a class="reference external" href="https://link.springer.com/content/pdf/10.1023/B:MACH.0000008084.60811.49.pdf">SVDD</a> (Support Vector Data Description), aiming to learn a <em>hypersphere</em> encompassing <em>most</em> data points with minimum volume.</p></li>
</ul>
<p>The PyOD class <a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM"><code class="docutils literal notranslate"><span class="pre">OCSVM</span></code></a> implements the first algorithm with the following hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nu</span></code>: the parameter <span class="math notranslate nohighlight">\(\nu\)</span> maintaining the allowance of misclassification (soft margin), being equivalant to <span class="math notranslate nohighlight">\(C\)</span> in original SVM. Defaults to <em>0.5</em>, must be between <em>0</em> and <em>1</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel</span></code>, <code class="docutils literal notranslate"><span class="pre">gamma</span></code>, <code class="docutils literal notranslate"><span class="pre">coef0</span></code> and <code class="docutils literal notranslate"><span class="pre">degree</span></code>: parameters of the kernel function, same usage as in original SVM.</p></li>
</ul>
</section>
</section>
<section id="proximity-based">
<h2>3. Proximity-based<a class="headerlink" href="#proximity-based" title="Permalink to this headline">#</a></h2>
<p>Also known as neighborhood-based, this approach assumes that outliers are far away from their neighbors.</p>
<section id="knn">
<h3>3.1. KNN<a class="headerlink" href="#knn" title="Permalink to this headline">#</a></h3>
<p>The idea is very simple: aggregating distances from a data point to its <span class="math notranslate nohighlight">\(K\)</span> nearest neighbors to get outlier score. The class <a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN"><code class="docutils literal notranslate"><span class="pre">KNN</span></code></a> has the following hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>: the number of neighbors considered, defaults to <em>5</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">method</span></code>: the aggregate function to summarize outlier scores, defaults to <em>largest</em>. Other options are <em>mean</em> and <em>median</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">metric</span></code>: the metric for distance computation. Defaults to <em>minkowski</em>, can also accepts many other metrics.</p></li>
</ul>
</section>
<section id="clustering-based">
<h3>3.2. Clustering-based<a class="headerlink" href="#clustering-based" title="Permalink to this headline">#</a></h3>
<p>Not only DBSCAN, but any clustering algorithm can be utilized for anomaly detection - this is the main idea behind <a class="reference external" href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=d8021847ed0e3f26b53416c2a254a85451ee5f1e">CBLOF</a> (Clustering Based Local Outlier Factor). Let’s say a clustering algorithm returns <span class="math notranslate nohighlight">\(K\)</span> clusters for our dataset
<span class="math notranslate nohighlight">\(\text{C}=\{\mathcal{C}_1,\mathcal{C}_2,\dots,\mathcal{C}_K\}\)</span>
(sorted by decreasing cluster size), CBLOF aims to split them into small and large clusters. We denoted
<span class="math notranslate nohighlight">\(\text{LC}=\{\mathcal{C}_1,\mathcal{C}_2,\dots,\mathcal{C}_B\}\)</span>
the set of large clusters and
<span class="math notranslate nohighlight">\(\text{SC}=\{\mathcal{C}_{B+1},\mathcal{C}_{B+2},\dots,\mathcal{C}_K\}\)</span>
the set of small clusters. With the introduction of two parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, the determination of <span class="math notranslate nohighlight">\(\text{SC}\)</span> and <span class="math notranslate nohighlight">\(\text{LC}\)</span> must satisfy one of the two conditions:</p>
<ul class="simple">
<li><p>The ratio of observations in large clusters is at least <span class="math notranslate nohighlight">\(\alpha\)</span> (usually <span class="math notranslate nohighlight">\(90\%\)</span>):
<span class="math notranslate nohighlight">\(|\mathcal{C}_1|+|\mathcal{C}_2|+\ldots+|\mathcal{C}_B|\geq\alpha N\)</span></p></li>
<li><p>All large clusters must be at least <span class="math notranslate nohighlight">\(\beta\)</span> (usually <span class="math notranslate nohighlight">\(5\)</span>) times larger than small clusters:
<span class="math notranslate nohighlight">\(|\mathcal{C}_{B}|\geq\beta|\mathcal{C}_{B+}|\)</span></p></li>
</ul>
<p>The anomaly score for a sample is then computed as the product of (1) the distance between the sample and the nearest centroid of a large cluster and (2) the size of the cluster it belongs to. The intuition behind the first term is we are viewing small clusters as groups of potential outliers, making their centroids carry no meaning. As the result, we use centroids of large clusters instead, but this distance will be very high for samples in small clusters. This leads us to the multiplication with the second term, it can be thought as a compensation for small clusters.</p>
<p>This algorithm is implemented via the class <a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF"><code class="docutils literal notranslate"><span class="pre">CBLOF</span></code></a> with the hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">clustering_estimator</span></code>: the clustering algorithm to be used, defaults to <em>None</em> (using K-Means). It accepts an instance returned by Scikit-learn clustering classes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_clusters</span></code>: the number of clusters, defaults to <em>8</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code>: the value of <span class="math notranslate nohighlight">\(\alpha\)</span>, defaults to <em>0.9</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">beta</span></code>: the value of <span class="math notranslate nohighlight">\(\beta\)</span>, defaults to <em>5</em>.</p></li>
</ul>
</section>
<section id="local-outlier-factor">
<h3>3.3. Local Outlier Factor<a class="headerlink" href="#local-outlier-factor" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof">LOF</a> is widely uses in anomaly detection specially for local outlier. It computes the local density deviation of a given data point with respect to its neighbors. A point will be considered as outlier when it has a significantly lower density than it neighbors. In other word, LOF compares the local density of a point to local density of its k-nearest neighbors and gives a score as final output. The disadvantage of LOF or proximity-based algorithms is it costs time very much to calculate the distance between large data points.</p>
<p>The process of LOF follow these step:</p>
<ul class="simple">
<li><p>Determined the distance from data point <span class="math notranslate nohighlight">\(p_i\)</span> to <span class="math notranslate nohighlight">\(k_{th}\)</span> nearest neighbors (pyod support all distance metrics from sklearn and scipy). Get the max distance among <span class="math notranslate nohighlight">\(k\)</span> points - this is called <em>K_distance</em>. The number of neighbors of <span class="math notranslate nohighlight">\(p_i\)</span> can greater or equal <span class="math notranslate nohighlight">\(k\)</span> due to the distance between them - denote <span class="math notranslate nohighlight">\(|N_p|\)</span></p></li>
<li><p>Computes the <em>reachability density (RD)</em> of each <span class="math notranslate nohighlight">\(p_i\)</span> related to others. RD is defined as the maximum of K-distance of <span class="math notranslate nohighlight">\(p_i\)</span> and the distance between <span class="math notranslate nohighlight">\(p_i\)</span> and <span class="math notranslate nohighlight">\(p_j\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{RD}(p_i,p_j) = \max(\text{K_distance}_{p_i}, d(p_i,p_j))\]</div>
<a class="reference internal image-reference" href="../_images/local_outlier_factor.png"><img alt="../_images/local_outlier_factor.png" class="align-center" src="../_images/local_outlier_factor.png" style="height: 250px;" /></a>
<br>
<ul class="simple">
<li><p>Computes the <em>local reachability density (LRD)</em>. LRD is inverse of the average RD of <span class="math notranslate nohighlight">\(p_i\)</span> from its neighbors. The larger average RD leads to the smaller LRD - it means the density of <span class="math notranslate nohighlight">\(p_i\)</span> is quite low:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{LRD}_{p_i}= \frac{1}{\sum_{p_j \in N_p}\frac{\text{RD}(p_i,p_j)}{|N_p|} } \]</div>
<ul class="simple">
<li><p>Calculates the <em>LOF score</em> for each <span class="math notranslate nohighlight">\(p_i\)</span> - LOF score is the ratio of the average LRD of the <span class="math notranslate nohighlight">\(K\)</span> neighbors of <span class="math notranslate nohighlight">\(p_i\)</span> to the LRD of <span class="math notranslate nohighlight">\(p_i\)</span>. If a point is inliner, the LRD of this point is approximately equal to its neighbors that leads to LOF is nearly equal to 1. On the other hand, if the point is an outlier, the LRD of a point is less than the average LRD of neighbors, then LOF value will be high:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{LOF}_{p_i} = \frac{\sum_{p_j \in N_p} \text{LRD}_{p_j}}{|N_p|} \cdot \frac{1}{\text{LRD}_{p_i}}\]</div>
<section id="connectivity-based">
<h4>Connectivity-based<a class="headerlink" href="#connectivity-based" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof">COF</a> (Connectivity-based Outlier Factor) is another version of LOF. In LOF, the theory is that data points are distributed in circle around the instance, but in the case there is a linear relationship between data points, the distance metric in LOF is no longer correct. COF calculates the anomaly score based on average <em>chain distance</em> between points and their neighbors. Therefore, COF is suitable for local and dependency outliers but like LOF, the time cosuming of COF is quite high with large dataset.</p>
<p>As same as LOF, COF firstly find <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors of point <span class="math notranslate nohighlight">\(p\)</span> then arrange them in order of closest distance to <span class="math notranslate nohighlight">\(p\)</span>. Call <span class="math notranslate nohighlight">\(e_k\)</span> is the <em>edge distance</em>, equals to each pair of points distance, example <span class="math notranslate nohighlight">\(e_2\)</span> is the distance between <span class="math notranslate nohighlight">\(P_2\)</span> and <span class="math notranslate nohighlight">\(P_3\)</span>, we calculates average chain distance for each instance:</p>
<div class="math notranslate nohighlight">
\[\text{ACD}_p = \sum_{i=1}^k \frac{2(k+1-i)}{k(k+1)} e_i\]</div>
<a class="reference internal image-reference" href="../_images/connectivity_based_outlier_factor.png"><img alt="../_images/connectivity_based_outlier_factor.png" class="align-center" src="../_images/connectivity_based_outlier_factor.png" style="height: 180px;" /></a>
<br>
<p>At last, anomaly score is generated by ratio of average chaining distance of instance and the average of average chaining distance of <span class="math notranslate nohighlight">\(k\)</span> nearest neighbor of this point. The higher the score, the easier it is to be an outlier.</p>
<div class="math notranslate nohighlight">
\[\text{COF}_p = \frac{\text{ACD}_p}{\frac{\sum ACD_k}{k}}\]</div>
</section>
</section>
</section>
<section id="probabilistic">
<h2>4. Probabilistic<a class="headerlink" href="#probabilistic" title="Permalink to this headline">#</a></h2>
<p>This method constructs the empirical distribution of the dataset then predict the tail probabilities of each given data point to determine its level of <em>extremeness</em>.</p>
<section id="w-4-1-ecod">
<h3>(w) 4.1. ECOD<a class="headerlink" href="#w-4-1-ecod" title="Permalink to this headline">#</a></h3>
</section>
<section id="copula-based">
<h3>4.2. COPula-based<a class="headerlink" href="#copula-based" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD">COPOD</a> (COPula-based Outlier Detection) is a parameter-free algorithm, which is built based on empirical multivariate distribution. Beside of no hyperparameters, it is also fast and computationally efficient, suitable for high-himensional data. The math behind this algorithm is quite complicated, simply understand that COPOD constructs the multivariate cumulative distribution function, and then uses it to predict tail probabilities of each given data point to determine its level of extremeness. The score in this case is not outlier probability but measuring the likelihood of a row, relative to other points in the dataset.</p>
</section>
<section id="histogram-based">
<h3>4.3. Histogram-based<a class="headerlink" href="#histogram-based" title="Permalink to this headline">#</a></h3>
<p>The idea of HBOS (Histogram-based Outlier Score) is based on assumption that the variables are independence of each other. HBOS is constructing a histogram for each variable, then calculating the <em>univariate outlier score</em> for each observation and finally sum up to measure the outlier score of an observation. HBOS is suitable for global and clustered outliers, it also has very fast computing time - especially for large dataset.</p>
<p>Firstly, the algorithm will create histogram of each variable <span class="math notranslate nohighlight">\(d\)</span>. If a feature is numerical, bin it to get its histogram, if it is categorical, just count the values of each category. The height of bins (<span class="math notranslate nohighlight">\(h_d\)</span>) which data point belongs is the measure of outlier-ness. To get higher value for outlier, we inverse <span class="math notranslate nohighlight">\(h_d\)</span> and standardize score by log transform to ensures all the univariate scores can be summed up with equal weight. The formula of outlier score of a point <span class="math notranslate nohighlight">\(p\)</span> is very simply:</p>
<div class="math notranslate nohighlight">
\[\text{HBOS}_p = \sum_{d=0}^D \log(\frac{1}{h_d})\]</div>
<p>The outliers will have a higher score than inliers points and these will be labelled 1 due to the outlier threshold we set. The challenge of HBOS is, it can be sensitive to the bin width of the histogram. If the bins are too narrow, the normal data points falling in these bins will be identified as outliers and vice versa. To produce a model with a stable outcome, the strategy is to build HBOS models with a range of histogram widths to obtain multiple scores and then aggregate the scores (by average or max average). This approach will reduce  overfitting and increase prediction accuracy.</p>
</section>
</section>
<section id="black-box">
<h2>5. Black-box<a class="headerlink" href="#black-box" title="Permalink to this headline">#</a></h2>
<section id="isolation-forest">
<h3>5.1. Isolation Forest<a class="headerlink" href="#isolation-forest" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest">Isolation Forest</a> detects the outliers based on ensembling binary decision trees to isolate outliers from the others. Relying on the characteristics of outliers are few and difference, IForest built each tree using sub-sample of dataset, then randomly seleted a feature and a random threshold to split the tree. The process of splitting continue until all instance has been isolated or the tree reach the maximum height or all same-value data points go into same node. The outliers will have short path to the root than others, especially when all tree in the forest say that. Formula of anomaly score depends on the average of path length to the root <span class="math notranslate nohighlight">\(\overline{h_p}\)</span>, number of instances in node - <span class="math notranslate nohighlight">\(n\)</span> and unsuccesfull path in binary search tree <span class="math notranslate nohighlight">\(c(n)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{iForest}_p &amp;= 2^{-\frac{\overline{h}_p}{c(n)}} \\
c(n) &amp;= 2 \cdot (\log(n-1)+0.577) - \frac{2(n-1)}{n} \\
\end{aligned}\end{split}\]</div>
<p>Iforest just requires 2 params which are number of trees and sub-sample size - it works very well with small sample sizes and high-dimensional data, time consuming of this method is also fast and it can apply for all 4 types of anomaly. But iForest also has a disadvantage that a node in an iTree is split based on a threshold value, the data is split into left and right branches resulting in horizontal and vertical branch cuts - this will lead to some outliers are passed.</p>
</section>
<section id="w-5-2-autoencoder">
<h3>(w) 5.2. Autoencoder<a class="headerlink" href="#w-5-2-autoencoder" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/openvinotoolkit/anomalib">https://github.com/openvinotoolkit/anomalib</a></p></li>
<li><p><a class="reference external" href="https://github.com/SeldonIO/alibi-detect">https://github.com/SeldonIO/alibi-detect</a></p></li>
<li><p><a class="reference external" href="https://github.com/vc1492a/PyNomaly">https://github.com/vc1492a/PyNomaly</a></p></li>
<li><p><a class="reference external" href="https://madm.dfki.de/_media/theses/bachelorthesis-amer_2011.pdf">https://madm.dfki.de/_media/theses/bachelorthesis-amer_2011.pdf</a></p></li>
<li><p><a class="reference external" href="https://www.goldiges.de/publications/Anomaly_Detection_Algorithms_for_RapidMiner.pdf">https://www.goldiges.de/publications/Anomaly_Detection_Algorithms_for_RapidMiner.pdf</a></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/313857/why-one-class-svm-seperate-from-the-origin">https://stats.stackexchange.com/questions/313857/why-one-class-svm-seperate-from-the-origin</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843">https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843</a></p></li>
<li><p><a class="reference external" href="https://medium.com/dataman-in-ai/anomaly-detection-with-histogram-based-outlier-detection-hbo-bc10ef52f23f">https://medium.com/dataman-in-ai/anomaly-detection-with-histogram-based-outlier-detection-hbo-bc10ef52f23f</a></p></li>
<li><p><a class="reference external" href="https://www.goldiges.de/publications/HBOS-KI-2012.pdf">https://www.goldiges.de/publications/HBOS-KI-2012.pdf</a></p></li>
<li><p><a class="reference external" href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest">https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2009.09463.pdf">https://arxiv.org/pdf/2009.09463.pdf</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/fast-accurate-anomaly-detection-based-on-copulas-copod-3133ce9041fa">https://towardsdatascience.com/fast-accurate-anomaly-detection-based-on-copulas-copod-3133ce9041fa</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "hungpq7/tabular-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./09-unsupervised-learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="sklearn-dimensional-reduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Sklearn: Dimensional Reduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../10-deep-learning/_intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><b>10. Deep Learning</b></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Quang Hung &#9829; Thuy Linh<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>