{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras: Recurrent Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sspipe import p, px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recurrent layers\n",
    "[Recurrent Neural Network] (RNN) is a class of neural network architectures where nodes in a layers have internal connections, allowing to express temporal behaviour. There are many types of RNN layers, but they all share the same architecture. The image below shows the information flow for an observation, or for a document in the context of NLP.\n",
    "\n",
    ":::{image} ../image/rnn_general.png\n",
    ":height: 175px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "Each green cell $\\mathbf{x}_t\\in\\mathbb{R}^{V\\times1}$ represents the embedding vector of a token, and each blue cell $\\mathbf{h}_t\\in\\mathbb{R}^{D\\times1}$ represents an output vector. With the input sequence size fixed at $T$, RNN adjusts itself to match the input length. The most important part of a RNN layer is the grey cell $A$ that repeats multiple times, being account for information processing. We can see that at a time step, the output value $\\mathbf{h}_t$ is influenced by all previous steps $\\mathbf{h}_{t-1},\\mathbf{h}_{t-2},\\dots$, besides the input $\\mathbf{x}_t$. This design resembles *memory* and enables RNN to capture sequential relationship.\n",
    "\n",
    "There are many architectures for a recurrent layers, the only difference between them is how the cell $A$ being desgined. In this article, we are going to learn the cell architectures of Simple RNN, LSTM and GRU.\n",
    "\n",
    "[Recurrent Neural Network]: https://en.wikipedia.org/wiki/Recurrent_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Building blocks\n",
    "This section introduces common blocks in recurrent architectures. Knowing each of them separately helps us understanding compicated designs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation\n",
    "\n",
    ":::{image} ../image/rnn_concatenation.png\n",
    ":height: 100px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "Let's say we want to transform two input vectors $\\mathbf{u}\\in\\mathbb{R}^{U\\times1}$ and $\\mathbf{v}\\in\\mathbb{R}^{V\\times1}$ into $\\mathbf{y}\\in\\mathbb{R}^{D\\times1}$. Note that $U$ and $V$ are fixed dimensionalities of input, while $D$ is the desired output size. With weight matrices\n",
    "$\\mathbf{W}_{yu}\\in\\mathbb{R}^{D\\times U},\\mathbf{W}_{yv}\\in\\mathbb{R}^{D\\times V}$\n",
    "and bias vector $\\mathbf{b}_y\\in\\mathbb{R}^{D\\times1}$,\n",
    "the actual formula behind the above image is:\n",
    "\n",
    "$$\\mathbf{y}=\\mathbf{W}_{yu}\\mathbf{u}+\\mathbf{W}_{yv}\\mathbf{v}+\\mathbf{b}_y$$\n",
    "\n",
    "Here, all three terms have size $(D\\times1)$, same as $\\mathbf{y}$. We can also view the above formula as concatenating $\\mathbf{u}$ and $\\mathbf{v}$ into a single input vector $\\mathbf{x}\\in\\mathbb{R}^{(U+V)\\times1}$, then scale it using a bigger weight matrix $\\mathbf{W}_{yx}\\in\\mathbb{R}^{D\\times(U+V)}$. This explains why the formula is visualized as a concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gate\n",
    "\n",
    ":::{image} ../image/rnn_gate.png\n",
    ":height: 80px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "A gate consists of two calculation steps, (1) passing a vector into sigmoid function and (2) using it as a percentage multiplier. The sigmoid function (denoted $\\sigma$) is account for producing numbers in range $(0,1)$. We can see the purpose of gates very clearly here: they control how much information should be let through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "We call the vanilla architecture [Simple RNN] (1980s) to distinguish from the family name. Its cells is very simple, with only a concatenated value pass through an activation function. The activation function is usually $\\tanh$ which produces values within the range $(-1,1)$, so that the network will be able to express *sentiment*. The cell architecture is described in the image and formula as follows:\n",
    "\n",
    ":::{image} ../image/rnn_cell.png\n",
    ":height: 160px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "$$\\mathbf{h}_t=\\phi(\\mathbf{W}_{hx}\\mathbf{x}_t+\\mathbf{W}_{hh}\\mathbf{h}_{t-1}+\\mathbf{b}_h)$$\n",
    "\n",
    "A well-known issue with Simple RNN is that it only has *short-term memory*. This property is very easy to understand if you are familiar with the gradient vanishing problem of S-shaped activation functions. During [backpropagation through time] for a pair of words with large $\\Delta t$, the product of partial derivatives may trigger saturation zones of $\\tanh$, making the derivative of a word with respect to the other almost zero. As a result, Simple RNN fails to capture long-term memory.\n",
    "\n",
    "[Simple RNN]: https://en.wikipedia.org/wiki/Recurrent_neural_network#Fully_recurrent\n",
    "[backpropagation through time]: https://en.wikipedia.org/wiki/Backpropagation_through_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sizing\n",
    "$\\mathbf{W}_{hx},\\mathbf{W}_{hh}$ and $\\mathbf{b}_h$, as explained earlier, are the weight matrices and bias vector. Their corresponding sizes are $(D\\times V)$, $(D\\times D)$ and $(D\\times 1)$. Note that these parameters are used across cells, hence taking sum of their sizes gets us the total number of parameters need to be trained:\n",
    "\n",
    "$$D\\times(D+V+1)$$\n",
    "\n",
    "For example, we use a BERT pretrained model to encode a corpus containing $N=10\\,000$ documents. The embedding dimension is $V=512$ and documents are truncated to have $T=128$ tokens. Then a RNN layer with $D=50$ will require $50\\times(50+512+1)=28\\,150$ parameters to process such input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "[LSTM] (Long Short-Term Memory) is a recurrent architecture published in 1997 that does not suffer from the gradient vanishing/exploding problem. This means, the network is able to capture long-term memory. The image below illustrates the architecture of a LSTM *memory cell*.\n",
    "\n",
    ":::{image} ../image/lstm_cell.png\n",
    ":height: 320px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "The ability to manipulate information of LSTM is regulated via three gates (green node): forget $\\mathbf{f}_t$, input $\\mathbf{i}_t$ and output $\\mathbf{o}_t$. Recall that a gate is a Sigmoid function ($\\phi_1$ in this picture) followed by a multiplication. These gates give LSTM the ability to learn when to remember and when to forget, based on previous *hidden state* $\\mathbf{h}_{t-1}$ and current input data $\\mathbf{x}_t$. They all share the same formula, but with different parameters:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{f}_t &= \\phi_1(\\mathbf{W}_{fh}\\mathbf{h}_{t-1}+\\mathbf{W}_{fx}\\mathbf{x}_t+\\mathbf{b}_f) \\\\\n",
    "\\mathbf{i}_t &= \\phi_1(\\mathbf{W}_{ih}\\mathbf{h}_{t-1}+\\mathbf{W}_{ix}\\mathbf{x}_t+\\mathbf{b}_i) \\\\\n",
    "\\mathbf{o}_t &= \\phi_1(\\mathbf{W}_{oh}\\mathbf{h}_{t-1}+\\mathbf{W}_{ox}\\mathbf{x}_t+\\mathbf{b}_o) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The key component contributing to the long-term-memory ability of LSTM is the *internal state* $\\mathbf{c}_t$. This variable works like a conveyor belt running straight through the entire chain, gathering *additional* information $\\tilde{\\mathbf{c}}_t$ at each memory cell it goes through. Because new information is really mathematically added, we are assured the gradients can pass many times without vanishing or exploding. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\tilde{\\mathbf{c}}_t &= \\phi_2(\\mathbf{W}_{ch}\\mathbf{h}_{t-1}+\\mathbf{W}_{cx}\\mathbf{x}_t+\\mathbf{b}_c) \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t\\odot\\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot\\tilde{\\mathbf{c}}_t \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Finally, the current *hidden state* $\\mathbf{h}_t$ is computed by $\\tanh$-activating $\\mathbf{c}_t$ and scaling it down by the output gate $\\mathbf{o}_t$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t\\odot\\phi_2(\\mathbf{c}_t) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can see LSTM has 4 concatenations between $\\mathbf{x}$ and $\\mathbf{h}$, so its number of parameters will be $4\\times D\\times(D+V+1)$.\n",
    "\n",
    "[LSTM]: https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    ":::{image} ../image/lstm_steps.png\n",
    ":height: 450px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "Because it is quite hard to track what is going on by looking at the full architecture of LSTM, we will break it down into four steps:\n",
    "- Step 1, construct the *forget gate* that decides how much old data should be kept.\n",
    "- Step 2, construct the *input gate* that decides what new data should be stored.\n",
    "- Step 3, compute the *internal state* by combining two processes, *forgetting* and *receiving* information.\n",
    "- Step 4, construct the *output gate* and compute the output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "[Gated Recurrent Units] (GRU) was published in 2014 as an alternative that retains key idea of LSTM but is faster in computation.\n",
    "\n",
    "[Gated Recurrent Units]: https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
    "\n",
    ":::{image} ../image/gru_cell.png\n",
    ":height: 320px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "GRU comes with the two-gate mechanism, *reset* ($\\mathbf{r}_t$) and *update* ($\\mathbf{z}_t$), rather than three like LSTM. They are also based on previous *hidden state* $\\mathbf{h}_{t-1}$ and current input data $\\mathbf{x}_t$ like LSTM's gates.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{r}_t &= \\phi_1(\\mathbf{W}_{rh}\\mathbf{h}_{t-1}+\\mathbf{W}_{rx}\\mathbf{x}_t+\\mathbf{b}_r) \\\\\n",
    "\\mathbf{z}_t &= \\phi_1(\\mathbf{W}_{zh}\\mathbf{h}_{t-1}+\\mathbf{W}_{zx}\\mathbf{x}_t+\\mathbf{b}_z) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Next, the new information $\\tilde{\\mathbf{h}}_t$ is computed by two processes, *receiving* new information $\\mathbf{x}_t$ and *forgetting* old information $\\mathbf{h}_{t-1}$. Then, it is added to the old state weightedly (weights are controled via the update gate) to get the current hidden state $\\mathbf{h}_t$. Unlike LSTM, GRU does not maintain the *internal state* but resembles its behaviour.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\tilde{\\mathbf{h}}_t &= \\phi_2(\\mathbf{W}_{hx}\\mathbf{x}_t+\\mathbf{W}_{hh}(\\mathbf{r}_t\\odot\\mathbf{h}_{t-1})+\\mathbf{b}_h) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{z}_t\\odot\\mathbf{h}_{t-1}+(1-\\mathbf{z}_t)\\odot\\tilde{\\mathbf{h}}_{t-1} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The number of parameters of a GRU layer is $3\\times D\\times(D+V+1)$, as it has 3 concatenations of $\\mathbf{x}$ and $\\mathbf{h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    ":::{image} ../image/gru_steps.png\n",
    ":height: 450px\n",
    ":align: center\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Bi-directional\n",
    "All the recurrent layers have been introduced so far are uni-directional, i.e. going from the left to the right. In other words, our networks only model leftward context. This design works fine for time series data, but for text data, rightward context also matters. This problem is solved by generalizing recurrent layers to [bi-directional]. The architecture is described in the following image:\n",
    "\n",
    ":::{image} ../image/rnn_bidirectional.png\n",
    ":height: 245px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "We can observe that a bi-directional layer is composed of a foward sub-layer and a backward sub-layer. They can be of the same type or not, and their cells are denoted $A$ and $A'$ respectively. Their corresponding outputs $\\overrightarrow{\\mathbf{h}}_t$ and $\\overleftarrow{\\mathbf{h}}_t$ are then merged into $\\mathbf{h}_t$ using various strategies, where the most common one is concatenating.\n",
    "\n",
    "[bi-directional]: https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Implementation\n",
    "TensorFlow implements all three recurrent layers via [`SimpleRNN`], [`LSTM`] and [`GRU`]. Each has a cell-level counterpart that can be used with the abstract class [`RNN`]. There is also [`Bidirectional`] working as a wrapper around recurrent layers for bi-directional behaviour. They have the following hyperparameters:\n",
    "- `units`: the dimensionality of output space ($D$).\n",
    "- `activation`: the activation function for processing data ($\\phi$ in Simple RNN and $\\phi_2$ in LSTM, GRU), defaults to *tanh*.\n",
    "- `recurrent_activation`: the activation function for gates ($\\phi_1$, only available for LSTM and GRU), defaults to *sigmoid*.\n",
    "\n",
    "[`SimpleRNN`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN\n",
    "[`LSTM`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "[`GRU`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "[`RNN`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\n",
    "[`Bidirectional`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sspipe import p, px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:54:45.257865Z",
     "iopub.status.busy": "2022-12-17T16:54:45.257460Z",
     "iopub.status.idle": "2022-12-17T16:54:45.274689Z",
     "shell.execute_reply": "2022-12-17T16:54:45.273178Z",
     "shell.execute_reply.started": "2022-12-17T16:54:45.257841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfSpam = pd.read_csv('../data/spam_message.csv')\n",
    "xTrainRaw, xTestRaw, yTrain, yTest = train_test_split(dfSpam.content, dfSpam.spam, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:54:45.276935Z",
     "iopub.status.busy": "2022-12-17T16:54:45.276538Z",
     "iopub.status.idle": "2022-12-17T16:55:55.791196Z",
     "shell.execute_reply": "2022-12-17T16:55:55.790387Z",
     "shell.execute_reply.started": "2022-12-17T16:54:45.276896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_feature(corpus):\n",
    "    bertProcessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "    bertEncoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/2')\n",
    "    corpus = corpus.str.lower()\n",
    "    corpus = bertProcessor(corpus)\n",
    "    corpus = bertEncoder(corpus)\n",
    "    corpus = corpus['sequence_output']\n",
    "    return corpus\n",
    "    \n",
    "xTrain = process_feature(xTrainRaw)\n",
    "xTest = process_feature(xTestRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:07:22.161985Z",
     "iopub.status.busy": "2022-12-17T03:07:22.161562Z",
     "iopub.status.idle": "2022-12-17T03:07:22.170452Z",
     "shell.execute_reply": "2022-12-17T03:07:22.169486Z",
     "shell.execute_reply.started": "2022-12-17T03:07:22.161941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 128, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:18:51.161934Z",
     "iopub.status.busy": "2022-12-17T03:18:51.161550Z",
     "iopub.status.idle": "2022-12-17T03:18:51.595588Z",
     "shell.execute_reply": "2022-12-17T03:18:51.594860Z",
     "shell.execute_reply.started": "2022-12-17T03:18:51.161911Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.SimpleRNN(16)(xTrain).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:19:05.115829Z",
     "iopub.status.busy": "2022-12-17T03:19:05.115469Z",
     "iopub.status.idle": "2022-12-17T03:19:05.455679Z",
     "shell.execute_reply": "2022-12-17T03:19:05.454843Z",
     "shell.execute_reply.started": "2022-12-17T03:19:05.115805Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 128, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.SimpleRNN(16, return_sequences=True)(xTrain).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "When using RNN with other layers, there are two cases:\n",
    "- The next layer being Fully Connected, then we only use the last hidden state, $\\mathbf{h}_T$. The output shape in this case is  $(N\\times D)$.\n",
    "- The next layer being another RNN layer (including LSTM and GRU), then we need to return the full sequence $\\mathbf{h}_1,\\mathbf{h}_2,\\dots,\\mathbf{h}_T$. This is done by specifying `return_sequences=True`. The output shape this time is $(N\\times T\\times D)$.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:11.976104Z",
     "iopub.status.busy": "2022-12-17T16:56:11.975625Z",
     "iopub.status.idle": "2022-12-17T16:56:16.846199Z",
     "shell.execute_reply": "2022-12-17T16:56:16.845320Z",
     "shell.execute_reply.started": "2022-12-17T16:56:11.976074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 5s 25ms/step - loss: 0.0793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9134fe7f40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.SimpleRNN(10, return_sequences=True),\n",
    "    layers.SimpleRNN(5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:25.009641Z",
     "iopub.status.busy": "2022-12-17T16:56:25.009088Z",
     "iopub.status.idle": "2022-12-17T16:56:25.031197Z",
     "shell.execute_reply": "2022-12-17T16:56:25.030317Z",
     "shell.execute_reply.started": "2022-12-17T16:56:25.009597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 128, 10)           2670      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 5)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,756\n",
      "Trainable params: 2,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:37.708607Z",
     "iopub.status.busy": "2022-12-17T16:56:37.708278Z",
     "iopub.status.idle": "2022-12-17T16:56:38.266645Z",
     "shell.execute_reply": "2022-12-17T16:56:38.266043Z",
     "shell.execute_reply.started": "2022-12-17T16:56:37.708585Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789832918592251"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTestPred = model.predict(xTest)\n",
    "AUC(yTest, yTestPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:58:19.396320Z",
     "iopub.status.busy": "2022-12-17T16:58:19.395930Z",
     "iopub.status.idle": "2022-12-17T16:58:19.401454Z",
     "shell.execute_reply": "2022-12-17T16:58:19.400422Z",
     "shell.execute_reply.started": "2022-12-17T16:58:19.396295Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 10)\n",
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "_ = [print(weight.shape) for weight in model.layers[0].weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([\n",
    "    [1,2,3,4,],\n",
    "    [0,4,1,1,],\n",
    "])\n",
    "y = tf.constant([0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 221ms/step - loss: 0.5121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb544b3f70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=10, output_dim=20),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(loss='mse')\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_19 (Embedding)    (None, None, 20)          200       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 1)           21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Seq2Seq\n",
    "[Seq2Seq] (Sequence-to-Sequence) is a Deep Learning architecture published in 2014 mainly for [neural machine translation] and [question answering]. It aims to transform an input sequence to a new one, both can be of arbitrary lengths. Its design is very natural to translation problems, and has resolved the fixed-length constraint in standalone recurrent layers.\n",
    "\n",
    "[Seq2Seq]: https://en.wikipedia.org/wiki/Seq2seq\n",
    "[neural machine translation]: https://en.wikipedia.org/wiki/Neural_machine_translation\n",
    "[question answering]: https://en.wikipedia.org/wiki/Question_answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "The Seq2Seq uses an encoder-decoder architecture, in which each process uses a recurrent layer(s) such as LSTM and GRU. With $(\\mathbf{x}_1,\\dots,\\mathbf{x}_M)$ a source sentence and $(\\mathbf{y}_1,\\dots,\\mathbf{y}_N)$ the target sentence, the architecture of Seq2Seq is described in the following image:\n",
    "\n",
    "The *encoder* is simply a recurrent layer where the hidden state at a time step is expressed by $(\\mathbf{x}_m,\\mathbf{e}_{m-1})\\mapsto\\mathbf{e}_m$. The last hidden state will be used as the output, known under the name *context vector* $\\mathbf{c}=\\mathbf{e}_M$ (sometimes called *sentence embedding* or *thought vector*).\n",
    "\n",
    "The *decoder* is a bit different, in which the hidden state has the signature $(\\mathbf{y}_{n-1},\\mathbf{d}_{n-1})\\mapsto\\mathbf{d}_n$. Note that each hidden state $\\mathbf{d}_n$ must be computed using previous target token $\\mathbf{y}_{n-1}$ to prevent target leakage. We also include $\\mathbf{c}$ as the initialization state.\n",
    "\n",
    "Because our goal is to take string input to predict string output, pre-trained embedding will not be used, at least for target text. Therefore, an embedding layer is used to process data in both encoder and decoder. This layer turns each word into a vector, just like Word2Vec or BERT do, but all embedding values are treated learnable parameters by our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T15:58:20.904617Z",
     "iopub.status.busy": "2023-01-01T15:58:20.904038Z",
     "iopub.status.idle": "2023-01-01T15:58:20.909486Z",
     "shell.execute_reply": "2023-01-01T15:58:20.908329Z",
     "shell.execute_reply.started": "2023-01-01T15:58:20.904583Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sspipe import p, px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T16:01:10.847510Z",
     "iopub.status.busy": "2023-01-01T16:01:10.847062Z",
     "iopub.status.idle": "2023-01-01T16:01:10.857860Z",
     "shell.execute_reply": "2023-01-01T16:01:10.856629Z",
     "shell.execute_reply.started": "2023-01-01T16:01:10.847469Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_addons.seq2seq.basic_decoder.BasicDecoder at 0x7fa012776fa0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.seq2seq.BasicDecoder(cell=layers.LSTMCell(4), sampler=tfa.seq2seq.TrainingSampler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding()\n",
    "    \n",
    "    def call(self, x):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Attention\n",
    "[Attention] implement [`Attention`]\n",
    "\n",
    "[Attention]: https://en.wikipedia.org/wiki/Attention_(machine_learning)\n",
    "[`Attention`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transformer\n",
    "[Transformer]\n",
    "\n",
    "[Transformer]: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- *bioinf.jku.at - [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)*\n",
    "- *arxiv.org - [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](https://arxiv.org/pdf/1409.1259.pdf)*\n",
    "- *arxiv.org - [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215v3.pdf)*\n",
    "- *arxiv.org - [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)*\n",
    "- *arxiv.org - [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)*\n",
    "- *arxiv.org - [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)*\n",
    "- *amitness.com - [Recurrent Keras layer](https://amitness.com/2020/04/recurrent-layers-keras/)*\n",
    "- *colah.github.io - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*\n",
    "- *d2l.ai - [Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)*\n",
    "- *d2l.ai - [Long Short-Term Memory](https://d2l.ai/chapter_recurrent-modern/lstm.html)*\n",
    "- *d2l.ai - [Gated Recurrent Units](https://d2l.ai/chapter_recurrent-modern/gru.html)*\n",
    "- *distill.pub - [Memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)*\n",
    "- *distill.pub - [Augumented RNNs](https://distill.pub/2016/augmented-rnns/)*\n",
    "- *lilianweng.github.io - [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)*\n",
    "- *blog.floydhub.com - [Attention mechanism](https://blog.floydhub.com/attention-mechanism/amp/)*\n",
    "---\n",
    "- https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "- https://github.com/bentrevett/pytorch-seq2seq\n",
    "- https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b\n",
    "- https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263\n",
    "- https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f\n",
    "- https://storrs.io/attention/amp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-02T05:11:48.788204Z",
     "iopub.status.busy": "2023-01-02T05:11:48.787827Z",
     "iopub.status.idle": "2023-01-02T05:11:49.495375Z",
     "shell.execute_reply": "2023-01-02T05:11:49.493277Z",
     "shell.execute_reply.started": "2023-01-02T05:11:48.788181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 22.3.1 from /opt/anaconda3/lib/python3.8/site-packages/pip (python 3.8)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-addons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
