{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras: Recurrent Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sspipe import p, px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recurrent layers\n",
    "[Recurrent Neural Network] (RNN) is a class of neural network architectures where nodes in a layers have internal connections, allowing to express temporal behaviour. There are many types of RNN layers, but they all share the same architecture. The image below shows the information flow for an observation, or for a document in the context of NLP.\n",
    "\n",
    ":::{image} ../image/rnn_general.png\n",
    ":height: 175px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "Each green cell $\\mathbf{x}_t\\in\\mathbb{R}^{V\\times1}$ represents the embedding vector of a token, and each blue cell $\\mathbf{h}_t\\in\\mathbb{R}^{D\\times1}$ represents an output vector. With the input sequence size fixed at $T$, RNN adjusts itself to match the input length. The most important part of a RNN layer is the grey cell $A$ that repeats multiple times, being account for information processing. We can see that at a time step, the output value $\\mathbf{h}_t$ is influenced by all previous steps $\\mathbf{h}_{t-1},\\mathbf{h}_{t-2},\\dots$, besides the input $\\mathbf{x}_t$. This design resembles *memory* and enables RNN to capture sequential relationship.\n",
    "\n",
    "There are many architectures for a recurrent layers, the only difference between them is how the cell $A$ being desgined. In this article, we are going to learn the cell architectures of Simple RNN, LSTM and GRU.\n",
    "\n",
    "[Recurrent Neural Network]: https://en.wikipedia.org/wiki/Recurrent_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Building blocks\n",
    "This section introduces common blocks in recurrent architectures. Knowing each of them separately helps us understanding compicated designs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation\n",
    "\n",
    ":::{image} ../image/rnn_concatenation.png\n",
    ":height: 300px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "Let's say we want to transform two input vectors $\\mathbf{u}\\in\\mathbb{R}^{U\\times1}$ and $\\mathbf{v}\\in\\mathbb{R}^{V\\times1}$ into $\\mathbf{y}\\in\\mathbb{R}^{D\\times1}$. Note that $U$ and $V$ are fixed dimensionalities of input, while $D$ is the desired output size. With weight matrices\n",
    "$\\mathbf{W}_{yu}\\in\\mathbb{R}^{D\\times U},\\mathbf{W}_{yv}\\in\\mathbb{R}^{D\\times V}$\n",
    "and bias vector $\\mathbf{b}_y\\in\\mathbb{R}^{D\\times1}$,\n",
    "the actual formula behind the above image is:\n",
    "\n",
    "$$\\mathbf{y}=\\mathbf{W}_{yu}\\mathbf{u}+\\mathbf{W}_{yv}\\mathbf{v}+\\mathbf{b}_y$$\n",
    "\n",
    "Here, all three terms have size $(D\\times1)$, same as $\\mathbf{y}$. We can also view the above formula as concatenating $\\mathbf{u}$ and $\\mathbf{v}$ into a single input vector $\\mathbf{x}\\in\\mathbb{R}^{(U+V)\\times1}$, then scale it using a bigger weight matrix $\\mathbf{W}_{yx}\\in\\mathbb{R}^{D\\times(U+V)}$. This explains why the formula is visualized as a concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gate\n",
    "\n",
    ":::{image} ../image/rnn_gate.png\n",
    ":height: 80px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "A gate consists of two calculation steps, (1) passing a vector into sigmoid function and (2) using it as a percentage multiplier. The sigmoid function (denoted $\\sigma$) is account for producing numbers in range $(0,1)$. We can see the purpose of gates very clearly here: they control how much information should be let through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "We call the vanilla architecture [Simple RNN] (1980s) to distinguish from the family name. Its cells is very simple, with only a concatenated value pass through an activation function. The activation function is usually $\\tanh$ which produces values within the range $(-1,1)$, so that the network will be able to express *sentiment*. The cell architecture is described in the image and formula as follows:\n",
    "\n",
    ":::{image} ../image/rnn_cell.png\n",
    ":height: 160px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "$$\\mathbf{h}_t=\\phi(\\mathbf{W}_{hx}\\mathbf{x}_t+\\mathbf{W}_{hh}\\mathbf{h}_{t-1}+\\mathbf{b}_h)$$\n",
    "\n",
    "A well-known issue with Simple RNN is that it only has *short-term memory*. This property is very easy to understand if you are familiar with the gradient vanishing problem of S-shaped activation functions. During [backpropagation through time] for a pair of words with large $\\Delta t$, the product of partial derivatives may trigger saturation zones of $\\tanh$, making the derivative of a word with respect to the other almost zero. As a result, Simple RNN fails to capture long-term memory.\n",
    "\n",
    "[Simple RNN]: https://en.wikipedia.org/wiki/Recurrent_neural_network#Fully_recurrent\n",
    "[backpropagation through time]: https://en.wikipedia.org/wiki/Backpropagation_through_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sizing\n",
    "$\\mathbf{W}_{hx},\\mathbf{W}_{hh}$ and $\\mathbf{b}_h$, as explained earlier, are the weight matrices and bias vector. Their corresponding sizes are $(D\\times V)$, $(D\\times D)$ and $(D\\times 1)$. Note that these parameters are used across cells, hence taking sum of their sizes gets us the total number of parameters need to be trained:\n",
    "\n",
    "$$D\\times(D+V+1)$$\n",
    "\n",
    "For example, we use a BERT pretrained model to encode a corpus containing $N=10\\,000$ documents. The embedding dimension is $V=512$ and documents are truncated to have $T=128$ tokens. Then a RNN layer with $D=50$ will require $50\\times(50+512+1)=28\\,150$ parameters to process such input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "[LSTM] (Long Short-Term Memory) is a recurrent architecture published in 1997 that does not suffer from the gradient vanishing/exploding problem. This means, the network is able to capture long-term memory. The image below illustrates the architecture of a LSTM *memory cell*.\n",
    "\n",
    ":::{image} ../image/lstm_cell.png\n",
    ":height: 320px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "The ability to manipulate information of LSTM is regulated via three gates (green node): forget $\\mathbf{f}_t$, input $\\mathbf{i}_t$ and output $\\mathbf{o}_t$. Recall that a gate is a Sigmoid function ($\\phi_1$ in this picture) followed by a multiplication. These gates give LSTM the ability to learn when to remember and when to forget, based on previous *hidden state* $\\mathbf{h}_{t-1}$ and current input data $\\mathbf{x}_t$. They all share the same formula, but with different parameters:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{f}_t &= \\phi_1(\\mathbf{W}_{fh}\\mathbf{h}_{t-1}+\\mathbf{W}_{fx}\\mathbf{x}_t+\\mathbf{b}_f) \\\\\n",
    "\\mathbf{i}_t &= \\phi_1(\\mathbf{W}_{ih}\\mathbf{h}_{t-1}+\\mathbf{W}_{ix}\\mathbf{x}_t+\\mathbf{b}_i) \\\\\n",
    "\\mathbf{o}_t &= \\phi_1(\\mathbf{W}_{oh}\\mathbf{h}_{t-1}+\\mathbf{W}_{ox}\\mathbf{x}_t+\\mathbf{b}_o) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The key component contributing to the long-term-memory ability of LSTM is the *internal state* $\\mathbf{c}_t$. This variable works like a conveyor belt running straight through the entire chain, gathering *additional* information $\\tilde{\\mathbf{c}}_t$ at each memory cell it goes through. Because new information is really mathematically added, we are assured the gradients can pass many times without vanishing or exploding. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\tilde{\\mathbf{c}}_t &= \\phi_2(\\mathbf{W}_{ch}\\mathbf{h}_{t-1}+\\mathbf{W}_{cx}\\mathbf{x}_t+\\mathbf{b}_c) \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t\\odot\\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot\\tilde{\\mathbf{c}}_t \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Finally, the current *hidden state* $\\mathbf{h}_t$ is computed by $\\tanh$-activating $\\mathbf{c}_t$ and scaling it down by the output gate $\\mathbf{o}_t$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t\\odot\\phi_2(\\mathbf{c}_t) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can see LSTM has 4 concatenations between $\\mathbf{x}$ and $\\mathbf{h}$, so its number of parameters will be $4\\times D\\times(D+V+1)$.\n",
    "\n",
    "[LSTM]: https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    ":::{image} ../image/lstm_steps.png\n",
    ":height: 450px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "Because it is quite hard to track what is going on by looking at the full architecture of LSTM, we will break it down into four steps:\n",
    "- Step 1, construct the *forget gate* that decides how much old data should be kept.\n",
    "- Step 2, construct the *input gate* that decides what new data should be stored.\n",
    "- Step 3, compute the *internal state* by combining two processes, *forgetting* and *receiving* information.\n",
    "- Step 4, construct the *output gate* and compute the output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "[Gated Recurrent Units] (GRU) was published in 2014 as an alternative that retains key idea of LSTM but is faster in computation.\n",
    "\n",
    "[Gated Recurrent Units]: https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
    "\n",
    ":::{image} ../image/gru_cell.png\n",
    ":height: 320px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "GRU comes with the two-gate mechanism, *reset* ($\\mathbf{r}_t$) and *update* ($\\mathbf{z}_t$), rather than three like LSTM. They are also based on previous *hidden state* $\\mathbf{h}_{t-1}$ and current input data $\\mathbf{x}_t$ like LSTM's gates.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{r}_t &= \\phi_1(\\mathbf{W}_{rh}\\mathbf{h}_{t-1}+\\mathbf{W}_{rx}\\mathbf{x}_t+\\mathbf{b}_r) \\\\\n",
    "\\mathbf{z}_t &= \\phi_1(\\mathbf{W}_{zh}\\mathbf{h}_{t-1}+\\mathbf{W}_{zx}\\mathbf{x}_t+\\mathbf{b}_z) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Next, the new information $\\tilde{\\mathbf{h}}_t$ is computed by two processes, *receiving* new information $\\mathbf{x}_t$ and *forgetting* old information $\\mathbf{h}_{t-1}$. Then, it is added to the old state weightedly (weights are controled via the update gate) to get the current hidden state $\\mathbf{h}_t$. Unlike LSTM, GRU does not maintain the *internal state* but resembles its behaviour.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\tilde{\\mathbf{h}}_t &= \\phi_2(\\mathbf{W}_{hx}\\mathbf{x}_t+\\mathbf{W}_{hh}(\\mathbf{r}_t\\odot\\mathbf{h}_{t-1})+\\mathbf{b}_h) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{z}_t\\odot\\mathbf{h}_{t-1}+(1-\\mathbf{z}_t)\\odot\\tilde{\\mathbf{h}}_{t-1} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The number of parameters of a GRU layer is $3\\times D\\times(D+V+1)$, as it has 3 concatenations of $\\mathbf{x}$ and $\\mathbf{h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    ":::{image} ../image/gru_steps.png\n",
    ":height: 450px\n",
    ":align: center\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Bi-directional\n",
    "All the recurrent layers have been introduced so far are uni-directional, i.e. going from the left to the right. In other words, our networks only model leftward context. This design works fine for time series data, but for text data, rightward context also matters. This problem is solved by generalizing recurrent layers to [bi-directional]. The architecture is described in the following image:\n",
    "\n",
    ":::{image} ../image/rnn_bidirectional.png\n",
    ":height: 245px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "We can observe that a bi-directional layer is composed of a foward sub-layer and a backward sub-layer. They can be of the same type or not, and their cells are denoted $A$ and $A'$ respectively. Their corresponding outputs $\\overrightarrow{\\mathbf{h}}_t$ and $\\overleftarrow{\\mathbf{h}}_t$ are then merged into $\\mathbf{h}_t$ using various strategies, where the most common one is concatenating.\n",
    "\n",
    "[bi-directional]: https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Implementation\n",
    "TensorFlow implements all three recurrent layers via [`SimpleRNN`], [`LSTM`] and [`GRU`]. Each has a cell-level counterpart that can be used with the abstract class [`RNN`]. There is also [`Bidirectional`] working as a wrapper around recurrent layers for bi-directional behaviour. They have the following hyperparameters:\n",
    "- `units`: the dimensionality of output space ($D$).\n",
    "- `activation`: the activation function for processing data ($\\phi$ in Simple RNN and $\\phi_2$ in LSTM, GRU), defaults to *tanh*.\n",
    "- `recurrent_activation`: the activation function for gates ($\\phi_1$, only available for LSTM and GRU), defaults to *sigmoid*.\n",
    "\n",
    "[`SimpleRNN`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN\n",
    "[`LSTM`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "[`GRU`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "[`RNN`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\n",
    "[`Bidirectional`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sspipe import p, px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:54:45.257865Z",
     "iopub.status.busy": "2022-12-17T16:54:45.257460Z",
     "iopub.status.idle": "2022-12-17T16:54:45.274689Z",
     "shell.execute_reply": "2022-12-17T16:54:45.273178Z",
     "shell.execute_reply.started": "2022-12-17T16:54:45.257841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfSpam = pd.read_csv('../data/spam_message.csv')\n",
    "xTrainRaw, xTestRaw, yTrain, yTest = train_test_split(dfSpam.content, dfSpam.spam, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:54:45.276935Z",
     "iopub.status.busy": "2022-12-17T16:54:45.276538Z",
     "iopub.status.idle": "2022-12-17T16:55:55.791196Z",
     "shell.execute_reply": "2022-12-17T16:55:55.790387Z",
     "shell.execute_reply.started": "2022-12-17T16:54:45.276896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_feature(corpus):\n",
    "    bertProcessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "    bertEncoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/2')\n",
    "    corpus = corpus.str.lower()\n",
    "    corpus = bertProcessor(corpus)\n",
    "    corpus = bertEncoder(corpus)\n",
    "    corpus = corpus['sequence_output']\n",
    "    return corpus\n",
    "    \n",
    "xTrain = process_feature(xTrainRaw)\n",
    "xTest = process_feature(xTestRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:07:22.161985Z",
     "iopub.status.busy": "2022-12-17T03:07:22.161562Z",
     "iopub.status.idle": "2022-12-17T03:07:22.170452Z",
     "shell.execute_reply": "2022-12-17T03:07:22.169486Z",
     "shell.execute_reply.started": "2022-12-17T03:07:22.161941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 128, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:18:51.161934Z",
     "iopub.status.busy": "2022-12-17T03:18:51.161550Z",
     "iopub.status.idle": "2022-12-17T03:18:51.595588Z",
     "shell.execute_reply": "2022-12-17T03:18:51.594860Z",
     "shell.execute_reply.started": "2022-12-17T03:18:51.161911Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.SimpleRNN(16)(xTrain).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:19:05.115829Z",
     "iopub.status.busy": "2022-12-17T03:19:05.115469Z",
     "iopub.status.idle": "2022-12-17T03:19:05.455679Z",
     "shell.execute_reply": "2022-12-17T03:19:05.454843Z",
     "shell.execute_reply.started": "2022-12-17T03:19:05.115805Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 128, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.SimpleRNN(16, return_sequences=True)(xTrain).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "When using RNN with other layers, there are two cases:\n",
    "- The next layer being Fully Connected, then we only use the last hidden state, $\\mathbf{h}_T$. The output shape in this case is  $(N\\times D)$.\n",
    "- The next layer being another RNN layer (including LSTM and GRU), then we need to return the full sequence $\\mathbf{h}_1,\\mathbf{h}_2,\\dots,\\mathbf{h}_T$. This is done by specifying `return_sequences=True`. The output shape this time is $(N\\times T\\times D)$.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:11.976104Z",
     "iopub.status.busy": "2022-12-17T16:56:11.975625Z",
     "iopub.status.idle": "2022-12-17T16:56:16.846199Z",
     "shell.execute_reply": "2022-12-17T16:56:16.845320Z",
     "shell.execute_reply.started": "2022-12-17T16:56:11.976074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 5s 25ms/step - loss: 0.0793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9134fe7f40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.SimpleRNN(10, return_sequences=True),\n",
    "    layers.SimpleRNN(5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:25.009641Z",
     "iopub.status.busy": "2022-12-17T16:56:25.009088Z",
     "iopub.status.idle": "2022-12-17T16:56:25.031197Z",
     "shell.execute_reply": "2022-12-17T16:56:25.030317Z",
     "shell.execute_reply.started": "2022-12-17T16:56:25.009597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 128, 10)           2670      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 5)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,756\n",
      "Trainable params: 2,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:37.708607Z",
     "iopub.status.busy": "2022-12-17T16:56:37.708278Z",
     "iopub.status.idle": "2022-12-17T16:56:38.266645Z",
     "shell.execute_reply": "2022-12-17T16:56:38.266043Z",
     "shell.execute_reply.started": "2022-12-17T16:56:37.708585Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789832918592251"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTestPred = model.predict(xTest)\n",
    "AUC(yTest, yTestPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:58:19.396320Z",
     "iopub.status.busy": "2022-12-17T16:58:19.395930Z",
     "iopub.status.idle": "2022-12-17T16:58:19.401454Z",
     "shell.execute_reply": "2022-12-17T16:58:19.400422Z",
     "shell.execute_reply.started": "2022-12-17T16:58:19.396295Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 10)\n",
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "_ = [print(weight.shape) for weight in model.layers[0].weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([\n",
    "    [1,2,3,4,],\n",
    "    [0,4,1,1,],\n",
    "])\n",
    "y = tf.constant([0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 221ms/step - loss: 0.5121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb544b3f70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=10, output_dim=20),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(loss='mse')\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_19 (Embedding)    (None, None, 20)          200       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 1)           21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Seq2Seq\n",
    "[Seq2Seq], introduced in the paper [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) (2014), is a Deep Learning architecture desgined mainly for [neural machine translation] and [question answering]. It aims to transform an input sequence to a new one, both can be of arbitrary lengths. Its design is very natural to translation problems, and has resolved the fixed-length constraint in standalone recurrent layers.\n",
    "\n",
    "[Seq2Seq]: https://en.wikipedia.org/wiki/Seq2seq\n",
    "[neural machine translation]: https://en.wikipedia.org/wiki/Neural_machine_translation\n",
    "[question answering]: https://en.wikipedia.org/wiki/Question_answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "The Seq2Seq uses an encoder-decoder architecture, in which each process uses a recurrent layer(s) such as LSTM and GRU. With $(\\mathbf{x}_1,\\dots,\\mathbf{x}_S)$ a source sentence and $(\\mathbf{y}_1,\\dots,\\mathbf{y}_T)$ the target sentence, the architecture of Seq2Seq is described in the following image:\n",
    "\n",
    ":::{image} ../image/seq2seq.png\n",
    ":height: 300px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "The *encoder* is simply a recurrent layer where the hidden state at a time step is expressed by $(\\mathbf{x}_s,\\mathbf{e}_{s-1})\\mapsto\\mathbf{e}_s$. The last hidden state will be used as the output, known under the name *context vector* $\\mathbf{c}=\\mathbf{e}_S$ (sometimes called *sentence embedding* or *thought vector*).\n",
    "\n",
    "The *decoder* is a bit different, in which the hidden state has the signature $(\\mathbf{y}_{t-1},\\mathbf{d}_{t-1})\\mapsto\\mathbf{d}_t$. Note that each hidden state $\\mathbf{d}_t$ must be computed using previous target token $\\mathbf{y}_{t-1}$ to prevent target leakage. We also include $\\mathbf{c}$ as the initialization state.\n",
    "\n",
    "Because our goal is to take string input to predict string output, pre-trained embedding will not be used, at least for target text. Therefore, an embedding layer is used to process data in both encoder and decoder. This layer turns each word into a vector, just like Word2Vec or BERT do, but all embedding values are treated learnable parameters by our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Attention\n",
    "A weakness of Seq2Seq can be easily observed, is the bottleneck at the context vector $\\mathbf{c}$, which prevents the network from remembering long sequences, especially the first part. This problem is resolved by applying an [Attention mechanism] to compute a distinct context vector for each target token, proposed by Bahdanau in 2014. Note that the idea of Attention mechanism has been around for a while and has been applied in different areas, but this is the first time applied in the NLP field.\n",
    "\n",
    "[Attention mechanism]: https://en.wikipedia.org/wiki/Attention_(machine_learning)\n",
    "[`Attention`]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention mechanism\n",
    "In 2017, the paper\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "generalizes Attention mechanism using the concept of *query*, *key* and *values*, denoted respectively by the vectors $\\mathbf{q}$, $\\mathbf{k}$ and $\\mathbf{v}$.\n",
    "\n",
    "Let's talk about *key* and *value* first, as they share the same meaning as in Python dictionaries, so they should be familiar to Data Scientists. Intuitively, think about a *key* as consumer behaviour and demographic data, and *value* as consumer income. Each pair of key-value forms an observation, then the database can be denoted $\\mathcal{D}=\\{(\\mathbf{k}_1,\\mathbf{v}_1),\\ldots,(\\mathbf{k}_N,\\mathbf{v}_N)\\}$. For a new user whose feature space denoted by $\\mathbf{q}$, we perform a *query over the database* to calculate the *attention score*:\n",
    "\n",
    "$$\\alpha_n=\\text{softmax }\\alpha(\\mathbf{q},\\mathbf{k}_n)$$\n",
    "\n",
    "Here, $\\alpha$ is the *alignment function* which measures the similarity between two vectors, in which the most simple form is just the dot product. This function outputs $\\alpha_n$, implies how similar the query is to each key. In practice, we would want $\\alpha_1,\\ldots,\\alpha_N$ to (1) be positive and (2) add up to $1$, so that they resemble normalized weights. This is addressed nicely by applying a softmax function, explaining its existence in the formula. Now, the output for $\\mathbf{q}$ can be easily computed as the weighted sum of all values in the database:\n",
    "\n",
    "$$\\mathbf{o}=\\sum_{n=1}^{N}{\\alpha_n\\mathbf{v}_n}$$\n",
    "\n",
    ":::{image} ../image/attention_mechanism.png\n",
    ":height: 300px\n",
    ":align: center\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bahdanau-style\n",
    "In the paper\n",
    "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "(2014), Bahdanau introduces Additive Attention being an extension to Seq2Seq. The architecture of this network leaves the encoder in Seq2Seq untouched (which produces hidden states $\\mathbf{e}_1,\\dots,\\mathbf{e}_S$), and modifies only the decoder. Let's say we are at decoder step $t$, the Attention mechanism is applied as follows:\n",
    "\n",
    "- The encoder hidden states are used as both keys and the corresponding values: $\\mathbf{k}_s=\\mathbf{v}_s=\\mathbf{e}_s$\n",
    "- The *previous* decoder hidden state is used as the query: $\\mathbf{q}=\\mathbf{d}_{t-1}$\n",
    "- The output of Attention mechanism is the context vector for the current step: $\\mathbf{o}=\\mathbf{c}_t$\n",
    "- The alignment function is designed to be *learnable* by concatenating the query and the key, then passing to two bias-free Fully Connected layers. The first layer with the weight $\\mathbf{W}^{(1)}$ brings $\\mathbf{q}$ and $\\mathbf{k}$ to the same dimension. The second layer with the weight $\\mathbf{W}^{(2)}$, in fact, is just a vector that accounts for producing a scalar.\n",
    "\n",
    "$$\\alpha(\\mathbf{q},\\mathbf{k}_s)=\\text{tanh}([\\mathbf{q},\\mathbf{k}_s]\\mathbf{W}^{(1)})\\mathbf{W}^{(2)} $$\n",
    "\n",
    "After producing $\\mathbf{c}_t$, this vector is concatenated to $\\mathbf{y}_{t-1}$ and the signature of the decode is turned into $([\\mathbf{y}_{t-1};\\mathbf{c}_t],\\mathbf{d}_{t-1})\\mapsto\\mathbf{d}_t$. This process is repeated for $t=1,2,\\ldots,T$, in which the intialization is $\\mathbf{d}_0=\\mathbf{e}_S$. The below image illustrates how to use Attention to predict the second word ($t=2$).\n",
    "\n",
    ":::{image} ../image/attention_bahdanau.png\n",
    ":height: 380px\n",
    ":align: center\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luong-style\n",
    "In 2015, Thang Luong proposed another Attention style via the paper\n",
    "[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025). It is also meant for improving Seq2Seq, with two differences.\n",
    "\n",
    "First, there is a modification in the order of steps. With an intermediate variable $\\tilde{\\mathbf{d}}_t$ used as query, Luong-style Attention is easier to implement and thus allows different alignment functions to be pluged in. The image below shows the prediction process for the same word as Bahdanau example.\n",
    "\n",
    ":::{image} ../image/attention_luong.png\n",
    ":height: 380px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "The second difference in this paper, is the introduction of three new alignment functions:\n",
    "- *dot* (which is simply the dot product): $\\alpha(\\mathbf{q},\\mathbf{k}_s)=\\mathbf{q}^\\text{T}\\mathbf{k}_s$\n",
    "- *general* (which extends the dot product to be learnable): $\\alpha(\\mathbf{q},\\mathbf{k}_s)=\\mathbf{q}^\\text{T}\\mathbf{W}\\mathbf{k}_s$\n",
    "- *location* (which is computed solely on the decoder hidden state): $\\alpha(\\mathbf{q},\\mathbf{k}_s)=\\mathbf{W}\\mathbf{q}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "Imagine we are translating documents from English to Vietnamese. With the Seq2Seq model, we read the full sentence at once, remember it (context vector) and then translate. Because the context vector is too small to summarize the sentence, there will be a high chance that we *forget* some details from the English document. A better way is to write down *keywords* during reading, which will be very helpful for translation later. This is exactly what Attention trying to model: how each Vietnamese word *attends* to different parts of the English sentence.\n",
    "\n",
    "So can we ask the Attention model to *share* those *keywords* with us after learning? The answer is yes, they are the set of attention weights $\\alpha_{t,s}$. They can be interpreted as a $S\\times T$ attention matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T14:24:35.701953Z",
     "iopub.status.busy": "2023-03-17T14:24:35.701527Z",
     "iopub.status.idle": "2023-03-17T14:24:38.387991Z",
     "shell.execute_reply": "2023-03-17T14:24:38.387398Z",
     "shell.execute_reply.started": "2023-03-17T14:24:35.701889Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(['seaborn', 'seaborn-whitegrid'])\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T14:24:38.389363Z",
     "iopub.status.busy": "2023-03-17T14:24:38.389023Z",
     "iopub.status.idle": "2023-03-17T14:24:38.393803Z",
     "shell.execute_reply": "2023-03-17T14:24:38.393237Z",
     "shell.execute_reply.started": "2023-03-17T14:24:38.389343Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.8, 0.2, 0.0],\n",
    "    [0.0, 0.2, 0.1, 0.7]\n",
    "]\n",
    "\n",
    "index = ['Tôi', 'đi', 'học']\n",
    "columns = ['I', 'go', 'to', 'school']\n",
    "\n",
    "df = pd.DataFrame(data=data, index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T14:25:29.317300Z",
     "iopub.status.busy": "2023-03-17T14:25:29.316821Z",
     "iopub.status.idle": "2023-03-17T14:25:29.404240Z",
     "shell.execute_reply": "2023-03-17T14:25:29.403501Z",
     "shell.execute_reply.started": "2023-03-17T14:25:29.317270Z"
    },
    "render": {
     "image": {
      "align": "center",
      "scale": "50%"
     }
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAGMCAYAAAA/cBDSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAABYlAAAWJQFJUiTwAAAbCklEQVR4nO3de3TMd/7H8dcgCU1kNWi6xGUdxnURbd21LlN2EaROj7j0glrXWtqlcrrZ07NKtVraXe1xqEtbquuysWvdU6Hq3jhbtELrsMIua1G5Sir5/v7oMb+mUWTyZej7+fir853vdz7vZkyeZuY7w+M4jiMAAPCTVi7YAwAAgFuP4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAyoEe4Cy6PzSnGCPgDL46NnBwR4BAO4691e9N6DjeIYPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABlQI9MBXXnlFnTp1UseOHf2Xb4bH49GUKVMCXRYAAAQg4OC/9957qly5sj/477333k0dR/ABALj9Ag7++++/r5o1axa7DAAA7kwBB79169bXvQwAAO4cAQf/x+Tk5Gjz5s1KT09XXl6eqlSpogYNGqhLly4KDw93ezkAAHATXA3+li1blJiYqMzMTDmO49/u8XgUGRmpGTNmqEuXLm4uCQAAbkKZPpZXWFioBQsWSJIOHTqk8ePHKz8/X0OHDtXbb7+tFStWaMGCBfrNb36jwsJCTZgwQenp6a4MDgAAbl7Az/DPnj2r3/72tzpz5oyGDx+ud955R+XLl9eyZcvUuHHjYvt26NBB3bt316BBgzRv3jzNmjWrzIMDAICbF3Dw+/btK0n+j9ilpaXJ5/OViP1VTZs2lc/n0549ewJdEgAABCjgl/S/+eYbde7cWT169JAk5ebmqlq1atc9pmrVqsrMzAx0SQAAEKCAg//4449r9erVGjJkiCSpdu3a2rVrl4qKiq65f2FhoXbv3q2YmJhAlwQAAAEKOPhTp07VrFmzVFBQIEmKi4vT0aNHlZSUpNzc3GL7Xrx4UYmJifrqq68UFxdXtokBAECpeZzvf34uABcuXFBUVJQKCgr09NNPa//+/apUqZKaNGmiypUr6+zZszpx4oTy8vIUGxur9957T6Ghoa4M3/mlOa7cDoLjo2cHB3sEALjr3F/13oCOK/Pn8KOioiRJoaGhWrx4sebPn6/Vq1crLS3Nv09MTIzi4+M1YsQI12IPAABuXsDP8Lt166annnpKTz755DWvz8nJUXZ2tsLDwxUREVGmIX8Mz/DvbjzDB4DSu+3P8E+fPn3dM+7Dw8P5Kl0AAO4QZfqmPQAAcHcg+AAAGFCmk/aysrL073//u9TH1ahRoyzLAgCAUipT8N9//329//77pTrG4/Hoyy+/LMuyAACglMoU/J///OeqWbOmW7MAAIBbpEzBf+yxxzRu3Di3ZgEAALcIJ+0BAGAAwQcAwACCDwCAAQEHf9y4cWrTpo2bswAAgFsk4JP2OFkPAIC7By/pAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwwOM4jhPsIQJ15vzFYI+AMhizKDnYI6AMJv66U7BHQIAa3F8t2COgDO6vem9Ax/EMHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAyoEOiBH3/8serVq6df/OIX/ss3q1u3boEuCwAAAhBw8MeOHatx48Zp3Lhx/ssej+e6xziOI4/Ho8OHDwe6LAAACEDAwR83bpzatGnjv3wzwQcAAMFRpuA3btxY48aN09ixY/Xss8+6ORcAAHBRwMFPTEyU4zhKSUnRqVOnbvo4j8ej6dOnB7osAAAIQMDBT05O9r8ff/U9+asv6TuOU2J/j8fjfw+f4AMAcHsF/LG85ORkOY6jhIQErV69WosXL1bVqlXVoEEDzZkzRzt37tQXX3yhzz77TAsWLFCLFi1Up04dbdy40c35AQDATQg4+I0bN5YkVatWTY0aNdLf//53hYSE6IMPPpDP51NUVJTKly+viIgIdejQQe+++66Kioo0c+ZM14YHAAA3x7Uv3klJSVHXrl31s5/97JrXR0REqHPnztqxY4dbSwIAgJtUpuB//6N5Ho9Hly5duu7+Z86cUcWKFcuyJAAACECZg//QQw9Jklq1aqUNGzZo165d19x348aNSklJUceOHcuyJAAACEDAZ+n/0IQJE7Rnzx4988wz6tSpk5o1a6aIiAhlZWVp//792r17t6pWraoJEya4tSQAALhJrgW/YcOGWrp0qaZNm6atW7dq69at/us8Ho86deqkpKQk1ahRw60lAQDATXIt+NJ3Z+4vWbJEZ8+e1ZEjR5SZmanIyEg1adJE1apVc3MpAABQCq4G/6ro6GhFR0ffipsGAAABcO1jeQAA4M5F8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABHsdxnGAPEagz5y8GewSUwVdn/hfsEVAGURH3BHsEBGj2+u3BHgFl8O6ohICO4xk+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAy4bcHPz8+/XUsBAIAfcD34R48e1ZgxY7RixYpi2zt16qRRo0bp9OnTbi8JAABuwNXgHzlyRAkJCUpNTdWlS5f82y9fvqymTZvq008/Vf/+/XX8+HE3lwUAADfgavDfeustOY6jDz/8UM8884x/e8WKFbVo0SJ98MEHysvL0+zZs91cFgAA3ICrwT9w4IB69+6t2NjYa14fGxurnj17avfu3W4uCwAAbsDV4Ofm5iokJOS6+4SHh3MCHwAAt5mrwa9fv762bdumnJyca16fn5+v7du3q169em4uCwAAbsDV4A8YMECnT5/WqFGj9Pnnn6uwsFCSVFRUpIMHD2rMmDE6efKkBgwY4OayAADgBiq4eWP9+/fX559/ruXLlyshIUHly5dXWFiY8vPzVVhYKMdx1L9/fyUkJLi5LAAAuAFXgy9Jf/zjH/XrX/9aa9eu1ZEjR5SZmal77rlHXq9Xffr0UYcOHdxeEgAA3IDrwZekdu3aqV27drfipgEAQABuSfBzcnK0efNmpaenKy8vT1WqVJHX61Xnzp0VHh5+K5YEAADX4Xrwt2zZosTERGVmZspxHP92j8ejyMhIzZgxQ126dHF7WQAAcB2uBv/QoUMaP368KlSooKFDh+qBBx7Qfffdp8zMTO3du1dLly7VhAkT9Je//EWNGjVyc2kAAHAdrgb/nXfeUfny5bVs2TI1bty42HUdOnRQ9+7dNWjQIM2bN0+zZs1yc2kAAHAdrn4OPy0tTT6fr0Tsr2ratKl8Pp/27t0rSUpJSVFiYqKbIwAAgGtw/at1q1Wrdt19qlatqszMTEnS4cOHtXr1ajdHAAAA1+DqS/q1a9fWrl27VFRUpHLlSv5dorCwULt371ZMTIwk6dFHH/X/NwAAuHVcfYYfFxeno0ePKikpSbm5ucWuu3jxohITE/XVV18pLi5OktSoUSPFx8e7OQIAALgGV5/hDxs2TJ988olWrVqldevWqUmTJqpcubLOnj2rEydOKC8vT7GxsRo+fLibywIAgBtwNfihoaFavHix5s+fr+TkZKWlpfmvi4mJUXx8vEaMGKHQ0FA3lwUAADfg+hfvhIaGauzYsRo7dqxycnKUnZ2t8PBwRUREuL0UAAC4Sa4HPzU1VStXrlRGRoZyc3OLfdveVR6PRykpKW4vDQAAfoSrwd+wYYMmTpwox3FUrlw5vjcfAIA7hKvBnzdvnsLCwvT666+rc+fOqlDhlvzbPAAAoJRcLfKxY8fUp08f+Xw+N28WAACUkaufw69cuTIv4wMAcAdyNfg9evTQpk2bdPnyZTdvFgAAlFGZXtJPT08vdrlHjx7asGGDhgwZoqeeekp16tT50c/c88/jAgBw+5Qp+P369ZPH4ym2zXEcnT9/XpMnT77usYcPHy7L0gAAoBRcDz4AALjzlCn4M2bMcGsOAABwC7l60h4AALgzEXwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGCAx3EcJ9hDAACAW4tn+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBP8OderUKTVs2FBPPPFEsEcBfrIOHDigTz/9NNhjoJSmTJmihg0b6vDhw0GbYc+ePWrYsKGmTZsWtBlKi+ADMGnr1q0aMGCAvv7662CPAtwWBB+ASRcuXFBRUVGwxwBuG4IPAIABBB8IwMmTJ/Xcc8+pffv2io2N1YgRI3Ts2DE9+uijxc67yMrK0muvvSafz6dmzZqpffv2ev7553X8+PEgTo8pU6YoMTFRkvTKK6+oYcOGOnXqlCRp3bp1SkhIUMuWLRUbG6uEhAStXbs2mOP+JFy5ckVz5sxRXFycWrZsqdatW2v48OHatWtXsf0uXLig6dOnq2vXrmrevLl69Oih2bNnKycnp8RtZmZmaurUqerYsaOaN2+u+Ph4rV+/vsR+BQUFmjt3rnr27KlmzZqpTZs2Gj16tA4ePFhi36KiIn344Yfq16+fmjdvrgceeEBDhw7Vjh073PthBEmFYA8A3G3+9a9/KSEhQd988418Pp9iYmKUmpqqQYMGqaioSPfff78k6eLFixo4cKCOHz+uli1bqlu3bsrIyNC6deu0detWLVy4UC1atAjy/41NPp9PmZmZ+vjjj9WxY0e1bNlSkZGRevXVV7Vw4UJVr15dvXv3lvTde/3PPfecvvzyS02aNCnIk9+9pk6dqo8++kitW7fWww8/rKysLK1bt07Dhw/XokWL1KZNG507d04DBgzQ6dOn1aZNG/Xo0UNffvml5s6dq88//1zvvvuuKlT4/2xNnDhRYWFh6tmzp3JycrRmzRpNmDBBoaGh6tatmyQpPz9fQ4cOVVpamrxerwYOHKj//e9/SklJ0fbt2/Xmm2/K5/NJ+i72EydO1IYNG1SrVi31799fubm5+vjjjzV8+HAlJSVp8ODBQfn5ucLBHSkjI8Pxer3OkCFDgj0KfmDkyJGO1+t11q9f79+Wn5/vDBw4sNh9lpiY6Hi9Xmf27NnFjt+6davTsGFDp3v37s6VK1du5+j4nlWrVjler9dZtGiR4ziOs2/fPsfr9Tr9+vVzzp8/79/v/PnzTu/evR2v1+vs3bs3SNPe3bKyspxGjRo5gwcPLrb9wIEDjtfrdZ599lnHcRxn0qRJxe6Tq5KSkhyv1+ts3LjRcRzHeeGFFxyv1+vEx8c72dnZ/v02b97seL1eZ9SoUf5tc+bMcbxerzNlyhTn22+/9W8/dOiQ07x5c+fBBx90srKyHMdxnOTkZMfr9TrDhg1zcnJy/PuePHnS6dChg9OkSRPn5MmTjuM4zu7dux2v1+u8/PLLLvyEbg9e0gdK4cKFC9q2bZsefPBB/epXv/JvDw0N1e9+9zv/5YKCAq1du1Y1a9bU+PHji93GI488ou7du+vEiRP67LPPbtvsuL6//vWvkqTJkycrKirKvz0qKkrPP/+8JGnVqlVBme1uV1RUJMdx9J///Efnzp3zb//lL3+plJQUvfHGGyooKNDmzZtVt25dPf3008WOHzlypEaNGqXq1asX2/7kk08qPDzcf/mRRx5RuXLl/G/PSFJycrIqVaqkF198sdirA02bNtWgQYOUmZmpTZs2+feVpJdeekn33HOPf99atWpp9OjRunLlilavXl3mn0ewEHygFL744gsVFRWpefPmJa5r0aKF/xfK8ePHdfnyZbVq1UrlypV8mD3wwAOSpPT09Fs7MG5aenq6ypUr579vvo/7q2wiIyPVs2dPnTp1Sl26dNETTzyh+fPn6+uvv1atWrUUEhKikydPKjc3Vy1btixxfM2aNTVx4kTFxsYW2163bt1il0NCQhQeHu5/vz87O1sZGRlq3LixIiIiStzuD+/X9PR0RUdHq1atWjfc925E8IFSuHjxoiSpWrVqJa4rX768/5lhdna2JKly5crXvJ377rtPknT58uVbMSYCkJ2drbCwMIWGhpa4rnLlyqpUqZLy8vKCMNlPw6uvvqoXXnhBdevW1d69e/X666+rV69e6t+/vw4fPqxLly5J0jXD/GPCwsKue/3V8N/s4zA7O/sn/Zgl+EApXP1ldDXoP3T1F8zVlxnPnj17zf0yMzMlSVWqVHF5QgQqPDxceXl5/vvm+/Lz83X58mXde++9QZjspyEkJETDhg3TP/7xD6Wmpurll19Wx44ddejQIY0cOdL/mLnW2fiSlJubW+o1S/s4DA8P/9F9r/6F5G5+zBJ8oBSaNm0qj8ejAwcOlLju66+/9v+yqlevnsLCwnTw4EEVFBSU2Hffvn2SpPr169/agfGjPB5PscuNGjWSJKWlpZXYNy0tTY7jcH8FKCMjQ7NmzVJqaqokqUaNGnr88ce1YMECtW3b1h/ZkJCQaz62zp49q9jYWCUlJZVq3YiICMXExOjEiRO6cOFCiet/+Dhs1KiRsrKydPTo0RL7Xj3f5m7+M0DwgVKIjo5Whw4dtHPnTm3bts2/vaCgQDNnzvRfDg0NVa9evfTf//5Xf/rTn4rdxieffKL169erTp06atWq1W2bHcVdPd/i22+/lSQ99thjkqRZs2YVi8OFCxf02muvSZL69u17m6f8aahYsaLmz5+vt956q9hfgAsKCnTu3DmFhoYqJiZGPXr00LFjx7R8+fJix8+dO1eS1K5du1KvHR8fr8uXL2v69Om6cuWKf/sXX3yhJUuWKDIyUl27dpX0/38Gpk2bVuwVhYyMDL399tsKCQlRr169Sj3DnYLP4QOl9OKLL2rAgAEaPXq0fD6foqOjtWPHDn8krp6kN2nSJO3fv1/z58/Xvn37FBsbq4yMDG3ZskXh4eGaOXNmiWeZuH2io6MlScuWLdOlS5f0xBNPaOjQoVq0aJH69OmjLl26SJJSU1N17tw5jRgxQg899FAwR75rVa9eXU899ZQWLVqk3r17+8+m3759u44dO6YxY8YoIiJCkydPVlpampKSkrRp0yY1aNBABw8e1L59++Tz+dSzZ89Srz1ixAh9+umnWrNmjY4cOaK2bdvq/PnzSklJkeM4mj17tv+tur59+2rLli3auHGj+vTpo4cfftj/Ofzs7Gz9/ve/V+3atd3+8dw2BB8opXr16mnZsmV64403tHPnTl25ckVt27bV7Nmz1adPH1WqVEnSdx/nWr58uebOnauNGzdqyZIlioqKUr9+/TR69Oi7+hfHT8FDDz2kwYMH629/+5uWLl2q9u3ba8qUKWrSpImWLl2qNWvWqEKFCmrcuLH+8Ic/qHv37sEe+a42adIk1alTRytWrFBycrIKCwtVv359zZgxQ/Hx8ZK++0vYihUr9Oc//1mpqanatWuXoqOjNXr0aI0ZMyagdcPCwrR48WItWLBAa9as0bJlyxQZGakuXbpo5MiRatKkiX9fj8ejN998U0uXLtXKlSu1cuVKVapUSS1bttTw4cPVtm1bV34WweJxHMcJ9hDA3aKoqEgZGRmqUaOGQkJCil2XkZEhn8+ngQMH6qWXXgrOgADwI3gPHygFj8ejfv36KS4ursTJeAsWLJAktWnTJhijAcB18ZI+UAoej0cJCQlauHCh/z2+8uXLa//+/frnP/+pjh07FvsGPgC4U/CSPlBKRUVFWrVqlVasWKHjx4/rypUriomJUVxcnIYOHVripX4AuBMQfAAADOA9fAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMCA/wPU9L/NB0fFfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 198,
       "width": 254
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "palette = sns.diverging_palette(20, 220, n=20)[10:]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(\n",
    "    df, square=True,\n",
    "    cmap=palette,\n",
    "    vmin=0, vmax=1, cbar=False\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transformer\n",
    "The final boss is here: state-of-the-art [Transformer], published in the paper\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017).\n",
    "It goes a step futher in applying Attention, which outperforms RNNs and becomes the foundation for many famous Deep Learning models such as BERT and GPT.\n",
    "\n",
    "[Transformer]: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    ":::{image} ../image/transformer_architecture.png\n",
    ":height: 420px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "Transformer also uses an encoder-decoder architecture. The encoder is composed of 6 identical layers; each layer has two\n",
    "sub-layers. The first is a Multi-head Self-Attention mechanism, and the second is a position-wise Feed-Forward network. Both sub-layers are wrapped by a Residual Connection, followed by Layer Normalization (denoted by *Add & Norm* in the image). The output of the last encoder layer will be used as input for the decoder.\n",
    "\n",
    "The decoder is also composed of 6 identical layers. These layers are similar to encoder layers, with two changes. First, the Self-Attention sub-layer is applied with a mask vector so that the prediction of a token is only made by previous tokens. Second, a third sub-layer being a Multi-head Cross-Attention mechanism is inserted to each decoder layer, in order to perform attention over the encoder output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention\n",
    "The Attention mechanism in Transformer is similar to Luong-style Attention with dot product alignment function, with the following improvements:\n",
    "- The dot-product alignment is divided by $\\sqrt{d_k}$, where $d_k$ is the dimension of keys. This is because for big matrices, dot products grow large in magnitude, thus push the softmax function into regions where it has extremely small gradients. The effect of this scaling factor is to stablize gradients.\n",
    "- Queries, keys and values go through a Feed Foward layer first, before being fed into Attention mechanism. This layer is represented as matrix multiplication, in which the respective weight matrices are $\\mathbf{W}_q$, $\\mathbf{W}_k$ and $\\mathbf{W}_v$.\n",
    "- Each Attention mechanism described earlier is a called a *head*. Transformer uses Multi-head Attention, each head learns a different set of weight matrices and thus can capture dependencies in a different range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Attention\n",
    "- Instead of traditional RNNs, the novel *Self-Attention* mechanism is used. It directly learns the relationship between words in a document.\n",
    "- Because Self-Attention is not aware of word order, *Positional Encoding* is proposed to address the problem.\n",
    "- The Attention mechanism accounts for communication between encoder and decoder is almost the same as Luong-style Attention, with a small change in the alignment function. It is refered to as Cross-Attention to distinguish with Self-Attention mentioned earlier.\n",
    "- All Attention applications in Transformer are multi-head, which get us different Attention matrices and enables the ability to capture dependencies in different ranges. Luong and Bahdanau-style Attention are both single-head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- *bioinf.jku - [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)*\n",
    "- *arxiv - [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](https://arxiv.org/pdf/1409.1259.pdf)*\n",
    "- *amitness - [Recurrent Keras layer](https://amitness.com/2020/04/recurrent-layers-keras/)*\n",
    "- *colah.github - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*\n",
    "- *d2l - [Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-neural-networks/index.html)*\n",
    "- *d2l - [Modern Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-modern/index.html)*\n",
    "- *d2l - [Attention Mechanisms and Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)*\n",
    "- *distill - [Memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)*\n",
    "- *distill - [Augumented RNNs](https://distill.pub/2016/augmented-rnns/)*\n",
    "- *lilianweng.github - [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)*\n",
    "- *blog.floydhub - [Attention mechanism](https://blog.floydhub.com/attention-mechanism/amp/)*\n",
    "- *sebastianraschka - [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)*\n",
    "- *slds-lmu.github - [Attention and Self-Attention for NLP](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html)*\n",
    "- *e2eml - [Transformers from Scratch](https://e2eml.school/transformers.html)*\n",
    "---\n",
    "- https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263\n",
    "- https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f\n",
    "- https://storrs.io/attention/amp/\n",
    "- https://erdem.pl/2021/05/introduction-to-attention-mechanism\n",
    "- https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified\n",
    "- https://www.baeldung.com/cs/attention-mechanism-transformers\n",
    "- https://theaisummer.com/attention/\n",
    "- https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f\n",
    "- https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7\n",
    "- https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-02T05:11:48.788204Z",
     "iopub.status.busy": "2023-01-02T05:11:48.787827Z",
     "iopub.status.idle": "2023-01-02T05:11:49.495375Z",
     "shell.execute_reply": "2023-01-02T05:11:49.493277Z",
     "shell.execute_reply.started": "2023-01-02T05:11:48.788181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 22.3.1 from /opt/anaconda3/lib/python3.8/site-packages/pip (python 3.8)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-addons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
