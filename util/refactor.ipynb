{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50bb9a42-8342-4461-a936-36f0b9f99d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file = '../chapter-06/pyspark-machine-learning.ipynb'\n",
    "\n",
    "f = open(file,'r')\n",
    "filedata = f.read()\n",
    "f.close()\n",
    "\n",
    "newdata = (\n",
    "    filedata\n",
    "    # .replace(\"###\", \"####\")\n",
    "    # .replace(\"# 1.\", \"## 1.\")\n",
    "    # .replace(\"# 2.\", \"## 2.\")\n",
    "    # .replace(\"# 3.\", \"## 3.\")\n",
    "    # .replace(\"# 4.\", \"## 4.\")\n",
    "    # .replace(\"# 5.\", \"## 5.\")\n",
    "    .replace(\"<code style='font-size:13px'>\", \"`\")\n",
    "    .replace(\"<code style='font-size:13px;'>\", \"`\")\n",
    "    .replace(\"<code style='font-size:13px; color:firebrick'>\", \"`\")\n",
    "    .replace(\"<code style='font-size:13px; color:firebrick;'>\", \"`\")\n",
    "    .replace(\"</code>\", \"`\")\n",
    ")\n",
    "\n",
    "f = open(file,'w')\n",
    "f.write(newdata)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b366e64f-576b-4721-b35f-dfbb61b6a086",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T13:51:10.323175Z",
     "iopub.status.busy": "2023-01-31T13:51:10.322665Z",
     "iopub.status.idle": "2023-01-31T13:51:10.327055Z",
     "shell.execute_reply": "2023-01-31T13:51:10.325869Z",
     "shell.execute_reply.started": "2023-01-31T13:51:10.323092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e728d2-f4dd-4042-893e-97e11a9d5831",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-01-31T13:51:44.449784Z",
     "iopub.status.busy": "2023-01-31T13:51:44.449406Z",
     "iopub.status.idle": "2023-01-31T13:51:44.473397Z",
     "shell.execute_reply": "2023-01-31T13:51:44.464659Z",
     "shell.execute_reply.started": "2023-01-31T13:51:44.449727Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../03-data-manipulation/numpy-arrays.ipynb',\n",
       " '../03-data-manipulation/pandas-data-transformation.ipynb',\n",
       " '../03-data-manipulation/pandas-data-exploratory.ipynb',\n",
       " '../03-data-manipulation/pandas-data-cleaning.ipynb',\n",
       " '../03-data-manipulation/janitor-pandas-extensions.ipynb',\n",
       " '../util/refactor.ipynb',\n",
       " '../01-python-programing/python-data-types.ipynb',\n",
       " '../01-python-programing/python-algorithms.ipynb',\n",
       " '../01-python-programing/python-data-containers.ipynb',\n",
       " '../01-python-programing/selenium-web-scraping.ipynb',\n",
       " '../01-python-programing/python-external-sources.ipynb',\n",
       " '../01-python-programing/python-basic-concepts.ipynb',\n",
       " '../01-python-programing/python-functions-objects.ipynb',\n",
       " '../05-data-visualization/plotly-interactive-visualization.ipynb',\n",
       " '../05-data-visualization/matplotlib-graph-construction.ipynb',\n",
       " '../05-data-visualization/seaborn-statistical-visualization.ipynb',\n",
       " '../09-unsupervised-learning/sklearn-clustering.ipynb',\n",
       " '../09-unsupervised-learning/pyod-anomaly-detection.ipynb',\n",
       " '../09-unsupervised-learning/sklearn-dimensional-reduction.ipynb',\n",
       " '../09-unsupervised-learning/mlxtend-association-rules.ipynb',\n",
       " '../02-mathematics/networkx-network-analysis.ipynb',\n",
       " '../02-mathematics/numpy-statistics.ipynb',\n",
       " '../02-mathematics/numpy-probability.ipynb',\n",
       " '../02-mathematics/scipy-hypothesis-testing.ipynb',\n",
       " '../02-mathematics/sympy-calculus.ipynb',\n",
       " '../02-mathematics/numpy-linear-algebra.ipynb',\n",
       " '../02-mathematics/numpy-applied-mathematics.ipynb',\n",
       " '../08-time-series/prophet-forecasting-algorithms.ipynb',\n",
       " '../08-time-series/statsmodels-temporal-analysis.ipynb',\n",
       " '../08-time-series/sktime-forecasting-pipeline.ipynb',\n",
       " '../08-time-series/darts-deep-forecasting.ipynb',\n",
       " '../04-big-data/pyspark-data-cleaning.ipynb',\n",
       " '../04-big-data/pyspark-data-exploratory.ipynb',\n",
       " '../04-big-data/pyspark-data-transformation.ipynb',\n",
       " '../04-big-data/hiveql-data-manipulation.ipynb',\n",
       " '../04-big-data/dask-parallelized-pandas.ipynb',\n",
       " '../07-tabular-learning/imblearn-targeted-modeling.ipynb',\n",
       " '../07-tabular-learning/shap-model-interpretation.ipynb',\n",
       " '../07-tabular-learning/sklearn-ensemble-learning.ipynb',\n",
       " '../07-tabular-learning/xgboost-tree-boosting.ipynb',\n",
       " '../07-tabular-learning/ray-hyperparameter-optimization.ipynb',\n",
       " '../06-machine-learning/sklearn-classification.ipynb',\n",
       " '../06-machine-learning/lenskit-recommendation.ipynb',\n",
       " '../06-machine-learning/pyspark-machine-learning.ipynb',\n",
       " '../06-machine-learning/sklearn-machine-learning.ipynb',\n",
       " '../06-machine-learning/sklearn-regression.ipynb',\n",
       " '../06-machine-learning/sklearn-feature-engineering.ipynb',\n",
       " '../10-deep-learning/keras-recurrent-networks.ipynb',\n",
       " '../10-deep-learning/keras-multilayer-perceptron.ipynb',\n",
       " '../10-deep-learning/keras-convolutional-networks.ipynb',\n",
       " '../10-deep-learning/numpy-gradient-descent.ipynb',\n",
       " '../10-deep-learning/pytorch-deep-learning.ipynb',\n",
       " '../10-deep-learning/opencv-image-processing.ipynb',\n",
       " '../10-deep-learning/gensim-text-mining.ipynb',\n",
       " '../11-r-programing/r-statistics.ipynb',\n",
       " '../11-r-programing/ggplot-data-visualization.ipynb',\n",
       " '../11-r-programing/tidyverse-data-wrangling.ipynb',\n",
       " '../11-r-programing/r-data-structures.ipynb',\n",
       " '../11-r-programing/r-basic-concepts.ipynb',\n",
       " '../11-r-programing/dplyr-data-cleaning.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('../*/*.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1666c3e-ae21-40b8-aa54-7fceb3c280db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T07:52:07.315992Z",
     "iopub.status.busy": "2023-01-29T07:52:07.315568Z",
     "iopub.status.idle": "2023-01-29T07:52:07.674110Z",
     "shell.execute_reply": "2023-01-29T07:52:07.672999Z",
     "shell.execute_reply.started": "2023-01-29T07:52:07.315958Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in glob.glob('../*/*.ipynb'):\n",
    "    f = open(file,'r')\n",
    "    filedata = f.read()\n",
    "    f.close()\n",
    "\n",
    "    newdata = (\n",
    "        filedata\n",
    "        .replace(\"## References\", \"## Resources\")\n",
    "    )\n",
    "\n",
    "    f = open(file,'w')\n",
    "    f.write(newdata)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87d1f5-4dbd-4f70-8f90-15b8b518bbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5709f4a-810b-4bdd-a86c-9a1920a40478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1438a4f-f662-4558-9cda-cb6c034e985d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03f46f55-5f97-4015-8b4e-2af8a4bf8b2b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "#### Local Outlier Factor\n",
    "[LOF](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof) is widely uses in anomaly detection specially for local outlier. It computes the local density deviation of a given data point with respect to its neighbors. A point will be considered as outlier when it has a significantly lower density than it neighbors. In other word, LOF compares the local density of a point to local density of its k-nearest neighbors and gives a score as final output. The disadvantage of LOF or proximity-based algorithms is it costs time very much to calculate the distance between large data points.\n",
    "\n",
    "The process of LOF follow these step:\n",
    "- Determined the distance from data point $p_i$ to $k_{th}$ nearest neighbors (pyod support all distance metrics from sklearn and scipy). Get the max distance among $k$ points - this is called *K_distance*. The number of neighbors of $p_i$ can greater or equal $k$ due to the distance between them - denote $|N_p|$ \n",
    "- Computes the *reachability density (RD)* of each $p_i$ related to others. RD is defined as the maximum of K-distance of $p_i$ and the distance between $p_i$ and $p_j$:\n",
    "\n",
    "$$\\text{RD}(p_i,p_j) = \\max(\\text{K_distance}_{p_i}, d(p_i,p_j))$$\n",
    "\n",
    ":::{image} ../image/local_outlier_factor_2.png\n",
    ":height: 250px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "- Computes the *local reachability density (LRD)*. LRD is inverse of the average RD of $p_i$ from its neighbors. The larger average RD leads to the smaller LRD - it means the density of $p_i$ is quite low:\n",
    "\n",
    "$$\\text{LRD}_{p_i}= \\frac{1}{\\sum_{p_j \\in N_p}\\frac{\\text{RD}(p_i,p_j)}{|N_p|} } $$\n",
    "\n",
    "- Calculates the *LOF score* for each $p_i$ - LOF score is the ratio of the average LRD of the $K$ neighbors of $p_i$ to the LRD of $p_i$. If a point is inliner, the LRD of this point is approximately equal to its neighbors that leads to LOF is nearly equal to 1. On the other hand, if the point is an outlier, the LRD of a point is less than the average LRD of neighbors, then LOF value will be high:\n",
    "\n",
    "$$\\text{LOF}_{p_i} = \\frac{\\sum_{p_j \\in N_p} \\text{LRD}_{p_j}}{|N_p|} \\cdot \\frac{1}{\\text{LRD}_{p_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74df977-7097-499a-ade5-d7d719d4be7b",
   "metadata": {},
   "source": [
    "#### Connectivity-based Outlier Factor\n",
    "[COF](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof) (Connectivity-based Outlier Factor) is another version of LOF. In LOF, the theory is that data points are distributed in circle around the instance, but in the case there is a linear relationship between data points, the distance metric in LOF is no longer correct. COF calculates the anomaly score based on average *chain distance* between points and their neighbors. Therefore, COF is suitable for local and dependency outliers but like LOF, the time cosuming of COF is quite high with large dataset.\n",
    "\n",
    "As same as LOF, COF firstly find $k$ nearest neighbors of point $p$ then arrange them in order of closest distance to $p$. Call $e_k$ is the *edge distance*, equals to each pair of points distance, example $e_2$ is the distance between $P_2$ and $P_3$, we calculates average chain distance for each instance:\n",
    "\n",
    "$$\\text{ACD}_p = \\sum_{i=1}^k \\frac{2(k+1-i)}{k(k+1)} e_i$$\n",
    "\n",
    ":::{image} ../image/connectivity_based_outlier_factor.png\n",
    ":height: 180px\n",
    ":align: center\n",
    ":::\n",
    "<br>\n",
    "\n",
    "At last, anomaly score is generated by ratio of average chaining distance of instance and the average of average chaining distance of $k$ nearest neighbor of this point. The higher the score, the easier it is to be an outlier.\n",
    "\n",
    "$$\\text{COF}_p = \\frac{\\text{ACD}_p}{\\frac{\\sum ACD_k}{k}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec0836ba",
   "metadata": {},
   "source": [
    "#### Isolation Forest\n",
    "[Isolation Forest](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest) detects the outliers based on ensembling binary decision trees to isolate outliers from the others. Relying on the characteristics of outliers are few and difference, IForest built each tree using sub-sample of dataset, then randomly seleted a feature and a random threshold to split the tree. The process of splitting continue until all instance has been isolated or the tree reach the maximum height or all same-value data points go into same node. The outliers will have short path to the root than others, especially when all tree in the forest say that. Formula of anomaly score depends on the average of path length to the root $\\overline{h_p}$, number of instances in node - $n$ and unsuccesfull path in binary search tree $c(n)$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{iForest}_p &= 2^{-\\frac{\\overline{h}_p}{c(n)}} \\\\\n",
    "c(n) &= 2 \\cdot (\\log(n-1)+0.577) - \\frac{2(n-1)}{n} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Iforest just requires 2 params which are number of trees and sub-sample size - it works very well with small sample sizes and high-dimensional data, time consuming of this method is also fast and it can apply for all 4 types of anomaly. But iForest also has a disadvantage that a node in an iTree is split based on a threshold value, the data is split into left and right branches resulting in horizontal and vertical branch cuts - this will lead to some outliers are passed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
