
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introduction &#8212; Data Science with Python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science with Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Python Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-python-basic-concepts.html">
   Python: Basic Concepts
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Visualization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="09-matplotlib-graph-construction.html">
   Matplotlib: Graph Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="27-plotly-interactive-visualization.html">
   Plotly: Interactive Visualization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="36-numpy-gradient-descent.html">
   Python: Gradient Descent
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/hungpq7/tabular-book/master?urlpath=tree/content/34. [xgboost] Tree Boosting.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/hungpq7/tabular-book/blob/master/content/34. [xgboost] Tree Boosting.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/hungpq7/tabular-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/content/34. [xgboost] Tree Boosting.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Introduction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#xgboost">
   1. XGBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm">
     1.1. Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#objective-function">
       Objective function
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost-tree">
       XGBoost tree
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#key-features">
       Key features
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameters">
     1.2. Hyperparameters
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     1.3. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scikit-learn-interface">
       Scikit-learn interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost-interface">
       XGBoost interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multilabel-problem">
       Multilabel problem
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auto-xgboost">
     1.4. Auto XGBoost
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lightgbm">
   2. LightGBM
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.1. Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-side-sampling">
       One-side sampling
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exclusive-feature-bundling">
       Exclusive feature bundling
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Key features
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     2.2. Hyperparameters
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     2.3. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Scikit-learn interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lightgbm-interface">
       LightGBM interface
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#catboost">
   3. CatBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     3.1. Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ordered-encoding">
       Ordered encoding
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ordered-boosting">
       Ordered boosting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id7">
       Key features
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     3.2. Hyperparameters
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     3.2. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id10">
       Scikit-learn interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-analysis">
       Model analysis
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#catboost-interface">
       CatBoost interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id11">
       Multilabel problem
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ngboost">
   4. NGBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     4.1. Algorithm
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     4.2. Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Introduction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#xgboost">
   1. XGBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm">
     1.1. Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#objective-function">
       Objective function
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost-tree">
       XGBoost tree
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#key-features">
       Key features
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameters">
     1.2. Hyperparameters
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     1.3. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scikit-learn-interface">
       Scikit-learn interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost-interface">
       XGBoost interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multilabel-problem">
       Multilabel problem
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auto-xgboost">
     1.4. Auto XGBoost
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lightgbm">
   2. LightGBM
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.1. Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-side-sampling">
       One-side sampling
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exclusive-feature-bundling">
       Exclusive feature bundling
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Key features
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     2.2. Hyperparameters
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     2.3. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Scikit-learn interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lightgbm-interface">
       LightGBM interface
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#catboost">
   3. CatBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     3.1. Algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ordered-encoding">
       Ordered encoding
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ordered-boosting">
       Ordered boosting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id7">
       Key features
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     3.2. Hyperparameters
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     3.2. Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id10">
       Scikit-learn interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-analysis">
       Model analysis
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#catboost-interface">
       CatBoost interface
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id11">
       Multilabel problem
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ngboost">
   4. NGBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     4.1. Algorithm
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     4.2. Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h1>
<p>Continue from the previous topic <em>Ensemble Learning</em>, this topic goes deeper in the Gradient Boosting branch. We are going to learn about each of the trio XGBoost, LightGBM and CatBoost, which are considered state-of-the-art algorithms for tabular data.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="xgboost">
<h1>1. XGBoost<a class="headerlink" href="#xgboost" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/XGBoost">XGBoost</a> (eXtreme Gradient Boosting) is a variant of GBM released in 2014 and dominated competitions in 2015. XGBoost develops an unique tree mechanic using both <a class="reference external" href="https://en.wikipedia.org/wiki/Gradient">gradient</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Hessian_matrix">hessian</a> of the loss function (these two terms refer to the first and second order derivatives in the context of univariate Calculus), while inherits the boosting process from GBM. Until now, it is still widely used by Data Scientists to achive state-of-the-art performance on tabular datasets.</p>
<section id="algorithm">
<h2>1.1. Algorithm<a class="headerlink" href="#algorithm" title="Permalink to this headline">#</a></h2>
<section id="objective-function">
<h3>Objective function<a class="headerlink" href="#objective-function" title="Permalink to this headline">#</a></h3>
<p>XGBoost’s authors strongly emphasize the role of regularization, which is usually treated less carefully in other tree packages. The objective function at step <span class="math notranslate nohighlight">\(t\)</span> is defined so that it consists of two components, the loss term <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and the regularization term <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p>
<p><em>Loss term</em>. There are a lot of choices for loss functions, but let’s start with the most friendly one, sum squared error. For each observation <span class="math notranslate nohighlight">\(\mathbf{s}_n\)</span>, the corresponding gradient is <span class="math notranslate nohighlight">\(g_n=\hat{y}_n-y_n\)</span> (refers to the residual) and hessian is <span class="math notranslate nohighlight">\(h_n=1\)</span> (refers to the size); the same idea can be applied to understand other loss functions. In general, due to the complexity of loss functions, XGBoost takes their second-order Taylor approximation. Notice that <span class="math notranslate nohighlight">\(\mathcal{L}\left(\hat{y}_n^{(t-1)}\right)\)</span> is a constant and is not going to be considered when minimizing the loss term.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathcal{L}\left(\hat{y}_n^{(t-1)}+f(\mathbf{s}_n)\right)
\approx\mathcal{L}\left(\hat{y}_n^{(t-1)}\right)+g_n^{(t)}f(\mathbf{s}_n)+\frac{1}{2}h_n^{(t)}f(\mathbf{s}_n)^2
\end{aligned}\]</div>
<p><em>Regularization term</em>. We denote <span class="math notranslate nohighlight">\(M\)</span> - the number of leaves and <span class="math notranslate nohighlight">\(w_m\)</span> - the predicted value by <span class="math notranslate nohighlight">\(m^{th}\)</span> leaf (<span class="math notranslate nohighlight">\(m=1,2,\dots,M\)</span>). The regularization term is defined using tree complexity as follow:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}(f)=\gamma M+\frac{1}{2}\lambda\sum_{m=1}^{M}{w_m^2}\]</div>
<p>Now we have prepared <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, the next step is to adding them up. Unfortunately, these two term are indexed differently. Notice that all samples in a leaf get the same prediction, we change the index of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> by defining <span class="math notranslate nohighlight">\(G_m\)</span> and <span class="math notranslate nohighlight">\(H_m\)</span>, the sum of sample gradients and sample hessians in the leaf <span class="math notranslate nohighlight">\(m^{th}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{O}^{(t)}
&amp;= \mathcal{L}^{(t)}+\mathcal{R}^{(t)} \\
&amp;= \left(\sum_{n=1}^{N}g_n f(\mathbf{s}_n)+\sum_{n=1}^{N}\frac{1}{2}h_n f(\mathbf{s}_n)^2\right)+
    \left(\gamma M+\frac{1}{2}\lambda\sum_{m=1}^{M}{w_m^2}\right) \\
&amp;= \left(\sum_{m=1}^M G_m w_m+\frac{1}{2}\sum_{m=1}^M H_m w_m^2\right)+
    \left(\gamma M+\frac{1}{2}\lambda\sum_{m=1}^{M}{w_m^2}\right) \\
&amp;= \sum_{m=1}^M\left(G_m w_m+\frac{H_m+\lambda}{2}w_m^2\right)+\gamma M
\end{aligned}\end{split}\]</div>
</section>
<section id="xgboost-tree">
<h3>XGBoost tree<a class="headerlink" href="#xgboost-tree" title="Permalink to this headline">#</a></h3>
<p>Using the objective function <span class="math notranslate nohighlight">\(\mathcal{O}(w_m)\)</span> constructed in the previous section, which is quadratic, we solve for <span class="math notranslate nohighlight">\(w_m\)</span> to minimize <span class="math notranslate nohighlight">\(\mathcal{O}\)</span>:</p>
<div class="math notranslate nohighlight">
\[w_m=\arg\min\mathcal{O}(w_m)=-\frac{G_m}{H_m+\lambda}\]</div>
<p>Plug the solution to <span class="math notranslate nohighlight">\(\mathcal{O}(w_m)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\mathcal{O}(w_m)=-\frac{1}{2}\sum_{m=1}^{M}\frac{G_m^2}{H_m+\lambda}+\gamma M\]</div>
<p>The second equation also measures the impurity of a node (lower is better) and shares the same functionality with Gini and Entropy formulas in traditional CART. Another difference in XGBoost tree is that it doesn’t normalize impurity values of child nodes before calculating information gain. Now let’s analyze the regularization effect of <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(\gamma\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> is intended to reduce the <span class="math notranslate nohighlight">\(G_m^2\div H_m\)</span> ratio: the higher <span class="math notranslate nohighlight">\(\lambda\)</span> is, the more reduction it makes in the impurity. Besides, <span class="math notranslate nohighlight">\(\lambda\)</span> is very sensitive to node sizes: it will pack a punch on a small leaf.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> works as a threshold, forcing the information gain of a split must be greater than <span class="math notranslate nohighlight">\(\gamma\)</span> so that the split can be performed.</p></li>
</ul>
</section>
<section id="key-features">
<h3>Key features<a class="headerlink" href="#key-features" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions">Histogram-base split finding</a>. XGBoost uses an approximate solution based on histograms to find optimal candidate when constructing Decsision Trees.</p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/dart.html">DART</a>. XGBoost implements DART, a technique inspired by dropout from Deep Learning.</p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html">Monotonic constraints</a>. An interesting feature that allows specifying real-world constraints between features and target that obviously exists has but cannot be learned from data. For example, larger houses (<span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span>) should be more expensive (<span class="math notranslate nohighlight">\(\mathbf{y}\)</span>) than smaller ones, then we write:
<code style='font-size:13px;'>[0,1,0,0,…]</code> which means an increasing constrain on the second feature.
By enforcing such a relationship, we are helping our model to learn patterns better.</p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_constraint.html">Feature interaction constraints</a>. Another constraint but for features only. Features in a group we are specifying can only <em>interact</em> with other group members but with no other feature. The term <em>interact</em> refers to features who are in the same decision path.</p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html">Categorical data support</a></p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html">Multiple ouputs support</a></p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/gpu/index.html">GPU support</a></p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/prediction.html#early-stopping">Early stopping</a></p></li>
</ul>
</section>
</section>
<section id="hyperparameters">
<h2>1.2. Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this headline">#</a></h2>
<p>XGBoost implementation has a huge number of <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/parameter.html">hyperparameters</a>, devided into many types.</p>
<p><em>Boosting parameters</em>. The configurations for boosting.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>n_estimators</code>: the number of boosting stages (<span class="math notranslate nohighlight">\(T\)</span>), defaults to <em>100</em>. Larger is usually better, but should go with a lower <em>learning_rate</em> and an <em>early_stopping_round</em> provided. Lower can speed up training.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>learning_rate</code>: the learning rate (<span class="math notranslate nohighlight">\(\eta\)</span>), defaults to <em>0.3</em>. Same usage as in GBM.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>early_stopping_round</code>: the maximum number of iterations without improvements, defaults to <em>0</em> (disabled). Keeping a low enough value may make boosting stop earlier, thus reduces the overall training time. Usually be set around <span class="math notranslate nohighlight">\(10\%\,T\)</span>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>booster</code>: the ensemble method, defaults to <em>gbtree</em> (traditional GBM). Other options are <em>gblinear</em> (using linear model as base learner) and <em>dart</em> (Dropouts meets Multiple Additive Regression Trees).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>tree_method</code>: the tree construction strategy, defaults to <em>auto</em> (heuristic selection). Other options are <em>approx</em> (quantile sketch and gradient histogram), <em>hist</em> (histogram-based split finding) and <em>gpu_hist</em> (GPU implementation of <em>hist</em>).</p></li>
</ul>
<p><em>Bagging parameters</em>. XGBoost even includes more bagging parameters than Scikit-learn does.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>subsample</code>: the ratio of instances used in each tree, defaults to <em>1</em>. A lower value will increase the randomness between trees that deals with overfitting and may speed up training.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>sampling_method</code>: the sampling method, defaults to <em>uniform</em> (treats samples equally). The other value is <em>gradient_based</em> (sample weight is <span class="math notranslate nohighlight">\(\sqrt{g_n^2+\lambda h_n^2}\)</span>, only avaiable in GPU training)</p></li>
<li><p><code style='font-size:13px; color:firebrick'>colsample_bytree</code>
<code style='font-size:13px; color:firebrick'>colsample_bylevel</code>
<code style='font-size:13px; color:firebrick'>colsample_bynode</code>: the ratio of features used in each tree/level/node, all default to <em>1.0</em> (enabled). These 3 hyperparameters can be stacked up.</p></li>
</ul>
<p><em>Tree learning parameters</em>. Most parameters in this group are for prunning trees in order to deal with overfitting. Due to the fact that pruned trees are shallower, the training is also faster.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>max_depth</code>: the maximum depth of each tree, defaults to <em>6</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_leaves</code>: the maximum number of leaves of each tree, defaults to <em>0</em> (no limit).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>min_child_weight</code>: the minimum sum of sample hessian (<span class="math notranslate nohighlight">\(G\)</span>) a node must have, defaults to <em>1</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_bin</code>: the maximum number of bins for histogram-based split finding, defaults to <em>256</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>gamma</code>: <span class="math notranslate nohighlight">\(\gamma\)</span>, the regularization term on number of leaves, defaults to <em>0.0</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>reg_alpha</code> and <code style='font-size:13px; color:firebrick'>reg_lambda</code>: the <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> regularization terms, both default to <em>0</em>. Optimal values are <span class="math notranslate nohighlight">\(10^k\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is around <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>grow_policy</code>: the method of adding new nodes to a tree, defaults to <em>depthwise</em> (prefers nodes closer to the root). The other option is <em>lossguide</em> (prefers nodes with highest loss reduction).</p></li>
</ul>
<p><em>DART’s parameters</em></p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>rate_drop</code>: the fraction of previous trees to drop during the dropout, defaults to <em>0.0</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>one_drop</code>: whether to use epsilon dropout, defaults to <em>0</em>. Setting this flag to <em>1</em> indicates allowing at least one tree is always droppped during dropout.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>skip_drop</code>: the probability of skipping the dropout procedure during a boosting iteration, defaults to <em>0.0</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>sample_type</code>: the type of sampling, defaults to <em>uniform</em>. The other option is <em>weight</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>normalize_type</code>: the type of normalization, defaults to <em>tree</em> (new trees have the same weight of each of dropped trees). The other option is <em>forest</em> (new trees have the same weight of sum of dropped trees).</p></li>
</ul>
<p><em>Others</em></p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>monotone_constraints</code>: whether to use monotonic constraints, defaults to <em>None</em>. Specify by a dictionary or a list using feature names or positions, along with type of monotonic (<em>0</em> for no constraint, <em>-1</em> for decreasing and <em>1</em> for increasing).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>interaction_constraints</code>: whether to use feature interaction constraints, defaults to <em>None</em>. Specify by a list of feature groups where features in each group have interation with the others.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>scale_pos_weight</code>: the weight attached to positive sample, defaults to <em>1</em>. Typical value is <em>nNegative/nPositive</em>, which is used to handle imbalanced classification data.</p></li>
</ul>
</section>
<section id="implementation">
<h2>1.3. Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h2>
<p>XGBoost supports a lot of customizations and useful features:</p>
<ul class="simple">
<li><p>It accepts a wide range of different loss functions, can be either a <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters">built-in one</a>, or a <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html#customized-objective-function">user-defined funnction</a> with the signature:
<code style='font-size:13px'>loss(yPred, dTrain) -&gt; (grad, hess)</code>.</p></li>
<li><p>You can provide XGBoost validation sets and evaluation metrics, it will print out evaluation results at each stage. XGBoost allows you to control the logging frequency.</p></li>
<li><p>It accepts either <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters">built-in metrics</a> or <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html#customized-metric-function">user-defined metrics</a> with the signature:
<code style='font-size:13px'>eval(yPred, dTrain) -&gt; (evalName, evalResult)</code>.</p></li>
<li><p>It accepts validation set(s) and their corresponding names.</p></li>
<li><p>You can create a <a class="reference external" href="https://en.wikipedia.org/wiki/Callback_(computer_programming)">callback</a> that activates <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/python/callbacks.html">early stopping</a> if one of the metrics doesn’t improve after a number of rounds. At least one validation set must be provided to active early stopping.</p></li>
<li><p>It can continue training if provided an existing model.</p></li>
<li><p>XGBoost supports 2 feature importances calculating strategies: <em>gain</em>, <em>weight</em> and <em>cover</em>.</p></li>
<li><p>XGBoost has an unique data type, <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/python/python_intro.html#data-interface">DMatrix</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span> <span class="k">as</span> <span class="nn">dt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;seaborn&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn-whitegrid&#39;</span><span class="p">])</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span> <span class="k">as</span> <span class="n">AUC</span><span class="p">,</span> <span class="n">fbeta_score</span> <span class="k">as</span> <span class="n">FScore</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span><span class="p">,</span> <span class="n">XGBRegressor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfCancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/breast_cancer.csv&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xTest</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yTest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xValid</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yValid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="scikit-learn-interface">
<h3>Scikit-learn interface<a class="headerlink" href="#scikit-learn-interface" title="Permalink to this headline">#</a></h3>
<p>The algorithm initialization syntax using
<code style='font-size:13px'><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier">XGBClassifier</a></code>
and
<code style='font-size:13px'><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor">XGBRegressor</a></code>
classes is the same as in Scikit-learn. Most of XGBoost’s useful features are accessed using the
<code style='font-size:13px'><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier.fit">fit()</a></code>
method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nIter</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">nIter</span><span class="o">*</span><span class="mf">0.05</span><span class="p">),</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">EvaluationMonitor</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">LearningRateScheduler</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="mf">0.3</span><span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">i</span><span class="p">),</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">TrainingCheckPoint</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s1">&#39;output/xgboost&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;checkpoint&#39;</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="c1"># create a model with 200 iterations</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="c1"># hyperparameters</span>
    <span class="n">num_leaves</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="n">nIter</span><span class="p">,</span>
    
    <span class="c1"># configurations</span>
    <span class="n">verbosity</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">importance_type</span><span class="o">=</span><span class="s1">&#39;gain&#39;</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="n">eval_metric</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># train the model with early stopping of 10% of number of trees</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span>
    <span class="c1"># eval_set=[(xValid, yValid)],</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0]	validation_0-logloss:0.65236	validation_0-auc:0.97855
[0]	validation_0-logloss:0.65236	validation_0-auc:0.97855
[1]	validation_0-logloss:0.45596	validation_0-auc:0.98125
[2]	validation_0-logloss:0.33452	validation_0-auc:0.99409
[3]	validation_0-logloss:0.25562	validation_0-auc:0.99544
[4]	validation_0-logloss:0.20158	validation_0-auc:0.99527
[5]	validation_0-logloss:0.16317	validation_0-auc:0.99595
[6]	validation_0-logloss:0.13233	validation_0-auc:0.99662
[7]	validation_0-logloss:0.10910	validation_0-auc:0.99662
[8]	validation_0-logloss:0.09481	validation_0-auc:0.99764
[9]	validation_0-logloss:0.08584	validation_0-auc:0.99797
[10]	validation_0-logloss:0.07739	validation_0-auc:0.99764
[10]	validation_0-logloss:0.07739	validation_0-auc:0.99764
[11]	validation_0-logloss:0.07212	validation_0-auc:0.99764
[12]	validation_0-logloss:0.06883	validation_0-auc:0.99764
[13]	validation_0-logloss:0.06582	validation_0-auc:0.99831
[14]	validation_0-logloss:0.06230	validation_0-auc:0.99831
[15]	validation_0-logloss:0.06294	validation_0-auc:0.99797
[16]	validation_0-logloss:0.05914	validation_0-auc:0.99831
[17]	validation_0-logloss:0.05492	validation_0-auc:0.99865
[18]	validation_0-logloss:0.05488	validation_0-auc:0.99865
[19]	validation_0-logloss:0.05245	validation_0-auc:0.99932
[20]	validation_0-logloss:0.05197	validation_0-auc:0.99865
[20]	validation_0-logloss:0.05197	validation_0-auc:0.99865
[21]	validation_0-logloss:0.04920	validation_0-auc:0.99899
[22]	validation_0-logloss:0.04885	validation_0-auc:0.99899
[23]	validation_0-logloss:0.05037	validation_0-auc:0.99899
[24]	validation_0-logloss:0.04817	validation_0-auc:0.99899
[25]	validation_0-logloss:0.04925	validation_0-auc:0.99899
[26]	validation_0-logloss:0.04881	validation_0-auc:0.99899
[27]	validation_0-logloss:0.04581	validation_0-auc:0.99899
[28]	validation_0-logloss:0.04429	validation_0-auc:0.99899
[29]	validation_0-logloss:0.04409	validation_0-auc:0.99899
[29]	validation_0-logloss:0.04409	validation_0-auc:0.99899
0:00:00.194506
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xTest</span><span class="p">)[:</span> <span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestPred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = 0.9807 [XGBClassifier]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.0026, 0.0205, 0.    , 0.    , 0.0046, 0.    , 0.0014, 0.0502,
       0.0044, 0.0006, 0.0033, 0.0025, 0.    , 0.0062, 0.0002, 0.0297,
       0.001 , 0.004 , 0.007 , 0.    , 0.1173, 0.0454, 0.5737, 0.0291,
       0.0165, 0.0177, 0.0157, 0.038 , 0.0031, 0.0054], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;output/xgboost/model.json&#39;</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="n">modelLoaded</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">()</span>
<span class="n">modelLoaded</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:722: UserWarning: callbacks is not saved in Scikit-Learn meta.
  warnings.warn(str(k) + &#39; is not saved in Scikit-Learn meta.&#39;, UserWarning)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nTop</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_names_in_</span><span class="p">,</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">})</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;importance&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;importance &gt; 0&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="n">nTop</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;importance&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;teal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/34. [xgboost] Tree Boosting_20_0.png" src="../_images/34. [xgboost] Tree Boosting_20_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">15</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/34. [xgboost] Tree Boosting_21_0.png" src="../_images/34. [xgboost] Tree Boosting_21_0.png" />
</div>
</div>
</section>
<section id="xgboost-interface">
<h3>XGBoost interface<a class="headerlink" href="#xgboost-interface" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfCancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/breast_cancer.csv&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xTest</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yTest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xValid</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yValid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dTrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>
<span class="n">dValid</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">xValid</span><span class="p">,</span> <span class="n">yValid</span><span class="p">)</span>
<span class="n">dTest</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">xTest</span><span class="p">,</span> <span class="n">yTest</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nIter</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="s1">&#39;max_leaves&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">nIter</span><span class="o">*</span><span class="mf">0.1</span><span class="p">),</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">EvaluationMonitor</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">LearningRateScheduler</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="mf">0.3</span><span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">i</span><span class="p">),</span>
    <span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">TrainingCheckPoint</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s1">&#39;output/xgboost&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;checkpoint&#39;</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">dTrain</span><span class="p">,</span>
    <span class="n">evals</span><span class="o">=</span><span class="p">[(</span><span class="n">dValid</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">dTrain</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="n">nIter</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0]	valid-logloss:0.47962	train-logloss:0.46577
[0]	valid-logloss:0.47962	train-logloss:0.46577
[1]	valid-logloss:0.35229	train-logloss:0.33031
[2]	valid-logloss:0.26789	train-logloss:0.24432
[3]	valid-logloss:0.21079	train-logloss:0.18088
[4]	valid-logloss:0.16759	train-logloss:0.13642
[5]	valid-logloss:0.13431	train-logloss:0.10408
[5]	valid-logloss:0.13431	train-logloss:0.10408
[6]	valid-logloss:0.11025	train-logloss:0.08186
[7]	valid-logloss:0.09418	train-logloss:0.06435
[8]	valid-logloss:0.08337	train-logloss:0.05263
[9]	valid-logloss:0.07342	train-logloss:0.04346
[9]	valid-logloss:0.07342	train-logloss:0.04346
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dTest</span><span class="p">)</span> <span class="c1"># predict probability</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestPred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = 0.9659 [Booster]
</pre></div>
</div>
</div>
</div>
</section>
<section id="multilabel-problem">
<h3>Multilabel problem<a class="headerlink" href="#multilabel-problem" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfEmotionTrain</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/emotions_train.csv&#39;</span><span class="p">)</span>
<span class="n">dfEmotionTest</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/emotions_test.csv&#39;</span><span class="p">)</span>

<span class="n">xTrain</span> <span class="o">=</span> <span class="n">dfEmotionTrain</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span>
<span class="n">yTrain</span> <span class="o">=</span> <span class="n">dfEmotionTrain</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">6</span><span class="p">:]</span>
<span class="n">xTest</span> <span class="o">=</span> <span class="n">dfEmotionTest</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span>
<span class="n">yTest</span> <span class="o">=</span> <span class="n">dfEmotionTest</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">6</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">xTrain</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">yTrain</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(391, 72) (391, 6)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;criterion&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">],</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span>
    <span class="s1">&#39;random_state&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="c1"># hyperparameters</span>
    <span class="n">num_leaves</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="n">nIter</span><span class="p">,</span>
    
    <span class="c1"># configurations</span>
    <span class="n">verbosity</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">importance_type</span><span class="o">=</span><span class="s1">&#39;gain&#39;</span><span class="p">,</span>
    <span class="c1"># callbacks=callbacks,</span>
    <span class="n">eval_metric</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> 
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0:00:04.438000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestProb</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xTest</span><span class="p">)</span>
<span class="n">listAuc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestProb</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">listAuc</span> <span class="o">=</span> <span class="n">listAuc</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">listAuc</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = [0.8481, 0.6956, 0.815, 0.9551, 0.8248, 0.8852] [XGBClassifier]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="auto-xgboost">
<h2>1.4. Auto XGBoost<a class="headerlink" href="#auto-xgboost" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span> <span class="k">as</span> <span class="nn">dt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span> <span class="k">as</span> <span class="n">AUC</span><span class="p">,</span> <span class="n">fbeta_score</span> <span class="k">as</span> <span class="n">FScore</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span><span class="p">,</span> <span class="n">XGBRegressor</span>
<span class="kn">from</span> <span class="nn">flaml</span> <span class="kn">import</span> <span class="n">AutoML</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfCancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/breast_cancer.csv&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xTest</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yTest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xValid</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yValid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoML</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">time_budget</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
    <span class="n">estimator_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;xgboost&#39;</span><span class="p">],</span>
    <span class="n">eval_method</span><span class="o">=</span><span class="s1">&#39;cv&#39;</span><span class="p">,</span>
    <span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0:00:30.719410
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xTest</span><span class="p">)[:</span> <span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestPred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = 0.9750 [AutoML]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="lightgbm">
<h1>2. LightGBM<a class="headerlink" href="#lightgbm" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/">LightGBM</a> (Light Gradient Boosting Machines) is an implementation of Gradient Boosting developed by Microsoft in 2016, in attempt to deal with large datasets. According to <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM paper</a>, the main cost in GBDT lies in learning the decision trees, and the most time-consuming part in learning a decision tree is to find the best split points. XGBoost has developed histogram-based split finding that reduces the complexity of the algorithm from <span class="math notranslate nohighlight">\(O(\#instance \times \#feature)\)</span> to <span class="math notranslate nohighlight">\(O(\#bin \times \#feature)\)</span> where <span class="math notranslate nohighlight">\(\#bin\ll \#instance\)</span>, which outperforms other implementations in terms of memory consumption.</p>
<p>However, XGBoost still does not meet the satisfactory of scalability when working on large datasets. To improve the performance of previous implementations, LightGBM introduces two novels techniques, GOSS (Gradient-based One-Side Sampling) for reducing data instances and EFB (Exclusive Feature Bundling) for reducing data dimensions.</p>
<section id="id1">
<h2>2.1. Algorithm<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<section id="one-side-sampling">
<h3>One-side sampling<a class="headerlink" href="#one-side-sampling" title="Permalink to this headline">#</a></h3>
<p>As the purpose of GOSS is to smartly sample data, it requires sample weights to achieve this. Althought Gradient Boosting does not have any native weight as in Adaptive Boosting, we can use residuals (generally gradients) as pseudo-weights. The GOSS algorithm is implemented right before the training of each weak learner and is described as follows.</p>
<p><em>Input:</em></p>
<ul class="simple">
<li><p>A dataset of <span class="math notranslate nohighlight">\(N\)</span> observations and their corresponding gradients computed from the most recent predictor</p></li>
<li><p><span class="math notranslate nohighlight">\(a\)</span> - the sampling ratio of large gradient data and <span class="math notranslate nohighlight">\(b\)</span> - the sampling ratio of small gradient data (<span class="math notranslate nohighlight">\(a+b\)</span> should not be greater than <span class="math notranslate nohighlight">\(1\)</span>)</p></li>
</ul>
<p><em>Step 1:</em> Sort (in descending order) the observations by the absolute values of their gradients and select top <span class="math notranslate nohighlight">\(\lfloor aN \rfloor\)</span> observations. This under-trained set is what the training of the next tree should focus on.</p>
<p><em>Step 2:</em> Perform random sampling <span class="math notranslate nohighlight">\(\lfloor bN \rfloor\)</span> observations from the rest of the data. In order to compensate the information losses that affect data distribution, each instance in this well-trained set is amplified by a constant <span class="math notranslate nohighlight">\(\dfrac{1-a}{b}&gt;1\)</span> when calculating information gain in the next tree. “One-side sampling” in fact means that sampling is done on the small gradient side.</p>
<p><em>Step 3:</em> Combine the under-trained set and the well-trained set above to form a single set to be used in learning the Decision Tree. By using this technique, <em>we put more focus on the under-trained instances without changing the original data distribution by much</em>.</p>
</section>
<section id="exclusive-feature-bundling">
<h3>Exclusive feature bundling<a class="headerlink" href="#exclusive-feature-bundling" title="Permalink to this headline">#</a></h3>
<p>One of the biggest problems that occur in high dimensional data is the sparsity. Sparse features spend A specific case is when some features <em>never</em> or <em>rarely</em> take non-zero values <em>simultaneously</em>; such features are called <em>mutually exclusive</em>. A typical situation is that features returned by an one-hot encoder are perfectly mutually exclusive. This situation is rarely met in real-world datasets, thus the EFB algorithm allows a small fraction of conflicts. To achieve this, two questions need to be answered: (1) <em>Which features should be bundled together?</em> and (2) <em>How to construct the bundles?</em>.</p>
<p><em>Step 1:</em> Greedy bundling.</p>
<ul class="simple">
<li><p>Construct a graph whose weights correspond the total conflicts between features, then order the features by their degrees in the graph in the descending order. By using this ordered list, we are priortizing features with high probability of having conflicts. Another ordering strategy which further improves the efficiency is sorting by the number of non-zero values.</p></li>
<li><p>Check each feature in the ordered list, either assign it to an existing bundle if the conflict rate is smaller than a preconfigured constant, or create a new bundle.</p></li>
</ul>
<p><em>Step 2:</em> Merging exclusive features. The key is to ensure features’ values do not overlap and can be identified after bundling. This can be achieve by adding an offset to the original values of each feature so that features reside in different bins. Assume a bundle has 3 features <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_3\)</span>; their values ranges are <span class="math notranslate nohighlight">\([0,20)\)</span>, <span class="math notranslate nohighlight">\([10,15)\)</span> and <span class="math notranslate nohighlight">\([0,5)\)</span>. We add to these features consecutively offset values of <span class="math notranslate nohighlight">\(0\)</span>, <span class="math notranslate nohighlight">\(20\)</span> and <span class="math notranslate nohighlight">\(35\)</span>, resulting in new feature ranges of <span class="math notranslate nohighlight">\([0,20)\)</span>, <span class="math notranslate nohighlight">\([30,35)\)</span> and <span class="math notranslate nohighlight">\([35,40)\)</span>. Now it’s safe to merge all the three features into a single bundle with range <span class="math notranslate nohighlight">\([0,40)\)</span>.</p>
<p>The EFB algorithm is processed only once before training, however, because of having the complexity of <span class="math notranslate nohighlight">\(O(\#feature^2)\)</span>, it may still suffer if there are millions of features.</p>
</section>
<section id="id2">
<h3>Key features<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-speed-and-memory-usage">Optimization in speed and memory usage</a>. As in XGBoost, LightGBM also uses histogram-based split finding and histogram substraction to reduce calculation cost.</p></li>
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-accuracy">Optimization in accuracy</a>. LightGBM grows trees leaf-wise instead of level-wise as in most Decision Tree learning algorithms. This strategy chooses the leaf with highest delta loss to grow. For the same amount of leaves, leaf-wise approach tends to achieve a lower bias. This may sometimes cause overfitting, but this issue can be address by decreasing the max depth or number of leaves of tress.</p></li>
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support">Categorical features support</a>. LightGBM has a built-in mechanism to work with categorical features.</p></li>
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning">Optimization in distributed learning</a>. LightGBM supports three parallelization algorithms which fit in different scenarios.</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:left head"><p>Small data</p></th>
<th class="text-align:left head"><p>Large data</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><strong>Small dimensional</strong></p></td>
<td class="text-align:left"><p>Feature parallel</p></td>
<td class="text-align:left"><p>Data parallel</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><strong>High dimensional</strong></p></td>
<td class="text-align:left"><p>Feaure parallel</p></td>
<td class="text-align:left"><p>Voting parallel</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#continued-training-with-input-score">Continued training</a>. LightGBM can continue training with initial scores.</p></li>
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html">GPU support</a>. LightGBM supports GPU training for accelaration.</p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io/en/stable/tutorials/dart.html">DART</a>. LightGBM also supports the DART boosting technique as XGBoost does.</p></li>
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#linear_tree">Piece-Wise Linear Regression Trees</a>.</p></li>
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#monotone_constraints_method">Monotone constraints</a>.</p></li>
<li><p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#cost-efficient-gradient-boosting">Cost efficient gradient boosting</a>.</p></li>
</ul>
</section>
</section>
<section id="id3">
<h2>2.2. Hyperparameters<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p>The LightGBM algorithm has a huge number of <a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">hyperparameters</a>, devided into many types. Some of them have aliases, which makes LightGBM compatible with other libraries. LightGBM has a <a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html">tuning guideline</a> page.</p>
<p><em>Boosting parameters</em>. The configurations for boosting.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>boosting</code>: the ensemble method, defaults to <em>gdbt</em> (traditional GBM). Other options are <em>rf</em> (Random Forest), <em>goss</em> (Gradient-based One-Side Sampling) and <em>dart</em> (Dropouts meets Multiple Additive Regression Trees).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>n_estimators</code>: the number of boosting stages (<span class="math notranslate nohighlight">\(T\)</span>), defaults to <em>100</em>. Larger is usually better, but should go with a lower <em>learning_rate</em> and an <em>early_stopping_round</em> provided. Lower can speed up training.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>learning_rate</code>: the learning rate (<span class="math notranslate nohighlight">\(\eta\)</span>), defaults to <em>0.1</em>. Same usage as in GBM.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>early_stopping_round</code>: the maximum number of iterations without improvements, defaults to <em>0</em> (disabled). Keeping a low enough value may make boosting stop earlier, thus reduces the overall training time. Usually be set around <span class="math notranslate nohighlight">\(10\%\)</span> of <em>n_estimators</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>tree_learner</code>: the parallelization strategy, defaults to <em>serial</em> (single machine, no parallel). Other options are <em>feature</em>, <em>data</em> and <em>voting</em>.</p></li>
</ul>
<p><em>Bagging parameters</em>. LightGBM even includes more bagging parameters compared to Scikit-learn.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>bagging_fraction</code>: the ratio of data (instances) used in each tree, defaults to <em>1</em>. A lower value will increase the randomness between trees that deals with overfitting and may speed up training.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>bagging_freq</code>: the iteration frequency to perform bagging, defaults to <em>0</em> (disable bagging). A positive value will decrease the randomness between trees.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>pos_bagging_fraction</code> and <code style='font-size:13px; color:firebrick'>neg_bagging_fraction</code>: the ratio of positive/negative samples used in each tree, both default to <em>1</em>. This pair of parameters should be used together to handle imbalance binary classification problems.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>feature_fraction</code>: the ratio of features used in each tree, defaults to <em>1</em>. A lower value will increase the randomness between trees that deals with overfitting and may speed up training.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>feature_fraction_bynode</code>: same as <em>feature_fraction</em>, but the sampling is done on tree nodes instead of trees; also defaults to <em>1</em>. A lower value can reduce overfitting but cannot speed up training.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>extra_trees</code>: whether to use Extremely Randomized Trees or not, defaults to <em>False</em>. This parameter increases the randomness, thus can deal with overfitting.</p></li>
</ul>
<p><em>Tree learning parameters</em>. Most parameters in this group are for prunning trees in order to deal with overfitting. Due to the fact that pruned trees are shallower, the training is also faster.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>num_leaves</code>: the maximum number of leaves in each tree, defaults to <em>31</em>. Since LightGBM grows trees leaf-wise, this is the main hyperparameter to control the complexity of trees. Optimal values range from <span class="math notranslate nohighlight">\(50\%\)</span> to <span class="math notranslate nohighlight">\(100\%\)</span> of <span class="math notranslate nohighlight">\(2^{\text{max_depth}}\)</span>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>min_samples_leaf</code>: the minimum number of data a leaf must have, defaults to <em>20</em>. The next important parameter to prevent overfitting, its optimal value depends on training data size and <em>num_leaves</em>. Practical values for large datasets range from hundreds to thousands.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_depth</code>: the maximum depth of each tree, defaults to <em>-1</em> (no depth limitation). Another important parameter that controls overfitting; however, it is less effective on leaf-wise compared to level-wise implementations. A value from 3 to 13 works well for most datasets.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>min_sum_hessian_in_leaf</code>: the minimum sum of Hessian (the second derivative of the objective function for each observation) of each leaf, defaults to <em>0.001</em>. When the loss function is MSE, its second derivative is <span class="math notranslate nohighlight">\(1\)</span>; the sum of Hessian in this case equals to the number of data. For other loss function, this parameter has different meanings and also has different optimal values. Thus, unless you know what you are doing, this parameter should be left alone.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>min_gain_to_split</code>: the minimum information gain required to perform a split, defaults to <em>0</em>. In practice, very small improvements in the training loss have no meaningful impact on the generalization error of the model. A small value of this parameter is enough if used.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>reg_alpha</code> and <code style='font-size:13px; color:firebrick'>reg_lambda</code>: the <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> regularization terms, both default to <em>0</em>. Optimal values are <span class="math notranslate nohighlight">\(10^k\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is around <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>linear_tree</code>: whether to use Piece-Wise Linear Regression Trees, defaults to <em>False</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>linear_lambda</code>: the coefficient for linear tree regularization, defaults to <em>0</em>.</p></li>
</ul>
<p><em>Categorical split finding parameters</em>.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>categorical_feature</code>: specify categorical features, defaults to <em>auto</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>min_data_per_group</code>: the minimum number of data per categorical group, defaults to <em>100</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>cat_smooth</code>: the coefficient for categorical smoothing, defaults to <em>10</em>. Can reduce the effect of noises in categorical features, especially for ones with few data.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_cat_threshold</code>: the maximum number of splits considered for categorical features, defaults to <em>32</em>. Higher means more split points and larger search space. Lower reduces training time.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>cat_l2</code>: L2 regularization in categorical split, defaults to <em>10</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_cat_to_onehot</code>: maximum number of categories of a feature to use one-hot encoding, otherwise the Fisher’s split finding will be used, defaults to <em>4</em>.</p></li>
</ul>
<p><em>Histogram building parameters</em>.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>max_bin</code> and <code style='font-size:13px; color:firebrick'>max_bin_by_feature</code>: the maximum number of bins when building histograms, defaults to <em>255</em>. The later parameter takes a list of intergers to specify the max number of bins for each feature. Smaller reduces training time but may hurt the accuracy.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>min_data_in_bin</code>: the minimum bin size, defaults to <em>3</em>. This parameter prevents bins from having a small number of data, as using their boundaries as splits isn’t likely to change the final model very much. Higher value reduces training time.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>bin_construct_sample_cnt</code>: the number of observations being sampled to determine bins, defaults to <em>200,000</em>. LightGBM only uses a part of data to find histogram boundaries, thus this parameter should not be set to a lower value. A higher value obviously improves prediction power but also leads to a longer data loading time.</p></li>
</ul>
<p><em>DART’s parameters</em></p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>drop_rate</code>: the fraction of previous trees to drop during the dropout, defaults to <em>0.1</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_drop</code>: the max number of dropped trees during one boosting iteration, defaults to <em>50</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>skip_drop</code>: the probability of skipping the dropout procedure during a boosting iteration, defaults to <em>0.5</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>uniform_drop</code>: whether to use uniform drop or not, defaults to <em>False</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>xgboost_dart_mode</code>: whether to enable XGBoost DART mode, which uses a bit different shrinkage rate, defaults to <em>False</em>.</p></li>
</ul>
<p><em>GOSS’s parameters</em></p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>top_rate</code>: the sampling ratio of large gradient data, defaults to <em>0.2</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>other_rate</code>: the sampling ratio of small gradient data, defaults to <em>0.1</em>.</p></li>
</ul>
<p><em>Preprocessing parameteres</em></p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>enable_bundle</code>: whether to use the EFB algorithm or not, defaults to <em>True</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>enable_sparse</code>: whether to use sparse optimization, defaults to <em>True</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>use_missing</code>: whether to use special handle of missing values, defaults to <em>True</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>feature_pre_filter</code>: whether to ignore unsplittable features based on <em>min_samples_leaf</em>, defaults to <em>True</em>. As <em>min_samples_leaf</em> was set, some features will perform a split results in a leaf not having enough minimum number of data. Such features will be filtered out once before training. Also remember to tune <em>min_samples_leaf</em> before this parameter.</p></li>
</ul>
</section>
<section id="id4">
<h2>2.3. Implementation<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h2>
<p>LightGBM supports a lot of customizations and useful features:</p>
<ul class="simple">
<li><p>It accepts a wide range of different loss functions, can be either a <a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective">built-in one</a>, or a user-defined function with the signature:
<code style='font-size:13px'>loss(yTrue, yPred) -&gt; (grad, hess)</code>.</p></li>
<li><p>You can provide LightGBM validation sets and evaluation metrics, it will print out evaluation results at each stage. LightGBM allows you to control the logging frequency or even surpress it.</p></li>
<li><p>It accepts either <a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric">built-in metrics</a> or user-defined metrics with the signature:
<code style='font-size:13px'>eval(yTrue, yPred) -&gt; (evalName, evalResult, isHigherBetter)</code>.</p></li>
<li><p>It accepts validation set(s) and their corresponding names.</p></li>
<li><p>You can create a <a class="reference external" href="https://en.wikipedia.org/wiki/Callback_(computer_programming)">callback</a> that activates <a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.early_stopping.html">early stopping</a> if one of the metrics doesn’t improve after a number of rounds. At least one validation set must be provided to active early stopping.</p></li>
<li><p>It can continue training if provided an existing model.</p></li>
<li><p>LightGBM supports 2 feature importances calculating strategies: <em>split</em> (default) - the number of times a feature is used and <em>gain</em> - the total of information gain associated with a feature.</p></li>
<li><p>You can specify a list of integer-encoded categorical features, since LightGBM has a special mechanism to work with them.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>

<span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>
<span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span><span class="p">,</span> <span class="n">LGBMRegressor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfCancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/breast_cancer.csv&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xTest</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yTest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xValid</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yValid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="id5">
<h3>Scikit-learn interface<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h3>
<p>The algorithm initialization syntax using
<code style='font-size:13px'><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">LGBMClassifier</a></code>
and
<code style='font-size:13px'><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html">LGBMRegressor</a></code>
classes is the same as in Scikit-learn. Most of LightGBM’s useful features are accessed using the
<code style='font-size:13px'><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier.fit">fit()</a></code>
method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a model with 200 iterations</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span>
    <span class="n">num_leaves</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    
    <span class="n">verbosity</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">importance_type</span><span class="o">=</span><span class="s1">&#39;gain&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># train the model with early stopping of 10% of number of trees</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">xValid</span><span class="p">,</span> <span class="n">yValid</span><span class="p">)],</span>
    <span class="n">eval_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Valid&#39;</span><span class="p">],</span>
    <span class="n">eval_metric</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;binary_logloss&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">],</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">first_metric_only</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 20 rounds
[20]	Valid&#39;s binary_logloss: 0.245227	Valid&#39;s auc: 0.996284
[40]	Valid&#39;s binary_logloss: 0.129513	Valid&#39;s auc: 0.997635
[60]	Valid&#39;s binary_logloss: 0.0819823	Valid&#39;s auc: 0.998649
[80]	Valid&#39;s binary_logloss: 0.0640199	Valid&#39;s auc: 0.998986
[100]	Valid&#39;s binary_logloss: 0.0622647	Valid&#39;s auc: 0.998649
Early stopping, best iteration is:
[97]	Valid&#39;s binary_logloss: 0.0609187	Valid&#39;s auc: 0.998986
Evaluated only: binary_logloss
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xTest</span><span class="p">)[:</span> <span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestPred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = 0.9835 [LGBMClassifier]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([   2.7932,   61.9895,    0.7611,    0.3866,    3.8635,    0.4405,
        117.783 ,   96.1331,    3.7455,    0.2102,   10.3477,    5.8531,
         13.2454,   72.8923,    6.2065,    6.9134,    0.    ,   11.753 ,
          3.9254,    8.9424,  449.2814,  157.9406, 1878.1435,  754.576 ,
         44.5361,   43.9856,  241.9603,  494.7697,   21.146 ,    4.1357])
</pre></div>
</div>
</div>
</div>
</section>
<section id="lightgbm-interface">
<h3>LightGBM interface<a class="headerlink" href="#lightgbm-interface" title="Permalink to this headline">#</a></h3>
<p>Beside Scikit-learn API, LightGBM also provides its own training API via the
<code style='font-size:13px'><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html">train()</a></code>
function, which takes a
<code style='font-size:13px'><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html">Dataset</a></code>
object as input instead of NumPy arrays. The prediction step still takes NumPy arrays as input as in Scikit-learn. Learn more <a class="reference external" href="https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">listFeatureCategorical</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dataTrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>
<span class="n">dataValid</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">xValid</span><span class="p">,</span> <span class="n">yValid</span><span class="p">,</span> <span class="n">reference</span><span class="o">=</span><span class="n">dataTrain</span><span class="p">)</span>
<span class="n">dataTest</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">xTest</span><span class="p">,</span> <span class="n">yTest</span><span class="p">,</span> <span class="n">reference</span><span class="o">=</span><span class="n">dataTrain</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;binary&#39;</span><span class="p">,</span>
    <span class="s1">&#39;metric&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;binary_logloss&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">],</span>
    
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
    <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;goss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;num_leaves&#39;</span><span class="p">:</span> <span class="mi">31</span><span class="p">,</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="s1">&#39;feature_fraction&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
    
    <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;force_col_wise&#39;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="n">train_set</span><span class="o">=</span><span class="n">dataTrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="n">dataValid</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">first_metric_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 20 rounds
[20]	valid_0&#39;s binary_logloss: 0.246778	valid_0&#39;s auc: 0.996622
[40]	valid_0&#39;s binary_logloss: 0.139876	valid_0&#39;s auc: 0.997297
[60]	valid_0&#39;s binary_logloss: 0.0911384	valid_0&#39;s auc: 0.997973
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.12314	valid_0&#39;s auc: 0.997973
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xTest</span><span class="p">)</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestPred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = 0.9821 [Booster]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">(</span><span class="s1">&#39;split&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 3, 19,  4,  0,  9,  5, 12, 19,  3,  6, 10, 20,  7, 23,  5, 11,  2,
        5,  4,  3, 17, 20, 17, 20,  5,  8, 17, 46, 10,  3])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="catboost">
<h1>3. CatBoost<a class="headerlink" href="#catboost" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Catboost">CatBoost</a> (Categorical Boosting) is a variant of Gradient Boosting developed by Yandex in 2017. CatBoost introduces <em>ordered encoding</em> and <em>ordered boosting</em>, two advanced algorithms to fight <em>prediction shift</em>, a special kind of target leakage presents in all other Graddient Boosting implementations. The implementation of CatBoost prepares default values for each hyperparameter carefully, which makes CatBoost works well without many effort in hyperparameter tuning.</p>
<section id="id6">
<h2>3.1. Algorithm<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h2>
<section id="ordered-encoding">
<h3>Ordered encoding<a class="headerlink" href="#ordered-encoding" title="Permalink to this headline">#</a></h3>
<p>First, let’s talk about a popular method in encoding categorical features: target encoding. This method simply replaces each category (<span class="math notranslate nohighlight">\(x_k\)</span>) of <em>a feature</em> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> by the mean of <em>the target</em> (<span class="math notranslate nohighlight">\(\mathbf{y}\)</span>) of the corresponding <em>subset</em> (<span class="math notranslate nohighlight">\(\mathbf{1}_{\{x_n=x_k\}}\)</span>). It is formally given by:</p>
<div class="math notranslate nohighlight">
\[x_k\leftarrow
\frac
  {\sum_{n=1}^{N}\mathbf{1}_{\{x_n=x_k\}}y_n+a\bar{y}}
  {\sum_{n=1}^{N}\mathbf{1}_{\{x_n=x_k\}}+a}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(a\)</span> is a smoothing parameter that pushes the encoding value towards the target mean <span class="math notranslate nohighlight">\(\bar{y}\)</span>. This encoding strategy easily leads to <em>target leakage</em>, when the distribution of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> given <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> differs for training and test data.</p>
<p>To handle this, CatBoost applies the above formula in an innovative way. It shuffles the dataset at each boosting iteration and uses the current order as an <em>artifical time</em>. Then, to encode a value <span class="math notranslate nohighlight">\(x_n\)</span>, CatBoost uses all available <em>historical data</em> <span class="math notranslate nohighlight">\(\mathcal{D}_n=\{x_i,y_i\}_{i=1}^{n-1}\)</span> to encode using the above formula. At first glance, this method looks wrong as it assigns different encoded values to the same category. That’s why CatBoost uses different permutations at different steps of gradient boosting procedure. This also means ordered encoding principle only works in gradient boosting where there a lot of iterations, and it does not work well standalone.</p>
</section>
<section id="ordered-boosting">
<h3>Ordered boosting<a class="headerlink" href="#ordered-boosting" title="Permalink to this headline">#</a></h3>
<p>In previous Gradient Boosting techniques, each weak model is built sequentially based on the same dataset. CatBoost authors argue that there is some kind of target leakage across iterations, which is called <em>prediction shift</em>.</p>
<p>To combat this, CatBoost authors propose a variant called Ordered Boosting, a method using the same principle with Ordered Encoding. The main difference of this method is that the residual of each data point is calculated using a model fitted on <em>historical data</em>. CatBooost also prepares a number of random permutations used in split finding and choosing leaf values, in order to neutralize the variance in the final prediction.</p>
</section>
<section id="id7">
<h3>Key features<a class="headerlink" href="#id7" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic">Categorical features transformation</a>. The most iconic feature of the algorithm.</p></li>
<li><p><a class="reference external" href="https://catboost.ai/en/docs/concepts/algorithm-main-stages_text-to-numeric">Text features transformation</a>.</p></li>
<li><p><a class="reference external" href="https://catboost.ai/en/docs/concepts/quantization">Quantization</a>. CatBoost also support histogram-based split finding like XGBoost and LightGBM do, but it goes one step further by supporting many choices of quantization modes.</p></li>
<li><p><a class="reference external" href="https://catboost.ai/en/docs/features/training-on-gpu">GPU support</a>. Like XGBoost and LightGBM, CatBoost also supports GPU training.</p></li>
<li><p><a class="reference external" href="https://catboost.ai/en/docs/concepts/overfitting-detector">Overfitting detector</a>. CatBoost has a built-in mechanism that can stop the training earlier if overfitting occurs.</p></li>
<li><p><a class="reference external" href="https://catboost.ai/en/docs/features/proceed-training">Proceed training</a>.</p></li>
<li><p><a class="reference external" href="https://catboost.ai/en/docs/concepts/algorithm-main-stages_bootstrap-options">Bootstrap options</a>. CatBoost supports different sampling options which gives regularization effect and speed up training.</p></li>
<li><p><a class="reference external" href="https://catboost.ai/en/docs/concepts/parameter-tuning#tree-growing-policy">Symmetric trees</a>. An interesting feature that CatBoost supports. This type of tree gives regularization effect, boosts computational performance and reduces the need of pruning trees.</p></li>
</ul>
</section>
</section>
<section id="id8">
<h2>3.2. Hyperparameters<a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h2>
<p>CatBoost has a huge number of <a class="reference external" href="https://catboost.ai/en/docs/references/training-parameters/">hyperparameters</a>, devided into many types. Some of them have aliases, which makes CatBoost compatible with other libraries. CatBoost has a <a class="reference external" href="https://catboost.ai/en/docs/concepts/parameter-tuning">tuning guideline</a> page.</p>
<p><em>Boosting parameters</em>. The configurations for boosting.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>boosting_type</code>: the boosting type, defaults to <em>plain</em> (normal gradient boosting). The other option is <em>ordered</em> (ordered boosting).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>n_estimators</code>: the number of boosting stages (<span class="math notranslate nohighlight">\(T\)</span>), defaults to <em>1000</em>. Larger is usually better, but should go with a lower <em>learning_rate</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>learning_rate</code>: the learning rate (<span class="math notranslate nohighlight">\(\eta\)</span>), defaults to <em>None</em> (automatically defined).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>early_stopping_rounds</code>: the maximum number of iterations without improvements, defaults to <em>False</em> (disabled). Keeping a low enough value may make boosting stop earlier, thus reduces the overall training time. Usually be set around <span class="math notranslate nohighlight">\(10\%\)</span> of <em>n_estimators</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>od_type</code>: the type of overfitting detector to use, defaults to <em>IncToDec</em>. The other option is <em>Iter</em>.</p></li>
</ul>
<p><em>Bagging parameters</em>.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>bootstrap_type</code>: strategy for sampling data, defaults to <em>None</em> (automatically defined). CatBoost supports <em>Bayesian</em>, <em>Bernoulli</em>, <em>MVS</em> and <em>Poisson</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>subsample</code>: the ratio of data for sampling, defaults to <em>None</em> (automatically defined).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>sampling_frequency</code>: the frequency to sample when building trees, defaults to <em>PerTreeLevel</em>. The other option is <em>PerTree</em>.</p></li>
</ul>
<p><em>Tree learning parameters</em>. Most parameters in this group are for prunning trees in order to deal with overfitting and increasing training speed. If symmetric tree is used, most parameters in this group are not neccessary.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>grow_policy</code>: the tree construction strategry, defaults to <em>SymmetricTree</em>. Other options are <em>Depthwise</em> and <em>Lossguide</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>min_data_in_leaf</code>: the minimum number of data a leaf must have, defaults to <em>1</em>. Cannot be used with <em>SymmetricTree</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_leaves</code>: the maximum number of leaves in each tree, defaults to <em>31</em>. Can only work with <em>Lossguide</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>l2_leaf_reg</code>: the <span class="math notranslate nohighlight">\(L_2\)</span> regularization term, defaults to <em>3</em>.</p></li>
</ul>
<p><em>Histogram building parameters</em>.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>border_count</code>: the maximum number of cut points when building histograms, defaults to <em>254</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>feature_border_type</code>: the quantization mode for numerical features, defaults to <em>GreedyLogSum</em>. Other options are <em>Median</em>, <em>Uniform</em>, <em>UniformAndQuantiles</em>, <em>MaxLogSum</em> and <em>MinEntropy</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>per_float_feature_quantization</code>: this parameter is used to specify <em>border_count</em> and <em>feature_border_type</em> for each feature.</p></li>
</ul>
<p><em>Categorical handling parameters</em>.</p>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>cat_features</code> and
<code style='font-size:13px; color:firebrick'>text_features</code>:
the list of indices or names of categorical/text features, default to <em>None</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>one_hot_max_size</code>: the maximum number of categories of a feature to use one-hot encoding, otherwise use CatBoost’s encoder, defaults to <em>None</em> (automatically defined).</p></li>
<li><p><code style='font-size:13px; color:firebrick'>model_size_reg</code>: the <a class="reference external" href="https://catboost.ai/en/docs/references/model-size-reg">regularization</a> term for the size of a model contributed by categorical features, defaults to <em>0.5</em>. Larger values reduce model size.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>max_ctr_complexity</code>: the maximum number of features that can be combined, defaults to <em>4</em>. Has to be less than <em>15</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>simple_ctr</code> and
<code style='font-size:13px; color:firebrick'>combinations_ctr</code>:
settings for encoding a single/combination of categorical features. These are both complex parameters with a lot of settings, but they share the same format. Please refer to CatBoost’s <a class="reference external" href="https://catboost.ai/en/docs/references/training-parameters/ctr#simple_ctr">documentation</a> for detailed usage.</p></li>
</ul>
</section>
<section id="id9">
<h2>3.2. Implementation<a class="headerlink" href="#id9" title="Permalink to this headline">#</a></h2>
<p>CatBoost support a lot of customizations and useful features. Please refer to CatBoost’s <a class="reference external" href="https://github.com/catboost/catboost/tree/master/catboost/tutorials">tutorials</a> for more details.</p>
<ul class="simple">
<li><p>It accepts either a <a class="reference external" href="https://catboost.ai/en/docs/references/custom-metric__supported-metrics">built-in loss function</a> or a <a class="reference external" href="https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function">custom loss function</a> with the signature:
<code style='font-size:13px'>loss(yPred, yTrue) -&gt; (grad, hess)</code>.</p></li>
<li><p>It accepts either a <a class="reference external" href="https://catboost.ai/en/docs/references/eval-metric__supported-metrics">built-in evaluation metric</a> or a <a class="reference external" href="https://catboost.ai/en/docs/concepts/python-usages-examples#custom-loss-function-eval-metric">custom metric</a>.</p></li>
<li><p>It provides a lot of <a class="reference external" href="https://catboost.ai/en/docs/concepts/model-analysis">model analysis</a> tools, including
<a class="reference external" href="https://catboost.ai/en/docs/concepts/fstr">feature importance</a> and
<a class="reference external" href="https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier_get_object_importance">object importance</a>.</p></li>
<li><p>It supports <a class="reference external" href="https://catboost.ai/en/docs/features/snapshots">snapshotting</a>, which periodically saves the current state of the model. This allows recovering training after an interuption by launching the training in the same folder with the same parameters.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span> <span class="k">as</span> <span class="nn">dt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span> <span class="k">as</span> <span class="n">AUC</span>
<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span><span class="p">,</span> <span class="n">CatBoostRegressor</span>
<span class="kn">import</span> <span class="nn">catboost</span> <span class="k">as</span> <span class="nn">cgb</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfCancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/breast_cancer.csv&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dfCancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xTest</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yTest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xValid</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yValid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="id10">
<h3>Scikit-learn interface<a class="headerlink" href="#id10" title="Permalink to this headline">#</a></h3>
<p>The algorithm initialization syntax using
<code style='font-size:13px'><a href="https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier">CatBoostClassifier</a></code>
and
<code style='font-size:13px'><a href="https://catboost.ai/en/docs/concepts/python-reference_catboostregressor">CatBoostRegressor</a></code>
classes is the same as in Scikit-learn. Most of CatBoost’s useful features are accessed using the
<code style='font-size:13px'><a href="https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier_fit">fit()</a></code>
method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">boosting_type</span><span class="o">=</span><span class="s1">&#39;Ordered&#39;</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    
    <span class="c1"># eval_metric=&#39;Logloss&#39;, # for evaluation</span>
    <span class="n">custom_metric</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC&#39;</span><span class="p">,</span> <span class="s1">&#39;F1&#39;</span><span class="p">,</span> <span class="s1">&#39;CrossEntropy&#39;</span><span class="p">],</span> <span class="c1"># for displaying only</span>
    <span class="n">train_dir</span><span class="o">=</span><span class="s1">&#39;output/catboost&#39;</span><span class="p">,</span>
    <span class="n">save_snapshot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">snapshot_interval</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">xValid</span><span class="p">,</span> <span class="n">yValid</span><span class="p">)],</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "33d977fbe2b04677ba5090bac026efa7", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Learning rate set to 0.032843
0:	learn: 0.6456139	test: 0.6406496	best: 0.6406496 (0)	total: 64.3ms	remaining: 32.1s
100:	learn: 0.0687001	test: 0.0543116	best: 0.0543116 (100)	total: 296ms	remaining: 1.17s
200:	learn: 0.0361472	test: 0.0392240	best: 0.0392240 (200)	total: 497ms	remaining: 740ms
300:	learn: 0.0173880	test: 0.0336209	best: 0.0335770 (296)	total: 724ms	remaining: 479ms
Stopped by overfitting detector  (50 iterations wait)

bestTest = 0.03301641292
bestIteration = 342

Shrink model to first 343 iterations.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xTest</span><span class="p">)</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yPred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = 0.9524 [CatBoostClassifier]
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-analysis">
<h3>Model analysis<a class="headerlink" href="#model-analysis" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dTrain</span> <span class="o">=</span> <span class="n">cgb</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_feature_importance</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">&#39;PredictionValuesChange&#39;</span><span class="p">,</span> <span class="n">prettified</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature Id</th>
      <th>Importances</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>worst_perimeter</td>
      <td>11.716592</td>
    </tr>
    <tr>
      <th>1</th>
      <td>worst_area</td>
      <td>10.885156</td>
    </tr>
    <tr>
      <th>2</th>
      <td>worst_concave_points</td>
      <td>9.509133</td>
    </tr>
    <tr>
      <th>3</th>
      <td>mean_concave_points</td>
      <td>8.031246</td>
    </tr>
    <tr>
      <th>4</th>
      <td>worst_concavity</td>
      <td>6.975604</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dTrain</span> <span class="o">=</span> <span class="n">cgb</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_feature_importance</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">dTrain</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;LossFunctionChange&#39;</span><span class="p">,</span> <span class="n">prettified</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature Id</th>
      <th>Importances</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>area_error</td>
      <td>0.009007</td>
    </tr>
    <tr>
      <th>1</th>
      <td>worst_area</td>
      <td>0.009000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>worst_concavity</td>
      <td>0.007121</td>
    </tr>
    <tr>
      <th>3</th>
      <td>worst_concave_points</td>
      <td>0.006818</td>
    </tr>
    <tr>
      <th>4</th>
      <td>mean_concave_points</td>
      <td>0.005860</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dTrain</span> <span class="o">=</span> <span class="n">cgb</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_feature_importance</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">dTrain</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;Interaction&#39;</span><span class="p">,</span> <span class="n">prettified</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>First Feature Index</th>
      <th>Second Feature Index</th>
      <th>Interaction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>23</td>
      <td>26</td>
      <td>5.117987</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13</td>
      <td>24</td>
      <td>5.069373</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17</td>
      <td>23</td>
      <td>3.986848</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23</td>
      <td>27</td>
      <td>3.222103</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td>13</td>
      <td>2.652889</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">tree_idx</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/34. [xgboost] Tree Boosting_83_0.svg" src="../_images/34. [xgboost] Tree Boosting_83_0.svg" /></div>
</div>
</section>
<section id="catboost-interface">
<h3>CatBoost interface<a class="headerlink" href="#catboost-interface" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dTrain</span> <span class="o">=</span> <span class="n">cgb</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>
<span class="n">dValid</span> <span class="o">=</span> <span class="n">cgb</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">xValid</span><span class="p">,</span> <span class="n">yValid</span><span class="p">)</span>
<span class="n">dTest</span> <span class="o">=</span> <span class="n">cgb</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">xTest</span><span class="p">,</span> <span class="n">yTest</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;Ordered&#39;</span><span class="p">,</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s1">&#39;early_stopping_rounds&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s1">&#39;allow_writing_files&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">cgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">pool</span><span class="o">=</span><span class="n">dTrain</span><span class="p">,</span> <span class="n">evals</span><span class="o">=</span><span class="n">dValid</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Learning rate set to 0.06564
0:	learn: 0.4636810	test: 0.4538482	best: 0.4538482 (0)	total: 2.75ms	remaining: 1.37s
100:	learn: 0.1374874	test: 0.1252798	best: 0.1252506 (99)	total: 219ms	remaining: 864ms
200:	learn: 0.1025667	test: 0.1185282	best: 0.1185282 (200)	total: 394ms	remaining: 586ms
300:	learn: 0.0868420	test: 0.1136188	best: 0.1136188 (300)	total: 570ms	remaining: 377ms
400:	learn: 0.0736333	test: 0.1091781	best: 0.1090910 (395)	total: 743ms	remaining: 183ms
Stopped by overfitting detector  (50 iterations wait)

bestTest = 0.1090909577
bestIteration = 395

Shrink model to first 396 iterations.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dTest</span><span class="p">)</span> <span class="c1"># predict probability</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestPred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = 0.9901 [CatBoost]
</pre></div>
</div>
</div>
</div>
</section>
<section id="id11">
<h3>Multilabel problem<a class="headerlink" href="#id11" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfEmotionTrain</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/emotions_train.csv&#39;</span><span class="p">)</span>
<span class="n">dfEmotionTest</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/emotions_test.csv&#39;</span><span class="p">)</span>

<span class="n">xTrain</span> <span class="o">=</span> <span class="n">dfEmotionTrain</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span>
<span class="n">yTrain</span> <span class="o">=</span> <span class="n">dfEmotionTrain</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">6</span><span class="p">:]</span>
<span class="n">xTest</span> <span class="o">=</span> <span class="n">dfEmotionTest</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span>
<span class="n">yTest</span> <span class="o">=</span> <span class="n">dfEmotionTest</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">6</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">xTrain</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">yTrain</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(391, 72) (391, 6)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="c1"># hyperparameters</span>
    <span class="n">boosting_type</span><span class="o">=</span><span class="s1">&#39;Ordered&#39;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    
<span class="c1">#     # configurations</span>
    <span class="n">loss_function</span><span class="o">=</span><span class="s1">&#39;MultiLogloss&#39;</span><span class="p">,</span>
    <span class="n">allow_writing_files</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Learning rate set to 0.030182
0:	learn: 0.6783843	total: 442ms	remaining: 1m 28s
50:	learn: 0.3692859	total: 23.4s	remaining: 1m 8s
100:	learn: 0.2811259	total: 49.9s	remaining: 48.9s
150:	learn: 0.2236028	total: 1m 14s	remaining: 24.3s
199:	learn: 0.1837092	total: 1m 38s	remaining: 0us
0:01:38.843101
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestProb</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">xTest</span><span class="p">)</span>
<span class="n">listAuc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestProb</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">listAuc</span> <span class="o">=</span> <span class="n">listAuc</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;AUC = </span><span class="si">{</span><span class="n">listAuc</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC = [0.8626, 0.7567, 0.8027, 0.9346, 0.843, 0.9159] [CatBoostClassifier]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ngboost">
<h1>4. NGBoost<a class="headerlink" href="#ngboost" title="Permalink to this headline">#</a></h1>
<p>NGBoost (Natural Gradient Boosting) is published in 2018 by Duan and Avati in order to add probabilistic <em>prediction interval</em> to point estimation which is typically seen in most regression algorithms. However, unlike XGBoost, LightGBM and CatBoost who integrate so many hyperparameters, optimizations, suppports and customizations; NGBoost’s implementation is quite simple.</p>
<section id="id12">
<h2>4.1. Algorithm<a class="headerlink" href="#id12" title="Permalink to this headline">#</a></h2>
<p><em>Input</em>: mostly the same as other tree boosting algorithms, with some differences:</p>
<ul class="simple">
<li><p>A <em>parametric distribution</em> <span class="math notranslate nohighlight">\(\mathcal{P}\)</span> (for example, normal distribution with two parameters mean and standard deviation) that you think our data follows.</p></li>
<li><p>A <em>scoring function</em> <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> which evaluates the closeness between <span class="math notranslate nohighlight">\(\mathcal{P}\)</span> and the observed label (think of it as the alternative for loss function in normal gradient boosting). NGBoost currently supports two options for <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, <em>log likelihood</em> and <em>continuous ranked probability</em>.</p></li>
</ul>
<p><em>Algorithm</em>: Being a boosting algorithm, NGBoost uses the same behaviour as normal gradient boosting. But instead of updating directly the predicted values of weak learners, NGBoost fit a sequence of trees <em>for each parameter</em> of the distribution <span class="math notranslate nohighlight">\(\mathcal{P}\)</span> so that the scoring function is minimized at each step. However, according to NGBoost’s paper, the <em>distance</em> between two parameter values does not correspond to an appropriate <em>distance</em> between the distributions that those parameters identify. This motivates the <em>natural gradient</em>, thus the name of the algorithm.</p>
</section>
<section id="id13">
<h2>4.2. Implementation<a class="headerlink" href="#id13" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><code style='font-size:13px; color:firebrick'>Dist</code>: the distribution to be used, defaults to <em>normal distribution</em> (for regression) and <em>categorical distribution</em> (for classification). Being a class provided by NGBoost.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>Score</code>: the scoring rule to be used, defaults to <em>log score</em>. Being a class provided by NGBoost.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>Base</code>: the base estimator, being</p></li>
<li><p><code style='font-size:13px; color:firebrick'>natural_gradient</code>: whether to use natural gradient or not, defaults to <em>True</em>.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>n_estimators</code>: the number of boosting stages, defaults to <em>500</em>. Larger is usually better, but should go with a lower <em>learning_rate</em> and an <em>early_stopping_rounds</em> provided. Lower can speed up training.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>learning_rate</code>: the learning rate, defaults to 0.01. Same usage as in GBM.</p></li>
<li><p><code style='font-size:13px; color:firebrick'>minibatch_frac</code>
and <code style='font-size:13px; color:firebrick'>col_sample</code>:
the ratio of instances/features used in each tree, both defaults to <em>1</em>. A lower value will increase the randomness between trees that deals with overfitting and may speed up training.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span> <span class="k">as</span> <span class="nn">dt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="p">;</span> <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;seaborn&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn-whitegrid&#39;</span><span class="p">])</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span><span class="p">,</span> <span class="n">mean_absolute_error</span> <span class="k">as</span> <span class="n">MAE</span><span class="p">,</span> <span class="n">r2_score</span> <span class="k">as</span> <span class="n">R2</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">ngboost</span> <span class="kn">import</span> <span class="n">NGBRegressor</span><span class="p">,</span> <span class="n">NGBClassifier</span>
<span class="kn">from</span> <span class="nn">ngboost.distns</span> <span class="kn">import</span> <span class="n">Poisson</span><span class="p">,</span> <span class="n">Bernoulli</span><span class="p">,</span> <span class="n">k_categorical</span><span class="p">,</span> <span class="n">ClassificationDistn</span>
<span class="kn">from</span> <span class="nn">ngboost.distns</span> <span class="kn">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">Laplace</span><span class="p">,</span> <span class="n">Cauchy</span><span class="p">,</span> <span class="n">Exponential</span><span class="p">,</span> <span class="n">RegressionDistn</span>
<span class="kn">from</span> <span class="nn">ngboost.scores</span> <span class="kn">import</span> <span class="n">LogScore</span><span class="p">,</span> <span class="n">CRPScore</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfBoston</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/boston.csv&#39;</span><span class="p">)</span>
<span class="n">dfBoston</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>crime_rate</th>
      <th>land_rate</th>
      <th>indus</th>
      <th>chas</th>
      <th>nox</th>
      <th>room</th>
      <th>age</th>
      <th>distance</th>
      <th>radial</th>
      <th>tax</th>
      <th>ptratio</th>
      <th>black</th>
      <th>lstat</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">dfBoston</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">xTrain</span><span class="p">,</span> <span class="n">xTest</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">yTest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="n">nIter</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">NGBRegressor</span><span class="p">(</span>
    <span class="n">Dist</span><span class="o">=</span><span class="n">Normal</span><span class="p">,</span>
    <span class="n">Score</span><span class="o">=</span><span class="n">LogScore</span><span class="p">,</span>
    <span class="n">Base</span><span class="o">=</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;friedman_mse&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="n">nIter</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">minibatch_frac</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">col_sample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    
    <span class="n">verbose_eval</span><span class="o">=</span><span class="n">nIter</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span>
    <span class="n">xTest</span><span class="p">,</span> <span class="n">yTest</span><span class="p">,</span>
    <span class="c1"># early_stopping_rounds=int(nIter/5)</span>
<span class="p">)</span>

<span class="n">end</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iter 0] loss=3.6106 val_loss=3.6037 scale=1.0000 norm=6.7694
[iter 200] loss=2.2211 val_loss=2.4719 scale=2.0000 norm=3.5080
[iter 400] loss=1.9163 val_loss=2.6082 scale=1.0000 norm=1.5952
[iter 600] loss=1.7287 val_loss=2.9839 scale=1.0000 norm=1.4241
[iter 800] loss=1.6338 val_loss=3.4870 scale=1.0000 norm=1.3173
0:00:07.316115
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yTestPred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xTest</span><span class="p">)</span>
<span class="n">r2Test</span> <span class="o">=</span> <span class="n">R2</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yTestPred</span><span class="p">)</span>
<span class="n">modelName</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;R2 = </span><span class="si">{</span><span class="n">r2Test</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> [</span><span class="si">{</span><span class="n">modelName</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R2 = 0.8455 [NGBRegressor]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.0927, 0.0091, 0.0445, 0.0088, 0.0669, 0.1906, 0.0847, 0.1422,
        0.0255, 0.0529, 0.0441, 0.0823, 0.1556],
       [0.0971, 0.0244, 0.0541, 0.0098, 0.0732, 0.1563, 0.1043, 0.1235,
        0.024 , 0.0595, 0.0522, 0.0822, 0.1393]])
</pre></div>
</div>
</div>
</div>
<p><b style='color:navy'><i class="fa fa-book"></i>  Case study</b><br>
For each house in the test set, use the distribution parameters (mean and standard deviation) returned by NGBoost to find:</p>
<ul class="simple">
<li><p>The probability that house price lies within the range <span class="math notranslate nohighlight">\((20,25)\)</span></p></li>
<li><p>The interval of price with <span class="math notranslate nohighlight">\(95\%\)</span> confidence</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="n">muTest</span><span class="p">,</span> <span class="n">sigmaTest</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pred_dist</span><span class="p">(</span><span class="n">xTest</span><span class="p">)</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">muTest</span><span class="p">,</span> <span class="n">sigmaTest</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;True&#39;</span><span class="p">:</span> <span class="n">yTest</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
    <span class="s1">&#39;MeanPredict&#39;</span><span class="p">:</span> <span class="n">muTest</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="s1">&#39;Proba(20,25)&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span> <span class="o">-</span> <span class="n">dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span>
    <span class="s1">&#39;Lower(95%)&#39;</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="s1">&#39;Upper(95%)&#39;</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">isf</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>True</th>
      <th>MeanPredict</th>
      <th>Proba(20,25)</th>
      <th>Lower(95%)</th>
      <th>Upper(95%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>21.7</td>
      <td>23.31</td>
      <td>0.962086</td>
      <td>21.45</td>
      <td>25.17</td>
    </tr>
    <tr>
      <th>1</th>
      <td>18.5</td>
      <td>19.26</td>
      <td>0.192169</td>
      <td>17.60</td>
      <td>20.93</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22.2</td>
      <td>20.80</td>
      <td>0.775838</td>
      <td>18.74</td>
      <td>22.85</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20.4</td>
      <td>19.91</td>
      <td>0.467780</td>
      <td>17.77</td>
      <td>22.05</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8.8</td>
      <td>5.13</td>
      <td>0.000000</td>
      <td>2.93</td>
      <td>7.33</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>97</th>
      <td>16.3</td>
      <td>11.41</td>
      <td>0.000097</td>
      <td>6.90</td>
      <td>15.93</td>
    </tr>
    <tr>
      <th>98</th>
      <td>17.2</td>
      <td>14.49</td>
      <td>0.000018</td>
      <td>11.87</td>
      <td>17.10</td>
    </tr>
    <tr>
      <th>99</th>
      <td>28.0</td>
      <td>27.11</td>
      <td>0.003637</td>
      <td>25.57</td>
      <td>28.65</td>
    </tr>
    <tr>
      <th>100</th>
      <td>15.2</td>
      <td>17.79</td>
      <td>0.016172</td>
      <td>15.77</td>
      <td>19.81</td>
    </tr>
    <tr>
      <th>101</th>
      <td>16.6</td>
      <td>16.90</td>
      <td>0.000591</td>
      <td>15.03</td>
      <td>18.77</td>
    </tr>
  </tbody>
</table>
<p>102 rows × 5 columns</p>
</div></div></div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p><em><a class="reference external" href="http://xgboost.readthedocs.io">xgboost.readthedocs.io</a> - <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/index.html">XGBoost documentation</a></em></p></li>
<li><p><em><a class="reference external" href="http://lightgbm.readthedocs.io">lightgbm.readthedocs.io</a> - <a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/">LightGBM documentation</a></em></p></li>
<li><p><em><a class="reference external" href="http://stanfordmlgroup.github.io">stanfordmlgroup.github.io</a> - <a class="reference external" href="https://stanfordmlgroup.github.io/ngboost/intro.html">NGBoost documentation</a></em></p></li>
<li><p><em><a class="reference external" href="http://catboost.ai">catboost.ai</a> - <a class="reference external" href="https://catboost.ai/en/docs/">CatBoost documentation</a></em></p></li>
<li><p><em><a class="reference external" href="http://cs.columbia.edu">cs.columbia.edu</a> - <a class="reference external" href="http://www.cs.columbia.edu/~jebara/6772/papers/IntroToBoosting.pdf">A short introduction to Boosting</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A scalable tree boosting system</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1706.09516.pdf">CatBoost: Unbiased Boosting with categorical features</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1910.03225.pdf">NGBoost: Natural Gradient Boosting for probabilistic prediction</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1505.01866.pdf">DART: Dropouts meet Multiple Additive Regression Trees</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1802.05640.pdf">Gradient Boosting with Piece-Wise Linear Regression Trees</a></em></p></li>
<li><p><em><a class="reference external" href="http://papers.nips.cc">papers.nips.cc</a> - <a class="reference external" href="https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM: A highly efficient Gradient Boosting Decision Tree</a></em></p></li>
<li><p><em><a class="reference external" href="http://papers.nips.cc">papers.nips.cc</a> - <a class="reference external" href="https://papers.nips.cc/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf">Cost efficient gradient boosting</a></em></p></li>
<li><p><em><a class="reference external" href="http://deep-and-shallow.com">deep-and-shallow.com</a> - <a class="reference external" href="https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/">The Gradient Booster IV(B): NGBoost</a></em></p></li>
<li><p><em><a class="reference external" href="http://pub.towardsai.net">pub.towardsai.net</a> - <a class="reference external" href="https://pub.towardsai.net/state-of-the-art-models-in-every-machine-learning-field-2021-c7cf074da8b2">State of the Art Models in Every Machine Learning Field 2021</a></em></p></li>
</ul>
<hr class="docutils" />
<p><em>♥ By Quang Hung x Thuy Linh ♥</em></p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "hungpq7/tabular-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By &#9829; Quang Hung x Thuy Linh &#9829;<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>