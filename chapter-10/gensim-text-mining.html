
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Gensim: Text Mining</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="OpenCV: Image Processing" href="opencv-image-processing.html" />
    <link rel="prev" title="10. Deep Learning" href="_intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../util/intro.html">
                    Data Science
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-01/_intro.html">
   <b>
    1. Python Programming
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-01/python-basic-concepts.html">
     Python: Basic Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-01/python-data-types.html">
     Python: Data Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-01/python-data-containers.html">
     Python: Data Containers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-01/python-functions-objects.html">
     Python: Functions and Objects
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-01/python-algorithms.html">
     (w) Python: Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-01/python-external-sources.html">
     Python: External Sources
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-01/selenium-web-scraping.html">
     Selenium: Web Scraping
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-02/_intro.html">
   <b>
    2. Mathematics
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-02/numpy-linear-algebra.html">
     NumPy: Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-02/sympy-calculus.html">
     SymPy: Calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-02/numpy-probability.html">
     NumPy: Probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-02/numpy-statistics.html">
     NumPy: Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-02/scipy-hypothesis-testing.html">
     SciPy: Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-02/numpy-applied-mathematics.html">
     (w) NumPy: Applied Mathematics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-02/networkx-network-analysis.html">
     (w) NetworkX: Network Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-03/_intro.html">
   <b>
    3. Data Manipulation
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-03/numpy-arrays.html">
     NumPy: Arrays
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-03/pandas-data-exploratory.html">
     Pandas: Data Exploratory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-03/pandas-data-cleaning.html">
     Pandas: Data Cleaning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-03/pandas-data-transformation.html">
     Pandas: Data Transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-03/janitor-pandas-extensions.html">
     Janitor: Pandas Extensions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-04/_intro.html">
   <b>
    4. Big Data
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-04/hiveql-data-manipulation.html">
     HiveQL: Data Manipulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-04/pyspark-data-exploratory.html">
     PySpark: Data Exploratory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-04/pyspark-data-cleaning.html">
     PySpark: Data Cleaning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-04/pyspark-data-transformation.html">
     PySpark: Data Transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-04/dask-parallelized-pandas.html">
     Dask: Parallelized Pandas
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-05/_intro.html">
   <b>
    5. Data Visualization
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-05/matplotlib-graph-construction.html">
     Matplotlib: Graph Construction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-05/seaborn-statistical-visualization.html">
     Seaborn: Statistical Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-05/plotly-interactive-visualization.html">
     Plotly: Interactive Visualization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-06/_intro.html">
   <b>
    6. Machine Learning
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-06/sklearn-machine-learning.html">
     Sklearn: Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-06/sklearn-classification.html">
     Sklearn: Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-06/sklearn-regression.html">
     Sklearn: Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-06/sklearn-feature-engineering.html">
     Sklearn: Feature Engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-06/lenskit-recommendation.html">
     Lenskit: Recommendation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-06/pyspark-machine-learning.html">
     PySpark: Machine Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-07/_intro.html">
   <b>
    7. Tabular Learning
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-07/sklearn-ensemble-learning.html">
     Sklearn: Ensemble Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-07/xgboost-tree-boosting.html">
     XGBoost: Tree Boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-07/ray-hyperparameter-optimization.html">
     Ray: Hyperparam Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-07/shap-model-interpretation.html">
     (w) Shap: Model Interpretation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-07/imblearn-targeted-modeling.html">
     (w) Imblearn: Targeted Modeling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-08/_intro.html">
   <b>
    8. Time Series
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-08/statsmodels-temporal-analysis.html">
     Statsmodels: Temporal Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-08/prophet-forecasting-algorithms.html">
     Prophet: Forecasting Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-08/sktime-forecasting-pipeline.html">
     Sktime: Forecasting Pipeline
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-08/darts-deep-forecasting.html">
     (w) Darts: Deep Forecasting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-09/_intro.html">
   <b>
    9. Unsupervised Learning
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-09/mlxtend-association-rules.html">
     (w) Mlxtend: Association Rules
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-09/sklearn-clustering.html">
     Sklearn: Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-09/sklearn-dimensional-reduction.html">
     Sklearn: Dimensional Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-09/pyod-anomaly-detection.html">
     PyOD: Anomaly Detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="_intro.html">
   <b>
    10. Deep Learning
   </b>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Gensim: Text Mining
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="opencv-image-processing.html">
     (w) Opencv: Image Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="numpy-gradient-descent.html">
     Python: Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="keras-multilayer-perceptron.html">
     Keras: Multilayer Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="keras-recurrent-networks.html">
     Keras: Recurrent Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="keras-convolutional-networks.html">
     (w) Keras: Convolutional Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pytorch-deep-learning.html">
     PyTorch: Deep Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-11/_intro.html">
   <b>
    11. R Programming
   </b>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-11/r-basic-concepts.html">
     R: Basic Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-11/r-data-structures.html">
     R: Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-11/tidyverse-data-wrangling.html">
     Tidyverse: Data Wrangling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-11/dplyr-data-cleaning.html">
     Dplyr: Data Cleaning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-11/ggplot-data-visualization.html">
     Ggplot: Data Visualizatio
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-11/r-statistics.html">
     R: Statistics
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/hungpq7/tabular-book/master?urlpath=tree/chapter-10/gensim-text-mining.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/hungpq7/tabular-book/blob/master/chapter-10/gensim-text-mining.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/hungpq7/tabular-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/chapter-10/gensim-text-mining.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lexical-analysis">
   1. Lexical analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-cleaning">
     1.1. Text cleaning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     1.2. Tokenization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sentence-tokenization">
       Sentence tokenization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#word-tokenization">
       Word tokenization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-normalization">
     1.3. Word normalization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#stemming">
       Stemming
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lemmatization">
       Lemmatization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stop-words-removal">
     1.4. Stop words removal
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-representation">
   2. Text representation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-vectorization">
     2.1. Text vectorization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#n-gram">
       N-gram
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bag-of-words">
       Bag Of Words
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tf-idf">
       TF-IDF
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementation-sklearn">
       Implementation: Sklearn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementation-keras">
       Implementation: Keras
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embedding">
     2.2. Word embedding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#word2vec">
       Word2Vec
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fasttext">
       FastText
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#glove">
       GloVe
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pre-trained-embeddings">
       Pre-trained embeddings
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#custom-training">
       Custom training
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#contextual-embedding">
     2.3. Contextual embedding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#w-bert">
       (w) BERT
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sequence-labeling">
   3. Sequence labeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#syntactic-analysis">
     3.1. Syntactic analysis
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#part-of-speech">
       Part of speech
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dependency">
       Dependency
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#named-entities">
       Named entities
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-hidden-markov-model">
     (w) Hidden Markov Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-conditional-random-field">
     (w) Conditional Random Field
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nlp-for-vietnamese">
   4. NLP for Vietnamese
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#character-normalization">
     4.1. Character normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underthesea">
     4.2. Underthesea
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#install">
   Install
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Gensim: Text Mining</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lexical-analysis">
   1. Lexical analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-cleaning">
     1.1. Text cleaning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     1.2. Tokenization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sentence-tokenization">
       Sentence tokenization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#word-tokenization">
       Word tokenization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-normalization">
     1.3. Word normalization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#stemming">
       Stemming
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lemmatization">
       Lemmatization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stop-words-removal">
     1.4. Stop words removal
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-representation">
   2. Text representation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-vectorization">
     2.1. Text vectorization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#n-gram">
       N-gram
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bag-of-words">
       Bag Of Words
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tf-idf">
       TF-IDF
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementation-sklearn">
       Implementation: Sklearn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementation-keras">
       Implementation: Keras
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embedding">
     2.2. Word embedding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#word2vec">
       Word2Vec
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fasttext">
       FastText
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#glove">
       GloVe
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pre-trained-embeddings">
       Pre-trained embeddings
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#custom-training">
       Custom training
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#contextual-embedding">
     2.3. Contextual embedding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#w-bert">
       (w) BERT
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sequence-labeling">
   3. Sequence labeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#syntactic-analysis">
     3.1. Syntactic analysis
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#part-of-speech">
       Part of speech
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dependency">
       Dependency
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#named-entities">
       Named entities
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-hidden-markov-model">
     (w) Hidden Markov Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#w-conditional-random-field">
     (w) Conditional Random Field
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nlp-for-vietnamese">
   4. NLP for Vietnamese
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#character-normalization">
     4.1. Character normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underthesea">
     4.2. Underthesea
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#install">
   Install
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="gensim-text-mining">
<h1>Gensim: Text Mining<a class="headerlink" href="#gensim-text-mining" title="Permalink to this headline">#</a></h1>
<p>So far, we have been familiar in dealing with <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-sectional_data">cross-sectional</a> data and <a class="reference external" href="https://en.wikipedia.org/wiki/Time_series">time series</a> data, which are all structured data. However, most real-world data are <a class="reference external" href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured</a>, such as text. So we would want to <a class="reference external" href="https://en.wikipedia.org/wiki/Text_mining">mine text data</a> to identify meaningful patterns and insights. This goal is achieved in <a class="reference external" href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> (NLP), a sub-field of <a class="reference external" href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a> aiming to help computers understand human language.</p>
<p>NLP is applied in a huge number of problems, some high-level applications appear in daily life are: autocomplete (like in Google Search), machine translation (like Google Translate), grammar error detection and correction (like in Microsoft Word) and virtual assistant (like Apple’s Siri and Amazon’s Alexa). In terms of data mining, common NLP tasks are: <a class="reference external" href="https://en.wikipedia.org/wiki/Keyword_extraction">keyword extraction</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Named-entity_recognition">named-entity recognition</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Document_classification">document classification</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Email_filtering">email filtering</a>.</p>
<p>In order to <em>teach</em> machines to <em>understand</em> human language, there are usually two tasks need to be done: (1) text vectorization, including cleaning text data and (2) modeling. This topic covers the first task, which the ultimate goal is to represent text numerically. The featuring libraries in this topic are <a class="reference external" href="https://github.com/nltk/nltk">NLTK</a> (Natural Language Toolkit), <a class="reference external" href="https://github.com/explosion/spaCy">Spacy</a> and <a class="reference external" href="https://github.com/RaRe-Technologies/gensim">Gensim</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">from</span> <span class="nn">sspipe</span> <span class="kn">import</span> <span class="n">p</span><span class="p">,</span> <span class="n">px</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>

<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>
<span class="kn">from</span> <span class="nn">gensim.parsing.preprocessing</span> <span class="kn">import</span> <span class="n">remove_stopwords</span><span class="p">,</span> <span class="n">STOPWORDS</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span><span class="p">,</span> <span class="n">FastText</span><span class="p">,</span> <span class="n">KeyedVectors</span>

<span class="kn">import</span> <span class="nn">underthesea</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">as</span> <span class="nn">layers</span>
<span class="kn">import</span> <span class="nn">tensorflow_hub</span> <span class="k">as</span> <span class="nn">hub</span>
<span class="kn">import</span> <span class="nn">tensorflow_text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="lexical-analysis">
<h2>1. Lexical analysis<a class="headerlink" href="#lexical-analysis" title="Permalink to this headline">#</a></h2>
<p>Lexical analysis is the process of converting a sequence of characters into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer or tokenizer. A lexer is the first step of NLP project that its output is the input of parsing process - helping the parsing more easier.</p>
<section id="text-cleaning">
<h3>1.1. Text cleaning<a class="headerlink" href="#text-cleaning" title="Permalink to this headline">#</a></h3>
<p>Typical text cleaning tasks should be done (if needed) before any more advance techniques. Text cleaning, if being done right, can reduce the dimensionality and improve language understanding. In this section I will make up a document and implement the following cleaning steps:</p>
<ul class="simple">
<li><p>Lowercasing</p></li>
<li><p>Spell correction</p></li>
<li><p>Removal of emojis and emoticons</p></li>
<li><p>Removal of HTML tags</p></li>
<li><p>Removal of URLs</p></li>
<li><p>Removal of punctuations</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Once a Spark session is created, its running jobs can be monitored at http://localhost:4040/jobs/.</span>
<span class="s1">The default `learning_rate` in XGBoost is 0.3.</span>
<span class="s1">By default, a notebook server runs locally at 127.0.0.1 and can be accessed from the browser using 127.0.0.1:8888.</span>
<span class="s1">Read the &lt;b&gt;paper&lt;/b&gt; of Word2Vec at https://arxiv.org/pdf/1301.3781.pdf and GloVe https://nlp.stanford.edu/pubs/glove.pdf.&#39;&#39;&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Once a Spark session is created, its running jobs can be monitored at http://localhost:4040/jobs/.
The default `learning_rate` in XGBoost is 0.3.
By default, a notebook server runs locally at 127.0.0.1 and can be accessed from the browser using 127.0.0.1:8888.
Read the &lt;b&gt;paper&lt;/b&gt; of Word2Vec at https://arxiv.org/pdf/1301.3781.pdf and GloVe https://nlp.stanford.edu/pubs/glove.pdf.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">patternURL</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;http[\S]+[^.\s]&#39;</span>
<span class="n">patternIP</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;\d+(?:\.\d+)</span><span class="si">{3}</span><span class="s1">(?::\d+)?&#39;</span>
<span class="n">patternHTML</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;&lt;.*?&gt;&#39;</span>
<span class="n">patternPunc</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span>
<span class="n">patternNum</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;\d&#39;</span>
<span class="n">patternEnChar</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;[^a-z\s+]+&#39;</span>
<span class="n">patternViChar</span> <span class="o">=</span><span class="s1">&#39;[^aàảãáạăằẳẵắặâầẩẫấậbcdđeèẻẽéẹêềểễếệfghiìỉĩíịjklmnoòỏõóọôồổỗốộơờởỡớợpqrstuùủũúụưừửữứựvwxyỳỷỹýỵz\s]&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">patternURL</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;http://localhost:4040/jobs/&#39;,
 &#39;https://arxiv.org/pdf/1301.3781.pdf&#39;,
 &#39;https://nlp.stanford.edu/pubs/glove.pdf&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">patternIP</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;127.0.0.1&#39;, &#39;127.0.0.1:8888&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">patternHTML</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&quot;`&quot;, &#39;`&#39;, &#39;&lt;b&gt;&#39;, &#39;&lt;/b&gt;&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="tokenization">
<h3>1.2. Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">#</a></h3>
<p>Tokenization is the process of separating a piece of text into smaller units called tokens, where tokens can be characters, words or sentences. For example, consider the sentence “Never give up”. The most common way of tokenizing is using white spaces. Assuming space as a delimiter, the tokenization of the sentence results in 3 tokens: “Never”, “give” and “up”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="sentence-tokenization">
<h4>Sentence tokenization<a class="headerlink" href="#sentence-tokenization" title="Permalink to this headline">#</a></h4>
<p>NLTK’s sentence tokenizer uses <a class="reference external" href="https://www.nltk.org/api/nltk.tokenize.punkt.html">Punkt</a>, a pre-trained model for English to devide a text into a list of sentences based on recognizing starting words and sentence boundaries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  </span>
<span class="s1">and sometimes sentences can start with non-capitalized words.</span>
<span class="s1">&#39;&#39;&#39;</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.&#39;,
 &#39;and sometimes sentences can start with non-capitalized words.&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="word-tokenization">
<h4>Word tokenization<a class="headerlink" href="#word-tokenization" title="Permalink to this headline">#</a></h4>
<p>Word tokenization is the most commonly used tokenization approach. It splits a piece of text into individual words based on certain delimiters (whitespaces, puntuations). Depending upon delimiters, different word-level tokens are formed. In NLTK, we can split a text piece into words using many strategies, most noticable:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.nltk.org/api/nltk.tokenize.treebank.html"><code class="docutils literal notranslate"><span class="pre">word_tokenize()</span></code></a> implements the Treebank pre-trained model.</p></li>
<li><p><a class="reference external" href="https://www.nltk.org/api/nltk.tokenize.regexp.html"><code class="docutils literal notranslate"><span class="pre">regexp_tokenize()</span></code></a> whichsplits text using a custom regex pattern. There are other convinent functions too, such as <code class="docutils literal notranslate"><span class="pre">wordpunct_tokenize()</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;NLTK has been called &quot;a wonderful tool for computational linguistics!&quot;&#39;&#39;&#39;</span>

<span class="n">text</span> <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">)</span> <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="nb">print</span><span class="p">)</span>
<span class="n">text</span> <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">wordpunct_tokenize</span><span class="p">)</span> <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="nb">print</span><span class="p">)</span>
<span class="n">text</span> <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">regexp_tokenize</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;\w+&#39;</span><span class="p">)</span> <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="nb">print</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;NLTK&#39;, &#39;has&#39;, &#39;been&#39;, &#39;called&#39;, &#39;``&#39;, &#39;a&#39;, &#39;wonderful&#39;, &#39;tool&#39;, &#39;for&#39;, &#39;computational&#39;, &#39;linguistics&#39;, &#39;!&#39;, &quot;&#39;&#39;&quot;]
[&#39;NLTK&#39;, &#39;has&#39;, &#39;been&#39;, &#39;called&#39;, &#39;&quot;&#39;, &#39;a&#39;, &#39;wonderful&#39;, &#39;tool&#39;, &#39;for&#39;, &#39;computational&#39;, &#39;linguistics&#39;, &#39;!&quot;&#39;]
[&#39;NLTK&#39;, &#39;has&#39;, &#39;been&#39;, &#39;called&#39;, &#39;a&#39;, &#39;wonderful&#39;, &#39;tool&#39;, &#39;for&#39;, &#39;computational&#39;, &#39;linguistics&#39;]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="word-normalization">
<h3>1.3. Word normalization<a class="headerlink" href="#word-normalization" title="Permalink to this headline">#</a></h3>
<p>English documents use different forms of a word, such as the <a class="reference external" href="https://en.wikipedia.org/wiki/Inflection">inflected word</a> “historical” is derived from the <a class="reference external" href="https://en.wikipedia.org/wiki/Root_(linguistics)">root word</a> “history”. Words in each family are usually related and have similar meanings, so in many situations, it is useful to use a word as a replacement for other words in the family. This technique is called <em>word normalization</em>; it has two approaches, <em>stemming</em> and <em>lemmatization</em>.</p>
<section id="stemming">
<h4>Stemming<a class="headerlink" href="#stemming" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Stemming">Stemming</a> is a <em>logical</em> normalization technique that removes common prefixes and suffixes from an inflected word. The result is called a <a class="reference external" href="https://en.wikipedia.org/wiki/Word_stem">stem</a>; a stem can be meaningless. For example, the word “studying” has its stem “studi” because the stemmer has a step that removes the “-ing” suffix and another step that substitues “-y” with “-i”. NLTK supports two stemming algorithms, <a class="reference external" href="https://www.nltk.org/api/nltk.stem.porter.html">Porter stemmer</a> (1980) and <a class="reference external" href="https://www.nltk.org/api/nltk.stem.lancaster.html">Lancaster stemmer</a> (1990).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">listWord</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;runner&#39;</span><span class="p">,</span> <span class="s1">&#39;running&#39;</span><span class="p">,</span> <span class="s1">&#39;easy&#39;</span><span class="p">,</span> <span class="s1">&#39;easily&#39;</span><span class="p">,</span> <span class="s1">&#39;studied&#39;</span><span class="p">,</span> <span class="s1">&#39;studying&#39;</span><span class="p">]</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">listWord</span><span class="p">:</span>
    <span class="n">stem</span> <span class="o">=</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">word</span><span class="si">:</span><span class="s1">&lt;9</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">stem</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>runner    -&gt; runner
running   -&gt; run
easy      -&gt; easi
easily    -&gt; easili
studied   -&gt; studi
studying  -&gt; studi
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">listWord</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;runner&#39;</span><span class="p">,</span> <span class="s1">&#39;running&#39;</span><span class="p">,</span> <span class="s1">&#39;easy&#39;</span><span class="p">,</span> <span class="s1">&#39;easily&#39;</span><span class="p">,</span> <span class="s1">&#39;studied&#39;</span><span class="p">,</span> <span class="s1">&#39;studying&#39;</span><span class="p">]</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">LancasterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">listWord</span><span class="p">:</span>
    <span class="n">stem</span> <span class="o">=</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">word</span><span class="si">:</span><span class="s1">&lt;9</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">stem</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>runner    -&gt; run
running   -&gt; run
easy      -&gt; easy
easily    -&gt; easy
studied   -&gt; study
studying  -&gt; study
</pre></div>
</div>
</div>
</div>
</section>
<section id="lemmatization">
<h4>Lemmatization<a class="headerlink" href="#lemmatization" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Lemmatisation">Lemmatization</a>, unlike stemming, is a <em>physical</em> technique that looks up the corresponding <a class="reference external" href="https://en.wikipedia.org/wiki/Lemma_(morphology)">lemma</a> of a word from a prepared database. Due to this behavious, the technique is capable of handling irregular cases such as past tense of some verbs. Lemmatization, however, requires a large database to match the number of cases that stemming can cover, and thus it is more slowly. In NLTK, <a class="reference external" href="https://www.nltk.org/api/nltk.stem.wordnet.html">Wordnet lemmatizer</a> works the best when being provided the appropriate <em>part-of-speech</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">listWord</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;feet&#39;</span><span class="p">,</span> <span class="s1">&#39;minima&#39;</span><span class="p">,</span> <span class="s1">&#39;dogs&#39;</span><span class="p">,</span> <span class="s1">&#39;leaves&#39;</span><span class="p">,</span> <span class="s1">&#39;axes&#39;</span><span class="p">,</span> <span class="s1">&#39;mice&#39;</span><span class="p">]</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">listWord</span><span class="p">:</span>
    <span class="n">lemma</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">word</span><span class="si">:</span><span class="s1">&lt;9</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">lemma</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feet      -&gt; foot
minima    -&gt; minimum
dogs      -&gt; dog
leaves    -&gt; leaf
axes      -&gt; ax
mice      -&gt; mouse
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">listWord</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ran&#39;</span><span class="p">,</span> <span class="s1">&#39;read&#39;</span><span class="p">,</span> <span class="s1">&#39;ate&#39;</span><span class="p">,</span> <span class="s1">&#39;fallen&#39;</span><span class="p">,</span> <span class="s1">&#39;sung&#39;</span><span class="p">,</span> <span class="s1">&#39;bought&#39;</span><span class="p">]</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">listWord</span><span class="p">:</span>
    <span class="n">lemma</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">word</span><span class="si">:</span><span class="s1">&lt;9</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">lemma</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ran       -&gt; run
read      -&gt; read
ate       -&gt; eat
fallen    -&gt; fall
sung      -&gt; sing
bought    -&gt; buy
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">listWord</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;higher&#39;</span><span class="p">,</span> <span class="s1">&#39;strongest&#39;</span><span class="p">,</span> <span class="s1">&#39;better&#39;</span><span class="p">]</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">listWord</span><span class="p">:</span>
    <span class="n">lemma</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">word</span><span class="si">:</span><span class="s1">&lt;9</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">lemma</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>higher    -&gt; high
strongest -&gt; strong
better    -&gt; good
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="stop-words-removal">
<h3>1.4. Stop words removal<a class="headerlink" href="#stop-words-removal" title="Permalink to this headline">#</a></h3>
<p>Another interesting preprocessing technique for text data is removing <a class="reference external" href="https://en.wikipedia.org/wiki/Stop_word">stop words</a>. They are common words but have a very little meaning (such as “the”, “a” and “in”), and are usually filtered out by search engines while fetching results from the database. In NLP, specifically for mining tasks such as keyword extraction, sentiment analysis and document classification, it makes sense to remove stop words. But for AI applications like machine translation and question answering, stop words serve an important role and should not be discarded.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stopwordsGensim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">STOPWORDS</span><span class="p">)</span>
<span class="n">stopwordsNltk</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>

<span class="n">display</span><span class="p">(</span><span class="n">stopwordsGensim</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="n">display</span><span class="p">(</span><span class="n">stopwordsNltk</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;we&#39;, &#39;been&#39;, &#39;everywhere&#39;, &#39;does&#39;, &#39;upon&#39;]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;my songs know what you did in the dark&#39;</span>
<span class="n">remove_stopwords</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">stopwordsNltk</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;songs know dark&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="text-representation">
<h2>2. Text representation<a class="headerlink" href="#text-representation" title="Permalink to this headline">#</a></h2>
<p>Machine Learning algorithms require numeric input and do not process the string or raw text. The task of converting text data into numerical data is generally called text vectorization. Before moving to the technical details, we define some basic terminologies:</p>
<ul class="simple">
<li><p>a <em>token</em> or a <em>term</em> is usually a word</p></li>
<li><p>a <em>document</em> is a collection of tokens, equivalent to an observation</p></li>
<li><p>a <em>corpus</em> is a collection of documents, equivalent to the dataset</p></li>
</ul>
<section id="text-vectorization">
<h3>2.1. Text vectorization<a class="headerlink" href="#text-vectorization" title="Permalink to this headline">#</a></h3>
<p>In this appoach, the final goal is to transform <em>each word into a scalar</em>. The corpus will then be transformed into a matrix, which resembles tabular data and let us apply tabular learning algorithms.</p>
<section id="n-gram">
<h4>N-gram<a class="headerlink" href="#n-gram" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/N-gram">N-gram</a> is an extended way to define a token, that considers a sequence of <span class="math notranslate nohighlight">\(n\)</span> consecutive words rather than a single word. In English, most of the time we would want to use <span class="math notranslate nohighlight">\(n=1\)</span> (unigram), <span class="math notranslate nohighlight">\(n=2\)</span> (bigram) or <span class="math notranslate nohighlight">\(n=3\)</span> (trigram).</p>
</section>
<section id="bag-of-words">
<h4>Bag Of Words<a class="headerlink" href="#bag-of-words" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag Of Words</a> (BOW) treats each token in the corpus a feature and counts how many times that word occurs in each document. This is a very simple technique with some notable downsides:</p>
<ul class="simple">
<li><p>The number of returned features equals to the vocabulary size and is thus very large. The matrix of token counts is very sparse and is not memory efficient.</p></li>
<li><p>It does not take into account order of words in the documents, which is a very important properties of sequential data.</p></li>
</ul>
</section>
<section id="tf-idf">
<h4>TF-IDF<a class="headerlink" href="#tf-idf" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> is an improved version of BOW, calculated as the product of two components, <span class="math notranslate nohighlight">\(\text{TF}\)</span> (Term Frequency) and <span class="math notranslate nohighlight">\(\text{IDF}\)</span> (Inverse Document Frequency). Let’s say for a token <span class="math notranslate nohighlight">\(t\)</span>, a document <span class="math notranslate nohighlight">\(d\)</span> and the corpus <span class="math notranslate nohighlight">\(D\)</span>, we first define two basic concepts, <em>term frequency</em> and <em>document frequency</em>:</p>
<ul class="simple">
<li><p><em>Term frequency</em>, denoted <span class="math notranslate nohighlight">\(\text{TF}(t,d)\)</span>, implies how frequent the token <span class="math notranslate nohighlight">\(t\)</span> appears in the document <span class="math notranslate nohighlight">\(d\)</span>. There are several definitions for <span class="math notranslate nohighlight">\(\text{TF}\)</span>, including <em>binary</em>, <em>count</em> and <em>ratio</em> (divided by document size).</p></li>
<li><p><em>Document frequency</em>, denoted <span class="math notranslate nohighlight">\(\text{DF}(t,d,D)\)</span>, is the ratio of the document <span class="math notranslate nohighlight">\(d\)</span> that contains token <span class="math notranslate nohighlight">\(t\)</span> in the entire corpus <span class="math notranslate nohighlight">\(D\)</span>. A high value of <span class="math notranslate nohighlight">\(\text{DF}\)</span> tells us that the token <span class="math notranslate nohighlight">\(t\)</span> is popular word in the corpus and is likely to be a stop word.</p></li>
</ul>
<p>In the calculation, we take the inverse of document frequency (so that it penalizes stop words) then log transform it (to avoid exploding values): <span class="math notranslate nohighlight">\(\text{IDF}=\log(1\div\text{DF})\)</span>. The final calculation is <span class="math notranslate nohighlight">\(\text{TF-IDF}=\text{TF}(t,d)\times\text{IDF}(t,d,D) \)</span>.</p>
</section>
<section id="implementation-sklearn">
<h4>Implementation: Sklearn<a class="headerlink" href="#implementation-sklearn" title="Permalink to this headline">#</a></h4>
<p>Scikit-learn implements BoW via the class <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"><code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> and TF-IDF via the class <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"><code class="docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a>. Both classes are very powerful as they already integrate tokenization, stop words removal and other customization for steps like word normalization. The later class is highly recommended in practice because it can be easily transformed to BoW by removing both IDF re-weighting and normalization. It has the following hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prepocessor</span></code>: the function that processes raw text, suites for tasks like lowercasing and HTML tags removal.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>: the custom tokenizer; the easiest way to think about it is NLTK’s tokenization functions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stop_words</span></code>: the list of stop words.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ngram_range</span></code>: the tuple containing lower and upper bounds of <span class="math notranslate nohighlight">\(n\)</span> in N-gram. For example, <em>(2,3)</em> means using bigram and trigram.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_df</span></code> and
<code class="docutils literal notranslate"><span class="pre">min_df</span></code>:
the thresholds for filtering very frequent and very rare words.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_features</span></code>: the maximum vocabulary size, default to <em>None</em>. Features are removed based on their rank of term frequency across the corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">binary</span></code>: whether to use binary TF, defaults to <em>False</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sublinear_tf</span></code>: whether to apply log transformation to TF, defaults to <em>False</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_idf</span></code>: whether to divide the result by IDF, defaults to <em>True</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">smooth_idf</span></code>: whether to add smoothing term to IDF calculation, defaults to <em>True</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">norm</span></code>: whether to normalize the final result, defaults to <em>l2</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;I want to buy a nice bike for my girl. She broke her old bike last year.&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;I had a great time watching that movie last night. We shouuld do the same next week&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;If you buy this now, you will get 3 different products for free in the next 10 days.&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;I am living in a small house in France, and my wish is to learn how to ski and snowboad&quot;</span><span class="p">,</span>
    <span class="s2">&quot;It is time to invest in some tech stock. The stock market is will become very hot in the next few months&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LemmaTokenizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">RegexpTokenizer</span><span class="p">(</span><span class="n">pattern</span><span class="o">=</span><span class="s1">&#39;\w+&#39;</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doc</span><span class="p">):</span>
        <span class="c1"># doc = doc.lower()</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">token</span>
            <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">)</span>
            <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">)</span>
            <span class="o">|</span> <span class="n">p</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="n">doc</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">LemmaTokenizer</span><span class="p">(),</span>
    <span class="n">token_pattern</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_words</span><span class="o">=</span><span class="n">gensim</span><span class="o">.</span><span class="n">parsing</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">STOPWORDS</span><span class="p">,</span>
    <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="n">columns</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>10</th>
      <th>10 day</th>
      <th>3</th>
      <th>3 different</th>
      <th>bike</th>
      <th>bike girl</th>
      <th>bike year</th>
      <th>break</th>
      <th>break old</th>
      <th>buy</th>
      <th>...</th>
      <th>time invest</th>
      <th>time watch</th>
      <th>want</th>
      <th>want buy</th>
      <th>watch</th>
      <th>watch movie</th>
      <th>week</th>
      <th>wish</th>
      <th>wish learn</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.463105</td>
      <td>0.231553</td>
      <td>0.231553</td>
      <td>0.231553</td>
      <td>0.231553</td>
      <td>0.186815</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.231553</td>
      <td>0.231553</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.231553</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.281151</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.281151</td>
      <td>0.281151</td>
      <td>0.281151</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.281151</td>
      <td>0.281151</td>
      <td>0.281151</td>
      <td>0.281151</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.226831</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.258199</td>
      <td>0.258199</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.245065</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 69 columns</p>
</div></div></div>
</div>
</section>
<section id="implementation-keras">
<h4>Implementation: Keras<a class="headerlink" href="#implementation-keras" title="Permalink to this headline">#</a></h4>
<p>While Scikit-learn interface is good for Machine Learning, the TensorFlow’s class <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization"><code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code></a> provides suitable input for Deep Learning. It has the following configurations:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">standardize</span></code>: the strategy to preprocess text, defaults to <em>lower_and_strip_punctuation</em>. Can be a user-defined function using <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/strings"><code class="docutils literal notranslate"><span class="pre">tf.strings</span></code></a> functions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">split</span></code>: the pattern to tokenize text, defaults to <em>whitespace</em>. Can be a user-defined function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code>: the vocabulary maximum size, defaults to <em>None</em> (no limit). If set to a number, low frequency words will be truncated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ngrams</span></code>: the tuple containing values of <span class="math notranslate nohighlight">\(n\)</span> in N-gram, defaults to <em>None</em> (uni-gram). For example, <em>(1,3)</em> means using unigram and trigram.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_mode</span></code>: how the output will be returned, defaults to <em>int</em> (integer indices). Other options are <em>multi_hot</em>, <em>count</em> and <em>tf_tdf</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_sequence_length</span></code>: the length of each document regardless how many tokens it has, defaults to <em>None</em> (not set). If set, documents will be either padded or truncated to satisfy this constraint.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ragged</span></code>: whether the output is a normal tensor or a ragged tensor (where sequences may have different lengths), defaults to <em>False</em>.</p></li>
</ul>
<p>This processor is mainly used for text generation tasks such as neural machine translation, which require integer-encoded tokens. Note that there are special tokens in the vocabulary like <code class="docutils literal notranslate"><span class="pre">[UNK]</span></code> (out-of-vocabulary word) and sometimes we might want to add <code class="docutils literal notranslate"><span class="pre">[START]</span></code> and <code class="docutils literal notranslate"><span class="pre">[END]</span></code>. After initialize the processor, we call the <code class="docutils literal notranslate"><span class="pre">adapt()</span></code> method to fit it to the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;I want to buy a nice bike for my girl. She broke her old bike last year.&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;I had a great time watching that movie last night. We shouuld do the same next week&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;If you buy this now, you will get 3 different products for free in the next 10 days.&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;I am living in a small house in France, and my wish is to learn how to ski and snowboad&quot;</span><span class="p">,</span>
    <span class="s2">&quot;It is time to invest in some tech stock. The stock market is will become very hot in the next few months&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">text_clean</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">regex_replace</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;[^a-z\s+]+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;[START]&#39;</span><span class="p">,</span> <span class="n">doc</span><span class="p">,</span> <span class="s1">&#39;[END]&#39;</span><span class="p">],</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">doc</span>

<span class="n">text_clean</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;[START] i want to buy a nice bike for my girl she broke her old bike last year [END]&#39;&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">TextVectorization</span><span class="p">(</span><span class="n">standardize</span><span class="o">=</span><span class="n">text_clean</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">output_sequence_length</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="n">vectorizer</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(5, 15), dtype=int64, numpy=
array([[ 3,  9, 26,  5, 18, 10, 42, 19, 17, 15, 57, 36, 65, 54, 39],
       [ 3,  9, 55, 10, 56, 13, 25, 29, 43, 16, 41, 24, 35, 62,  6],
       [ 3, 50, 11, 18, 28, 40, 11, 12, 58, 63, 38, 17, 59,  2,  6],
       [ 3,  9, 67, 46,  2, 10, 33, 52,  2, 60, 20, 15, 22,  8,  5],
       [ 3, 48,  8, 13,  5, 49,  2, 31, 30, 14,  6, 14, 45,  8, 12]],
      dtype=int64)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;&#39;, &#39;[UNK]&#39;, &#39;in&#39;, &#39;[START]&#39;, &#39;[END]&#39;, &#39;to&#39;, &#39;the&#39;, &#39;next&#39;, &#39;is&#39;, &#39;i&#39;]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="word-embedding">
<h3>2.2. Word embedding<a class="headerlink" href="#word-embedding" title="Permalink to this headline">#</a></h3>
<p>Text vectorization techniques have proven to be too naive for complicated tasks. This leads to the idea of representing words by vectors, in order to increase the capability of capturing hidden information. There are existing methods to encode words as vectors such as one-hot encoding, but it wastes a lot of memory because of its sparsity. With such an expensive representation, we would want word vectors to be more dense and to have the ability to capture semantics.</p>
<p>To efficiently perform word embedding, there need to be two components: a good learning algorithm and a lot of text to train a pre-trained model. In most popular word embedding methods, who you associated with tell you who your are. In this section, we are going to learn about three word embedding techniques, <a class="reference external" href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a> (by Google in 2013), <a class="reference external" href="https://en.wikipedia.org/wiki/GloVe">GloVe</a> (by Stanford in 2014) and <a class="reference external" href="https://en.wikipedia.org/wiki/FastText">FastText</a> (by Facebook in 2015), and how to implement them using <a class="reference external" href="https://radimrehurek.com/gensim/">Gensim</a>.</p>
<section id="word2vec">
<h4>Word2Vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://code.google.com/archive/p/word2vec/">Word2Vec</a> is a pre-trained probabilistic word embedding model based on simple Neural Networks. It relies on conditional probabilities that predict words using the their surrounding neighbors. For example, in the sentence “The quick brown fox jumps over the lazy dog”, consider the <em>center word</em> “fox” and a <em>context window</em> of size <span class="math notranslate nohighlight">\(2\)</span>, then the <em>context words</em> will be “quick”, “brown”, “jumps” and “over”. They will be one-hot encoded so that we have a vector for the center word and another vector for the context words, with <span class="math notranslate nohighlight">\(1\)</span> in the positions of words they are meant to present and <span class="math notranslate nohighlight">\(0\)</span> where else. If the vocabulary has <span class="math notranslate nohighlight">\(V=10\,000\)</span> words, then both the <em>center vector</em> and the <em>context vector</em> also have the size of <span class="math notranslate nohighlight">\(10\,000\)</span>.</p>
<p>The Word2Vec model can be obtained via two self-supervised algorithms, Skip-Gram and CBOW (Continuous Bag Of Words). The Skip-Gram algorithm models the probability that <em>context words</em> occur given a <em>center word</em>, for example:
<span class="math notranslate nohighlight">\(\text{P}(\text{quick, brown, jumps, over}\mid\text{fox})\)</span>.
CBOW, in contrast, predicts the <em>center word</em> when seeing some <em>context words</em>:
<span class="math notranslate nohighlight">\(\text{P}(\text{fox}\mid\text{quick, brown, jumps, over})\)</span>.
Each pair of a center vector and a context vector will contribute an observation to our training data.</p>
<a class="reference internal image-reference" href="../_images/skip_gram_architecture.png"><img alt="../_images/skip_gram_architecture.png" class="align-center" src="../_images/skip_gram_architecture.png" style="height: 270px;" /></a>
<br>
<p>Both algorithms use the same Neural Network architecture with only a single hidden layer, but switch the input and output layers. We are going to describe only Skip-Gram, as the same logic also applies to CBOW.</p>
<ul class="simple">
<li><p>The input layer is constructed by <em>center vectors</em>. Because they are one-hot vectors, multiplying the input layer with any matrix is like looking up the corresponding information from a table.</p></li>
<li><p>The hidden layer has no bias term as well as activation function, so that the architecture is simple enough to handle a large amount of data. The corresponding weight matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> will be then used as embedding vectors. If we set up the hidden layer with <span class="math notranslate nohighlight">\(N=300\)</span> nodes, then this will be the size of embedding vectors.</p></li>
<li><p>The output layer uses cross entropy loss function and softmax activation function, making what we are doing is predicting probabilities of context words.</p></li>
</ul>
<p>Overall, CBOW is a faster architecture than Skip-Gram. Besides, Word2Vec is usually impelemented with some improvements: negative sampling, subsampling of frequent words and context window shrinking.</p>
</section>
<section id="fasttext">
<h4>FastText<a class="headerlink" href="#fasttext" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://fasttext.cc/docs/en/support.html">FastText</a> use the same training algorithm as Word2Vec, but it treats each center word as a composed of n-gram characters. For example, for <span class="math notranslate nohighlight">\(n=3\)</span>, the word “train” will get you a <span class="math notranslate nohighlight">\(n\)</span>-gram sub-word for each character (“tr”, “tra”, “rai”, “ain”, “in”). In practice, <span class="math notranslate nohighlight">\(n\)</span> is set to be in range <span class="math notranslate nohighlight">\([3,6]\)</span> so that the model can capture all types of prefixes and suffixes. This design gives two advantages over Word2Vec:</p>
<ul class="simple">
<li><p><em>Capturing morphology</em>. For words such as “train” and “trained”, FastText can understand they are similar beacause of both internal structures and contexts. Word2Vec only considers contexts and is likely to be less accurate.</p></li>
<li><p><em>Handling out-of-vocabulary words</em>. The embedding for a word in FastText is computed as the sum of all subword vectors.</p></li>
</ul>
</section>
<section id="glove">
<h4>GloVe<a class="headerlink" href="#glove" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a> (Global Vectors) is an embedding algorithm makes use of the <a class="reference external" href="https://en.wikipedia.org/wiki/Co-occurrence_matrix">co-occurrance matrix</a> <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{V\times V}\)</span> (<span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size). The algorithm takes into account global statistics and thus neutralize the effect of frequent words. To understand how GloVe works, let’s denote:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_{ik}\)</span> the number of times the pair of words <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(k\)</span> occurs</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i=\sum_k x_{ik}\)</span> the number of times any word <span class="math notranslate nohighlight">\(k\)</span> appears in the context of word <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{ik}=P(k|i)=x_{ik}\div x_i\)</span> the probability that the word <span class="math notranslate nohighlight">\(k\)</span> appears in the context of word <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span> the vector representation of word <span class="math notranslate nohighlight">\(i\)</span>, which is what we want to learn</p></li>
</ul>
<p>The underlying idea of GloVe is about the ratio <span class="math notranslate nohighlight">\(P_{ik}:P_{jk}\)</span>, where <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(j\)</span> are indices of context words and <span class="math notranslate nohighlight">\(k\)</span> is the index of out-of-context words. In the example below, we examine the relationship between two words <span class="math notranslate nohighlight">\(i=\text{ice}\)</span> and <span class="math notranslate nohighlight">\(j=\text{steam}\)</span>, by studying the ratio of their co-occurrance probabilities with different <em>probe words</em>, <span class="math notranslate nohighlight">\(k\)</span>. We observe that this ratio is extremely large or small when word <span class="math notranslate nohighlight">\(k\)</span> is related to one of <span class="math notranslate nohighlight">\(i,j\)</span>; and is very close to <span class="math notranslate nohighlight">\(1\)</span> when the probe word is related to both <span class="math notranslate nohighlight">\(i,j\)</span> or neither.</p>
<a class="reference internal image-reference" href="../_images/co_occurance_probabilities.png"><img alt="../_images/co_occurance_probabilities.png" class="align-center" src="../_images/co_occurance_probabilities.png" style="height: 100px;" /></a>
<br>
<p>Based on this insight, GloVe constructs its loss function as the squared error between the dot product of two embedding vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i^\text{T}\mathbf{x}_j \)</span> and the probability of co-occurrance <span class="math notranslate nohighlight">\(\log P(x_{ij})\)</span>.</p>
</section>
<section id="pre-trained-embeddings">
<h4>Pre-trained embeddings<a class="headerlink" href="#pre-trained-embeddings" title="Permalink to this headline">#</a></h4>
<p>Gensim integrates all pre-trained embedding vectors from Word2Vec, GloVe and FastText, physically. When you load a specific package, Gensim will download it and load to a <a class="reference external" href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors"><code class="docutils literal notranslate"><span class="pre">KeyedVectors</span></code></a>. Note that this object is a <em>physical</em> collection of word vectors, not a <em>logical</em> model. You can use this object to extract embedded vectors and compute word similarities in different ways.</p>
<p>All methods relates to <em>similarity</em> of this object, by default, use cosine similarity, which enables an interesting feature, <em>word calculation</em>. For example, if we remove <em>man</em> from <em>king</em> and then add <em>woman</em>, we should get something very close to <em>queen</em>. This is completely achievable using cosine similarity and will be demonstrated later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">info</span><span class="p">()[</span><span class="s1">&#39;models&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;fasttext-wiki-news-subwords-300&#39;,
 &#39;conceptnet-numberbatch-17-06-300&#39;,
 &#39;word2vec-ruscorpora-300&#39;,
 &#39;word2vec-google-news-300&#39;,
 &#39;glove-wiki-gigaword-50&#39;,
 &#39;glove-wiki-gigaword-100&#39;,
 &#39;glove-wiki-gigaword-200&#39;,
 &#39;glove-wiki-gigaword-300&#39;,
 &#39;glove-twitter-25&#39;,
 &#39;glove-twitter-50&#39;,
 &#39;glove-twitter-100&#39;,
 &#39;glove-twitter-200&#39;,
 &#39;__testing_word2vec-matrix-synopsis&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;fasttext-wiki-news-subwords-300&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[==================================================] 100.0% 958.5/958.4MB downloaded
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder</span><span class="p">[</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.9642, -0.6098,  0.6745,  0.3511,  0.4132, -0.2124,  1.3796,
         0.1285,  0.3157,  0.6633,  0.3391, -0.1893, -3.325 , -1.1491,
        -0.4129,  0.2195,  0.8706, -0.5062, -0.1278, -0.067 ,  0.0658,
         0.4393,  0.1758, -0.5606,  0.1353],
       [-1.242 , -0.3598,  0.5728,  0.3668,  0.6002, -0.189 ,  1.2729,
        -0.3692,  0.0891,  0.4034,  0.2513, -0.2555, -3.9209, -1.11  ,
        -0.2131, -0.2385,  0.9532, -0.5275, -0.0008, -0.3577,  0.5558,
         0.7787,  0.4687, -0.778 ,  0.7838]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;queen&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9202422
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;dog&#39;, 0.9590820074081421),
 (&#39;monkey&#39;, 0.920357882976532),
 (&#39;bear&#39;, 0.9143136739730835),
 (&#39;pet&#39;, 0.9108031392097473),
 (&#39;girl&#39;, 0.8880629539489746)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;king&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;man&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;meets&#39;, 0.8841924071311951),
 (&#39;prince&#39;, 0.832163393497467),
 (&#39;queen&#39;, 0.8257461190223694),
 (&#39;’s&#39;, 0.8174097537994385),
 (&#39;crow&#39;, 0.813499391078949)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="custom-training">
<h4>Custom training<a class="headerlink" href="#custom-training" title="Permalink to this headline">#</a></h4>
<p>An important thing we should be sure to distinguish, is a pre-trained model and a learning algorithm. All the mathematics above are learning algorithms, they will be useful when we want to train our custom embedding model for unpopular languages such as Vietnamese. We can intialize a new model and train from scratch with our own dataset via two Gensim classes, <a class="reference external" href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"><code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code></a> and <a class="reference external" href="https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"><code class="docutils literal notranslate"><span class="pre">FastText</span></code></a>. They have the following hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vector_size</span></code>: the size of each embedding vector, defaults to <em>100</em>. Usually higher is better.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sg</span></code>: whether to use Skip-Gram, otherwise use CBOW, defaults to <em>True</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">window</span></code>: the context window size, defaults to <em>5</em>. Recommended values are <em>10</em> for Skip-Gram and <em>5</em> for CBOW.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">negative</span></code>: the number of noisy words drawn for negative sampling, defaults to <em>5</em>. Should be in range <span class="math notranslate nohighlight">\([5,20]\)</span>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: the sub-sampling rate of frequent words, defaults to <em>0.001</em>. Recommended values are in range <span class="math notranslate nohighlight">\([10^{-5},10^{-3}]\)</span>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shrink_windows</span></code>: whether to shrink context window, defaults to <em>True</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_n</span></code> and
<code class="docutils literal notranslate"><span class="pre">max_n</span></code>: the range of <span class="math notranslate nohighlight">\(n\)</span> in FastText’s N-gram, defaults to <em>3</em> and <em>6</em>.</p></li>
</ul>
<p>The appropriate format for these two classes is a list of lists (tokenized sentences). When our custom model has been intialized successfully on a corpus, you can keep teaching it using more and more data by calling the
<code class="docutils literal notranslate"><span class="pre">train()</span></code> method. You can also call the <code class="docutils literal notranslate"><span class="pre">wv</span></code> (word vectors) method to access the embedding vectors via a <code class="docutils literal notranslate"><span class="pre">KeyedVectors</span></code> object. A good practice which is highly recommended by Gensim is only using the embedded vectors (<em>physical</em>) instead of the full model (<em>logical</em>) because the physical option spends less memory and can extract vectors much faster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">info</span><span class="p">()[</span><span class="s1">&#39;corpora&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;semeval-2016-2017-task3-subtaskBC&#39;,
 &#39;semeval-2016-2017-task3-subtaskA-unannotated&#39;,
 &#39;patent-2017&#39;,
 &#39;quora-duplicate-questions&#39;,
 &#39;wiki-english-20171001&#39;,
 &#39;text8&#39;,
 &#39;fake-news&#39;,
 &#39;20-newsgroups&#39;,
 &#39;__testing_matrix-synopsis&#39;,
 &#39;__testing_multipart-matrix-synopsis&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus1</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;text8&#39;</span><span class="p">))[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">corpus2</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;interface&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;survey&#39;</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">,</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;response&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;interface&#39;</span><span class="p">,</span> <span class="s1">&#39;system&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;response&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;graph&#39;</span><span class="p">,</span> <span class="s1">&#39;trees&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;graph&#39;</span><span class="p">,</span> <span class="s1">&#39;minors&#39;</span><span class="p">,</span> <span class="s1">&#39;trees&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;graph&#39;</span><span class="p">,</span> <span class="s1">&#39;minors&#39;</span><span class="p">,</span> <span class="s1">&#39;survey&#39;</span><span class="p">],</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="n">corpus1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">corpus2</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(28, 28)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;output/word2vec_sample.vectors&#39;</span><span class="p">)</span>
<span class="n">embedder</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/word2vec_sample.vectors&#39;</span><span class="p">,</span> <span class="n">mmap</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder</span><span class="p">[</span><span class="s1">&#39;system&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.6818,  1.1834, -2.3567, -1.0533, -0.7062, -1.593 , -0.3097],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="contextual-embedding">
<h3>2.3. Contextual embedding<a class="headerlink" href="#contextual-embedding" title="Permalink to this headline">#</a></h3>
<p>Word embedding has done a decent job improving the text representation task, but they cannot handle <a class="reference external" href="https://en.wikipedia.org/wiki/Heteronym_(linguistics)">heteronyms</a>. This is because, the context is needed to determine the true meaning of such words. For example, the word “bank” in “river bank” and “bank deposit” has completely different meanings but will gain the same embedding by Word2Vec or GloVe.</p>
<p>This problem can be solved using contextual embedding, a technique that pretrains a <em>logical</em> model rather than a <em>physical</em> vectors collection. The model has the capable of transforming a word into different embedding vectors based on specific context. Many contextual embedding methods have been proposed out there, where the most noticeable ones are ULMFiT, ELMo, BERT and <a class="reference external" href="https://tfhub.dev/google/collections/transformer_encoders_text/1">Transformer</a>.</p>
<section id="w-bert">
<h4>(w) BERT<a class="headerlink" href="#w-bert" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> (Bidirectional Encoder Representations from Transformers) is a state-of-the-art architecture for pretraining word representation proposed by Google in 2018. It is designed so that the embedding of a word takes into account both left and right context words. BERT models are pre-trained on a large corpus of text, can be then fine-tuned for specific tasks. There are a lot of BERT pre-train models can be found in TensorFlow Hub:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/bert/1">BERT-base</a>, models by original authors</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/experts/bert/1">BERT-experts</a>, fine-tuned BERT-base models for different domains</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/albert/1">ALBERT</a>, a lite version of BERT with reduced number of parameters</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/electra/1">ELECTRA</a>, a BERT-like architecture that pre-trains a discriminator</p></li>
</ul>
<p>When using BERT in TensorFlow, the corpus needs to go through a preprocessor first (read the documentation to know which processor should be used). Then, the BERT model takes the processed data as input to generate embedding vectors. This object has two important keys,
<code class="docutils literal notranslate"><span class="pre">sequence_output</span></code> which represents each token a vector and
<code class="docutils literal notranslate"><span class="pre">pooled_output</span></code> which represents the whole sequence a vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bertProcessor</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&#39;</span><span class="p">)</span>
<span class="n">bertEncoder</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;this is such an amazing movie&#39;</span><span class="p">,</span>
    <span class="s1">&#39;this movie is terrible&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">bertProcessor</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">bertEncoder</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embed</span><span class="p">[</span><span class="s1">&#39;pooled_output&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorShape([2, 512])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embed</span><span class="p">[</span><span class="s1">&#39;sequence_output&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorShape([2, 128, 512])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sequence-labeling">
<h2>3. Sequence labeling<a class="headerlink" href="#sequence-labeling" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Sequence_labeling">Sequential labeling</a>, as its name states, is the task of classifying members of a sequence into pre-defined categories. The main application of sequence labeling in NLP is syntactic analysis.</p>
<section id="syntactic-analysis">
<h3>3.1. Syntactic analysis<a class="headerlink" href="#syntactic-analysis" title="Permalink to this headline">#</a></h3>
<p>Syntactic analysis is the process of analyzing word roles and relationship. We are going to implement three most common analyses of this type using SpaCy.</p>
<section id="part-of-speech">
<h4>Part of speech<a class="headerlink" href="#part-of-speech" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Part_of_speech">Part-of-speech</a> (POS, somtimes called POS tag) is the grammatical category of words. The problem of finding POS of words is call <em>POS tagging</em>. It is the simplest sequence labeling problem, as each word has only one POS tag. The complete list of POS tags can be found <a class="reference external" href="https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L23">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;en_core_web_sm&#39;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I&#39;m hungry, but there is nothing in the fridge&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">listToken</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="n">listPos</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="n">listDesc</span> <span class="o">=</span> <span class="p">[</span><span class="n">spacy</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;token&#39;</span><span class="p">:</span> <span class="n">listToken</span><span class="p">,</span>
    <span class="s1">&#39;postag&#39;</span><span class="p">:</span> <span class="n">listPos</span><span class="p">,</span>
    <span class="s1">&#39;description&#39;</span><span class="p">:</span> <span class="n">listDesc</span><span class="p">,</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>token</th>
      <th>postag</th>
      <th>description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>I</td>
      <td>PRON</td>
      <td>pronoun</td>
    </tr>
    <tr>
      <th>1</th>
      <td>'m</td>
      <td>AUX</td>
      <td>auxiliary</td>
    </tr>
    <tr>
      <th>2</th>
      <td>hungry</td>
      <td>ADJ</td>
      <td>adjective</td>
    </tr>
    <tr>
      <th>3</th>
      <td>,</td>
      <td>PUNCT</td>
      <td>punctuation</td>
    </tr>
    <tr>
      <th>4</th>
      <td>but</td>
      <td>CCONJ</td>
      <td>coordinating conjunction</td>
    </tr>
    <tr>
      <th>5</th>
      <td>there</td>
      <td>PRON</td>
      <td>pronoun</td>
    </tr>
    <tr>
      <th>6</th>
      <td>is</td>
      <td>VERB</td>
      <td>verb</td>
    </tr>
    <tr>
      <th>7</th>
      <td>nothing</td>
      <td>PRON</td>
      <td>pronoun</td>
    </tr>
    <tr>
      <th>8</th>
      <td>in</td>
      <td>ADP</td>
      <td>adposition</td>
    </tr>
    <tr>
      <th>9</th>
      <td>the</td>
      <td>DET</td>
      <td>determiner</td>
    </tr>
    <tr>
      <th>10</th>
      <td>fridge</td>
      <td>NOUN</td>
      <td>noun</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="dependency">
<h4>Dependency<a class="headerlink" href="#dependency" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Dependency_grammar">Dependency</a> parsing is the process of analyzing the sentence structures and word relationships. It can be used to examine phrasal nouns and verbs. The full list of dependency labels can be found <a class="reference external" href="https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L205">here</a>.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I&#39;m hungry, but there is nothing in the fridge&quot;</span><span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;dep&#39;</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;distance&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" id="35676c0b8fbc4b19892f6ccc63891fee-0" class="displacy" width="1050" height="287.0" direction="ltr" style="max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr">
<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="50">I</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="50">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="150">'m</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="150">AUX</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="250">hungry,</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="250">ADJ</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="350">but</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="350">CCONJ</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="450">there</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="450">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="550">is</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="550">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="650">nothing</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="650">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="750">in</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="750">ADP</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="850">the</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="850">DET</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="197.0">
    <tspan class="displacy-word" fill="currentColor" x="950">fridge</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="950">NOUN</tspan>
</text>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-0" stroke-width="2px" d="M70,152.0 C70,102.0 140.0,102.0 140.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-0" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M70,154.0 L62,142.0 78,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-1" stroke-width="2px" d="M170,152.0 C170,102.0 240.0,102.0 240.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-1" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">acomp</textPath>
    </text>
    <path class="displacy-arrowhead" d="M240.0,154.0 L248.0,142.0 232.0,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-2" stroke-width="2px" d="M170,152.0 C170,52.0 345.0,52.0 345.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-2" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">cc</textPath>
    </text>
    <path class="displacy-arrowhead" d="M345.0,154.0 L353.0,142.0 337.0,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-3" stroke-width="2px" d="M470,152.0 C470,102.0 540.0,102.0 540.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-3" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">expl</textPath>
    </text>
    <path class="displacy-arrowhead" d="M470,154.0 L462,142.0 478,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-4" stroke-width="2px" d="M170,152.0 C170,2.0 550.0,2.0 550.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-4" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">conj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M550.0,154.0 L558.0,142.0 542.0,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-5" stroke-width="2px" d="M570,152.0 C570,102.0 640.0,102.0 640.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-5" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">attr</textPath>
    </text>
    <path class="displacy-arrowhead" d="M640.0,154.0 L648.0,142.0 632.0,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-6" stroke-width="2px" d="M670,152.0 C670,102.0 740.0,102.0 740.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-6" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">prep</textPath>
    </text>
    <path class="displacy-arrowhead" d="M740.0,154.0 L748.0,142.0 732.0,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-7" stroke-width="2px" d="M870,152.0 C870,102.0 940.0,102.0 940.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-7" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">det</textPath>
    </text>
    <path class="displacy-arrowhead" d="M870,154.0 L862,142.0 878,142.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-35676c0b8fbc4b19892f6ccc63891fee-0-8" stroke-width="2px" d="M770,152.0 C770,52.0 945.0,52.0 945.0,152.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-8" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">pobj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M945.0,154.0 L953.0,142.0 937.0,142.0" fill="currentColor"/>
</g>
</svg></span></div></div>
</div>
</section>
<section id="named-entities">
<h4>Named entities<a class="headerlink" href="#named-entities" title="Permalink to this headline">#</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named entity recognition</a> is the problem of determining if a word has real-world meanings, such as name of people, organizations and locations.
The list of Spacy’s entity names can be found <a class="reference external" href="https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L325">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Apple is looking at buying U.K. startup for $1 billion&quot;</span><span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;ent&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><div class="entities" style="line-height: 2.5; direction: ltr">
<mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Apple
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">ORG</span>
</mark>
 is looking at buying 
<mark class="entity" style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    U.K.
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">GPE</span>
</mark>
 startup for 
<mark class="entity" style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    $1 billion
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">MONEY</span>
</mark>
</div></span></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[(</span><span class="n">ent</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="o">.</span><span class="n">label</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Apple&#39;, 383), (&#39;U.K.&#39;, 384), (&#39;$1 billion&#39;, 394)]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="w-hidden-markov-model">
<h3>(w) Hidden Markov Model<a class="headerlink" href="#w-hidden-markov-model" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Model</a>
<a class="reference external" href="https://github.com/hmmlearn/hmmlearn">https://github.com/hmmlearn/hmmlearn</a></p>
</section>
<section id="w-conditional-random-field">
<h3>(w) Conditional Random Field<a class="headerlink" href="#w-conditional-random-field" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Conditional_random_field">Conditional Random Field</a>
<a class="reference external" href="https://github.com/TeamHG-Memex/sklearn-crfsuite">https://github.com/TeamHG-Memex/sklearn-crfsuite</a></p>
<p>Conditional Random Fields are a discriminative model, used for predicting sequences. They use contextual information from previous labels, thus increasing the amount of information the model has to make a good prediction.
Discriminative classifier - they model the decision boundary between the different classes (just like logistic regression)</p>
</section>
</section>
<section id="nlp-for-vietnamese">
<h2>4. NLP for Vietnamese<a class="headerlink" href="#nlp-for-vietnamese" title="Permalink to this headline">#</a></h2>
<section id="character-normalization">
<h3>4.1. Character normalization<a class="headerlink" href="#character-normalization" title="Permalink to this headline">#</a></h3>
<p>There are two types of Vietnamese characters being widely used, <a class="reference external" href="https://en.wikipedia.org/wiki/Combining_character">decomposed</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Precomposed_character">composed</a>. They are visually identical, but are constructed from different Unicode code-points. We can see the the difference in the example below for the word “Tiếng Việt”. Because of this, we use the <a class="reference external" href="https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize"><code class="docutils literal notranslate"><span class="pre">unicodedata.normalize()</span></code></a> function to convert between the two forms, with the corresponding names NFD and NFC. As the composed (NFC) form takes less memory, it will be the go-to choice.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">strNFC</span> <span class="o">=</span> <span class="s1">&#39;Tiếng Việt&#39;</span>
<span class="n">strNFD</span> <span class="o">=</span> <span class="s1">&#39;Tiếng Việt&#39;</span>
<span class="n">strNFC</span> <span class="o">==</span> <span class="n">strNFD</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">strNFD</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">strNFC</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>b&#39;Ti\xc3\xaa\xcc\x81ng Vi\xc3\xaa\xcc\xa3t&#39;
b&#39;Ti\xe1\xba\xbfng Vi\xe1\xbb\x87t&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">strNFD</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;unicode_escape&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">strNFC</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;unicode_escape&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>b&#39;Ti\\xea\\u0301ng Vi\\xea\\u0323t&#39;
b&#39;Ti\\u1ebfng Vi\\u1ec7t&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFC&#39;</span><span class="p">,</span> <span class="n">strNFD</span><span class="p">)</span> <span class="o">==</span> <span class="n">strNFC</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>We can also vectorize the normalization function so that we can transform a corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">vectorize</span>
<span class="nd">@vectorize</span>
<span class="k">def</span> <span class="nf">normalize_vn</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFC&#39;</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;Tiếng Việt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Hà Nội&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ngôn ngữ&#39;</span>
<span class="p">]</span>

<span class="n">normalize_vn</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Tiếng Việt&#39;, &#39;Hà Nội&#39;, &#39;ngôn ngữ&#39;], dtype=&#39;&lt;U10&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="underthesea">
<h3>4.2. Underthesea<a class="headerlink" href="#underthesea" title="Permalink to this headline">#</a></h3>
<p>This section uses <a class="reference external" href="https://github.com/undertheseanlp/underthesea">Underthesea</a> for Vietnamese NLP. Other resources, <a class="reference external" href="https://github.com/vncorenlp/VnCoreNLP">VNCoreNLP</a> and <a class="reference external" href="https://github.com/trungtv/pyvi">PyVi</a>, are available as well. Compared to English, processing Vietnames requires no stemming and lemmatization, but there will be additional tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">underthesea</span><span class="o">.</span><span class="n">text_normalize</span><span class="p">(</span><span class="s1">&#39;Ðảm baỏ chất lựơng phòng thí nghịêm hoá học&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Đảm bảo chất lượng phòng thí nghiệm hóa học&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">underthesea</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="s1">&#39;Hồ Gươm là danh lam thắng cảnh Hà Nội&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Hồ Gươm&#39;, &#39;là&#39;, &#39;danh lam&#39;, &#39;thắng cảnh&#39;, &#39;Hà Nội&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">underthesea</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="s1">&#39;Hồ Gươm là danh lam thắng cảnh Hà Nội&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Hồ Gươm&#39;, &#39;Np&#39;),
 (&#39;là&#39;, &#39;V&#39;),
 (&#39;danh lam&#39;, &#39;N&#39;),
 (&#39;thắng cảnh&#39;, &#39;V&#39;),
 (&#39;Hà Nội&#39;, &#39;Np&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">underthesea</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="s1">&#39;Hồ Gươm là danh lam thắng cảnh Hà Nội!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Hồ Gươm&#39;, &#39;Np&#39;, &#39;B-NP&#39;),
 (&#39;là&#39;, &#39;V&#39;, &#39;B-VP&#39;),
 (&#39;danh lam&#39;, &#39;N&#39;, &#39;B-NP&#39;),
 (&#39;thắng cảnh&#39;, &#39;V&#39;, &#39;B-VP&#39;),
 (&#39;Hà Nội&#39;, &#39;Np&#39;, &#39;B-NP&#39;),
 (&#39;!&#39;, &#39;CH&#39;, &#39;O&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">underthesea</span><span class="o">.</span><span class="n">dependency_parse</span><span class="p">(</span><span class="s1">&#39;Hồ Gươm là danh lam thắng cảnh Hà Nội!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                                             
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Hồ Gươm&#39;, 4, &#39;nsubj&#39;),
 (&#39;là&#39;, 4, &#39;cop&#39;),
 (&#39;danh lam&#39;, 4, &#39;nummod&#39;),
 (&#39;thắng cảnh&#39;, 0, &#39;root&#39;),
 (&#39;Hà Nội&#39;, 4, &#39;nmod&#39;),
 (&#39;!&#39;, 3, &#39;punct&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">underthesea</span><span class="o">.</span><span class="n">ner</span><span class="p">(</span><span class="s1">&#39;Hồ Gươm là danh lam thắng cảnh Hà Nội!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;Hồ Gươm&#39;, &#39;Np&#39;, &#39;B-NP&#39;, &#39;B-PER&#39;),
 (&#39;là&#39;, &#39;V&#39;, &#39;B-VP&#39;, &#39;O&#39;),
 (&#39;danh lam&#39;, &#39;N&#39;, &#39;B-NP&#39;, &#39;O&#39;),
 (&#39;thắng cảnh&#39;, &#39;V&#39;, &#39;B-VP&#39;, &#39;O&#39;),
 (&#39;Hà Nội&#39;, &#39;Np&#39;, &#39;B-NP&#39;, &#39;B-LOC&#39;),
 (&#39;!&#39;, &#39;CH&#39;, &#39;O&#39;, &#39;O&#39;)]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><em><a class="reference external" href="http://tartarus.org">tartarus.org</a> - <a class="reference external" href="https://tartarus.org/martin/PorterStemmer/def.txt">An algorithm for suffix stripping</a></em></p></li>
<li><p><em><a class="reference external" href="http://dl.acm.org">dl.acm.org</a> - <a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/101306.101310">Another stemmer</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1607.04606.pdf">Enriching word vectors with subword information</a></em></p></li>
<li><p><em><a class="reference external" href="http://nlp.stanford.edu">nlp.stanford.edu</a> - <a class="reference external" href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for word representation</a></em></p></li>
<li><p><em><a class="reference external" href="http://lilianweng.github.io">lilianweng.github.io</a> - <a class="reference external" href="https://lilianweng.github.io/posts/2017-10-15-word-embedding/">Learning word embedding</a></em></p></li>
<li><p><em><a class="reference external" href="http://mccormickml.com">mccormickml.com</a> - <a class="reference external" href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec tutorial - The Skip-Gram model</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for language understanding</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1801.06146.pdf">Universal Language Model Fine-tuning for text classification</a></em></p></li>
<li><p><em><a class="reference external" href="http://arxiv.org">arxiv.org</a> - <a class="reference external" href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a></em></p></li>
<li><p><em><a class="reference external" href="http://d2l.ai">d2l.ai</a> - <a class="reference external" href="https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html">The Skip-Gram model</a></em></p></li>
<li><p><em><a class="reference external" href="http://aegis4048.github.io">aegis4048.github.io</a> - <a class="reference external" href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling">Demystifying Neural Network in Skip-Gram language modeling</a></em></p></li>
<li><p><em><a class="reference external" href="http://jonathan-hui.medium.com">jonathan-hui.medium.com</a> - <a class="reference external" href="https://jonathan-hui.medium.com/nlp-word-embedding-glove-5e7f523999f6">NLP - Word embedding &amp; GloVe</a></em></p></li>
<li><p><em><a class="reference external" href="http://maelfabien.github.io">maelfabien.github.io</a> - <a class="reference external" href="https://maelfabien.github.io/machinelearning/NLP_3/">Word embedding with Skip-Gram Word2Vec</a></em></p></li>
<li><p><em><a class="reference external" href="http://amitness.com">amitness.com</a> - <a class="reference external" href="https://amitness.com/2020/06/fasttext-embeddings/">FastText embeddings</a></em></p></li>
<li><p><em><a class="reference external" href="http://medium.datadriveninvestor.com">medium.datadriveninvestor.com</a> - <a class="reference external" href="https://medium.datadriveninvestor.com/word2vec-skip-gram-model-explained-383fa6ddc4ae">Word2Vec (Skip-Gram model) explained</a></em></p></li>
<li><p><em><a class="reference external" href="http://towardsdatascience.com">towardsdatascience.com</a> - <a class="reference external" href="https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010">Intuitive guide to understanding GloVe embedding</a></em></p></li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p><a class="reference external" href="https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook#Conversion-of-Emoticon-to-Words">https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook#Conversion-of-Emoticon-to-Words</a></p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/learn-guide/natural-language-processing">https://www.kaggle.com/learn-guide/natural-language-processing</a></p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code?resource=download">https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code?resource=download</a></p></li>
<li><p><a class="reference external" href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c">https://medium.com/&#64;samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c</a></p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation">https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation</a></p></li>
<li><p><a class="reference external" href="https://www.projectpro.io/article/bert-nlp-model-explained/558">https://www.projectpro.io/article/bert-nlp-model-explained/558</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/bert-to-the-rescue-17671379687f">https://towardsdatascience.com/bert-to-the-rescue-17671379687f</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2011.06727.pdf">https://arxiv.org/pdf/2011.06727.pdf</a></p></li>
</ul>
</section>
<section id="install">
<h2>Install<a class="headerlink" href="#install" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">gensim</span> <span class="o">--</span><span class="n">upgrade</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="s2">&quot;tensorflow-text==2.8.*&quot;</span> <span class="o">--</span><span class="n">user</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">underthesea</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">underthesea</span><span class="p">[</span><span class="n">deep</span><span class="p">]</span> <span class="o">--</span><span class="n">user</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "hungpq7/tabular-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapter-10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="_intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><b>10. Deep Learning</b></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="opencv-image-processing.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">OpenCV: Image Processing</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Quang Hung &#9829; Thuy Linh<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>